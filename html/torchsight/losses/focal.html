<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.5.3" />
<title>torchsight.losses.focal API documentation</title>
<meta name="description" content="Focal Loss module â€¦" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.name small{font-weight:normal}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase;cursor:pointer}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title"><code>torchsight.losses.focal</code> module</h1>
</header>
<section id="section-intro">
<p>Focal Loss module.</p>
<p>Loss implemented based on the original paper "Focal Loss For Dense Object Detection":
<a href="https://arxiv.org/pdf/1708.02002.pdf">https://arxiv.org/pdf/1708.02002.pdf</a></p>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">&#34;&#34;&#34;Focal Loss module.

Loss implemented based on the original paper &#34;Focal Loss For Dense Object Detection&#34;:
https://arxiv.org/pdf/1708.02002.pdf
&#34;&#34;&#34;
import torch
from torch import nn
from ..metrics import iou as compute_iou
from ..models import Anchors


class FocalLoss(nn.Module):
    &#34;&#34;&#34;Loss to penalize the detection of objects.&#34;&#34;&#34;

    def __init__(self, alpha=0.25, gamma=2.0, sigma=3.0, iou_thresholds=None):
        &#34;&#34;&#34;Initialize the loss.

        Train as background (minimize all the probabilities of the classes) if the IoU is below the &#39;background&#39;
        threshold and train with the label of the object if the IoU is over the &#39;object&#39; threshold.
        Ignore the anchors between both thresholds.

        Args:
            alpha (float): Alpha parameter for the focal loss.
            gamma (float): Gamma parameter for the focal loss.
            sigma (float): Point that defines the change from L1 loss to L2 loss (smooth L1).
            iou_thresholds (dict): Indicates the thresholds to assign an anchor as background or object.
            device (str, optional): Indicates the device where to run the loss.
        &#34;&#34;&#34;
        super(FocalLoss, self).__init__()

        self.alpha = alpha
        self.gamma = gamma
        self.sigma = sigma
        if iou_thresholds is None:
            iou_thresholds = {&#39;background&#39;: 0.4, &#39;object&#39;: 0.5}
        self.iou_background = iou_thresholds[&#39;background&#39;]
        self.iou_object = iou_thresholds[&#39;object&#39;]
        self.device = &#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39;

    def forward(self, anchors, regressions, classifications, annotations):
        &#34;&#34;&#34;Forward pass to get the loss.

        Args:
            anchors (torch.Tensor): The base anchors (without the transformation to adjust the
                bounding boxes).
                Shape:
                    (batch size, total boxes, 4)
            regressions (torch.Tensor): The regression values to adjust the anchors to the predicted
                bounding boxes.
                Shape:
                    (batch size, total boxes, 4)
            classifications (torch.Tensor): The probabilities for each class at each bounding box.
                Shape:
                    (batch size, total boxes, number of classes)
            annotations (torch.Tensor): Ground truth. Tensor with the bounding boxes and the label for
                the object. The values must be x1, y1 (top left corner), x2, y2 (bottom right corner)
                and the last value is the label.
                Shape:
                    (batch size, maximum objects in any image, 5).

                Why maximum objects in any image? Because if we have more than one image, each image
                could have different amounts of objects inside and have different dimensions in the
                ground truth (dim 1 of the batch). So we could have the maximum amount of objects
                inside any image and then the rest of the images ground truths could be populated
                with -1.0. So if this loss finds a ground truth box populated with -1.0 it understands
                that it was to match the dimensions and have only one tensor.

        Returns:
            torch.Tensor: The mean classification loss.
            torch.Tensor: The mean regression loss.
        &#34;&#34;&#34;
        batch_size = anchors.shape[0]
        batch_anchors = anchors
        batch_regressions = regressions
        batch_classifications = classifications
        batch_annotations = annotations

        classification_losses = []
        regression_losses = []

        for index in range(batch_size):
            anchors = batch_anchors[index]
            regressions = batch_regressions[index]
            classifications = batch_classifications[index]
            annotations = batch_annotations[index]
            # Keep only the real labels
            annotations = annotations[annotations[:, -1] != -1]

            if annotations.shape[0] == 0:
                classification_losses.append(torch.zeros((1,)).mean().to(self.device))
                regression_losses.append(torch.zeros((1,)).mean().to(self.device))
                continue

            # Get assignations of the annotations to the anchors
            # Get the assigned annotations (the i-th assigned annotation is the annotation assigned to the i-th
            # anchor)
            # Get the masks to select the anchors assigned to an object (IoU bigger than iou_object threshold)
            # Get the mask to select the anchors assigned to background (IoU lower than iou_background)
            assignations = Anchors.assign(anchors,
                                          annotations,
                                          thresholds={&#39;object&#39;: self.iou_object, &#39;background&#39;: self.iou_background})
            assigned_annotations, selected_anchors_objects, selected_anchors_backgrounds = assignations

            # Compute classification loss

            # Create the target tensor. Shape (number anchors, number of classes) where the
            # index of the class for the annotation has a 1 and all the others zero.
            n_classes = classifications.shape[1]
            targets = torch.zeros((anchors.shape[0], n_classes)).to(self.device)
            # Get the label for each anchor based on its assigned annotation ant turn it on. Do this only
            # for the assigned anchors.
            targets[selected_anchors_objects, assigned_annotations[selected_anchors_objects, 4].long()] = 1.
            # Avoid NaN in log clamping the classifications probabilities
            classifications = classifications.clamp(min=1e-5, max=1 - 1e-5)
            # Generate the alpha factor
            alpha = self.alpha * torch.ones(targets.shape).to(self.device)
            # It must be alpha for the correct label and 1 - alpha for the others
            alpha = torch.where(targets == 1, alpha, 1. - alpha)
            # Generate the focal weight
            focal = torch.where(targets == 1, 1 - classifications, classifications)
            focal = alpha * (focal ** self.gamma)
            # Get the binary cross entropy
            bce = -(targets * torch.log(classifications) + (1. - targets) * torch.log(1 - classifications))
            # Free memory
            del targets
            # Remove the loss for the not assigned anchors (not background, not object)
            selected_anchors = selected_anchors_backgrounds + selected_anchors_objects
            ignored_anchors = 1 - selected_anchors
            bce[ignored_anchors, :] = 0
            # Append to the classification losses
            classification_losses.append((focal * bce).sum() / selected_anchors.sum().type(torch.float))
            # Free memory
            del alpha, focal, bce, ignored_anchors, classifications, selected_anchors_backgrounds

            # Compute regression loss

            # Append zero loss if there is no object
            if not selected_anchors_objects.sum() &gt; 0:
                regression_losses.append(torch.zeros((1,)).mean().to(self.device))
                continue

            # Get the assigned ground truths to each one of the selected anchors to train the regression
            assigned_annotations = assigned_annotations[selected_anchors_objects, :-1]  # Shape (n selected anchors, 4)
            # Keep only the regressions for the selected anchors
            regressions = regressions[selected_anchors_objects, :]  # Shape (n selected anchors, 4)
            # Keep only the selected anchors for regression
            anchors = anchors[selected_anchors_objects, :]  # Shape (n selected anchors, 4)
            # Free memory
            del selected_anchors_objects

            # Get the parameters from the anchors
            anchors_widths = anchors[:, 2] - anchors[:, 0]
            anchors_heights = anchors[:, 3] - anchors[:, 1]
            anchors_x = anchors[:, 0] + (anchors_widths / 2)
            anchors_y = anchors[:, 1] + (anchors_heights / 2)

            # Get the parameters from the ground truth
            gt_widths = (assigned_annotations[:, 2] - assigned_annotations[:, 0]).clamp(min=1)
            gt_heights = (assigned_annotations[:, 3] - assigned_annotations[:, 1]).clamp(min=1)
            gt_x = assigned_annotations[:, 0] + (gt_widths / 2)
            gt_y = assigned_annotations[:, 1] + (gt_heights / 2)

            # Generate the targets
            targets_dw = torch.log(gt_widths / anchors_widths)
            targets_dh = torch.log(gt_heights / anchors_heights)
            targets_dx = (gt_x - anchors_x) / anchors_widths
            targets_dy = (gt_y - anchors_y) / anchors_heights

            # Stack the targets as it could come from the regression module
            targets = torch.stack([targets_dx, targets_dy, targets_dw, targets_dh], dim=1)
            regression_diff = torch.abs(targets - regressions)
            # Compute Smooth L1 loss
            regression_loss = torch.where(
                regression_diff &lt;= 1 / (self.sigma**2),
                0.5 * (self.sigma * regression_diff) ** 2,
                regression_diff - (0.5 / (self.sigma ** 2))
            )

            # Append the squared error
            regression_losses.append(regression_loss.mean())

        # Return the mean classification loss and the mean regression loss
        return torch.stack(classification_losses).mean(), torch.stack(regression_losses).mean()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="torchsight.losses.focal.FocalLoss"><code class="flex name class">
<span>class <span class="ident">FocalLoss</span></span>
<span>(</span><span><small>ancestors:</small> torch.nn.modules.module.Module)</span>
</code></dt>
<dd>
<section class="desc"><p>Loss to penalize the detection of objects.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class FocalLoss(nn.Module):
    &#34;&#34;&#34;Loss to penalize the detection of objects.&#34;&#34;&#34;

    def __init__(self, alpha=0.25, gamma=2.0, sigma=3.0, iou_thresholds=None):
        &#34;&#34;&#34;Initialize the loss.

        Train as background (minimize all the probabilities of the classes) if the IoU is below the &#39;background&#39;
        threshold and train with the label of the object if the IoU is over the &#39;object&#39; threshold.
        Ignore the anchors between both thresholds.

        Args:
            alpha (float): Alpha parameter for the focal loss.
            gamma (float): Gamma parameter for the focal loss.
            sigma (float): Point that defines the change from L1 loss to L2 loss (smooth L1).
            iou_thresholds (dict): Indicates the thresholds to assign an anchor as background or object.
            device (str, optional): Indicates the device where to run the loss.
        &#34;&#34;&#34;
        super(FocalLoss, self).__init__()

        self.alpha = alpha
        self.gamma = gamma
        self.sigma = sigma
        if iou_thresholds is None:
            iou_thresholds = {&#39;background&#39;: 0.4, &#39;object&#39;: 0.5}
        self.iou_background = iou_thresholds[&#39;background&#39;]
        self.iou_object = iou_thresholds[&#39;object&#39;]
        self.device = &#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39;

    def forward(self, anchors, regressions, classifications, annotations):
        &#34;&#34;&#34;Forward pass to get the loss.

        Args:
            anchors (torch.Tensor): The base anchors (without the transformation to adjust the
                bounding boxes).
                Shape:
                    (batch size, total boxes, 4)
            regressions (torch.Tensor): The regression values to adjust the anchors to the predicted
                bounding boxes.
                Shape:
                    (batch size, total boxes, 4)
            classifications (torch.Tensor): The probabilities for each class at each bounding box.
                Shape:
                    (batch size, total boxes, number of classes)
            annotations (torch.Tensor): Ground truth. Tensor with the bounding boxes and the label for
                the object. The values must be x1, y1 (top left corner), x2, y2 (bottom right corner)
                and the last value is the label.
                Shape:
                    (batch size, maximum objects in any image, 5).

                Why maximum objects in any image? Because if we have more than one image, each image
                could have different amounts of objects inside and have different dimensions in the
                ground truth (dim 1 of the batch). So we could have the maximum amount of objects
                inside any image and then the rest of the images ground truths could be populated
                with -1.0. So if this loss finds a ground truth box populated with -1.0 it understands
                that it was to match the dimensions and have only one tensor.

        Returns:
            torch.Tensor: The mean classification loss.
            torch.Tensor: The mean regression loss.
        &#34;&#34;&#34;
        batch_size = anchors.shape[0]
        batch_anchors = anchors
        batch_regressions = regressions
        batch_classifications = classifications
        batch_annotations = annotations

        classification_losses = []
        regression_losses = []

        for index in range(batch_size):
            anchors = batch_anchors[index]
            regressions = batch_regressions[index]
            classifications = batch_classifications[index]
            annotations = batch_annotations[index]
            # Keep only the real labels
            annotations = annotations[annotations[:, -1] != -1]

            if annotations.shape[0] == 0:
                classification_losses.append(torch.zeros((1,)).mean().to(self.device))
                regression_losses.append(torch.zeros((1,)).mean().to(self.device))
                continue

            # Get assignations of the annotations to the anchors
            # Get the assigned annotations (the i-th assigned annotation is the annotation assigned to the i-th
            # anchor)
            # Get the masks to select the anchors assigned to an object (IoU bigger than iou_object threshold)
            # Get the mask to select the anchors assigned to background (IoU lower than iou_background)
            assignations = Anchors.assign(anchors,
                                          annotations,
                                          thresholds={&#39;object&#39;: self.iou_object, &#39;background&#39;: self.iou_background})
            assigned_annotations, selected_anchors_objects, selected_anchors_backgrounds = assignations

            # Compute classification loss

            # Create the target tensor. Shape (number anchors, number of classes) where the
            # index of the class for the annotation has a 1 and all the others zero.
            n_classes = classifications.shape[1]
            targets = torch.zeros((anchors.shape[0], n_classes)).to(self.device)
            # Get the label for each anchor based on its assigned annotation ant turn it on. Do this only
            # for the assigned anchors.
            targets[selected_anchors_objects, assigned_annotations[selected_anchors_objects, 4].long()] = 1.
            # Avoid NaN in log clamping the classifications probabilities
            classifications = classifications.clamp(min=1e-5, max=1 - 1e-5)
            # Generate the alpha factor
            alpha = self.alpha * torch.ones(targets.shape).to(self.device)
            # It must be alpha for the correct label and 1 - alpha for the others
            alpha = torch.where(targets == 1, alpha, 1. - alpha)
            # Generate the focal weight
            focal = torch.where(targets == 1, 1 - classifications, classifications)
            focal = alpha * (focal ** self.gamma)
            # Get the binary cross entropy
            bce = -(targets * torch.log(classifications) + (1. - targets) * torch.log(1 - classifications))
            # Free memory
            del targets
            # Remove the loss for the not assigned anchors (not background, not object)
            selected_anchors = selected_anchors_backgrounds + selected_anchors_objects
            ignored_anchors = 1 - selected_anchors
            bce[ignored_anchors, :] = 0
            # Append to the classification losses
            classification_losses.append((focal * bce).sum() / selected_anchors.sum().type(torch.float))
            # Free memory
            del alpha, focal, bce, ignored_anchors, classifications, selected_anchors_backgrounds

            # Compute regression loss

            # Append zero loss if there is no object
            if not selected_anchors_objects.sum() &gt; 0:
                regression_losses.append(torch.zeros((1,)).mean().to(self.device))
                continue

            # Get the assigned ground truths to each one of the selected anchors to train the regression
            assigned_annotations = assigned_annotations[selected_anchors_objects, :-1]  # Shape (n selected anchors, 4)
            # Keep only the regressions for the selected anchors
            regressions = regressions[selected_anchors_objects, :]  # Shape (n selected anchors, 4)
            # Keep only the selected anchors for regression
            anchors = anchors[selected_anchors_objects, :]  # Shape (n selected anchors, 4)
            # Free memory
            del selected_anchors_objects

            # Get the parameters from the anchors
            anchors_widths = anchors[:, 2] - anchors[:, 0]
            anchors_heights = anchors[:, 3] - anchors[:, 1]
            anchors_x = anchors[:, 0] + (anchors_widths / 2)
            anchors_y = anchors[:, 1] + (anchors_heights / 2)

            # Get the parameters from the ground truth
            gt_widths = (assigned_annotations[:, 2] - assigned_annotations[:, 0]).clamp(min=1)
            gt_heights = (assigned_annotations[:, 3] - assigned_annotations[:, 1]).clamp(min=1)
            gt_x = assigned_annotations[:, 0] + (gt_widths / 2)
            gt_y = assigned_annotations[:, 1] + (gt_heights / 2)

            # Generate the targets
            targets_dw = torch.log(gt_widths / anchors_widths)
            targets_dh = torch.log(gt_heights / anchors_heights)
            targets_dx = (gt_x - anchors_x) / anchors_widths
            targets_dy = (gt_y - anchors_y) / anchors_heights

            # Stack the targets as it could come from the regression module
            targets = torch.stack([targets_dx, targets_dy, targets_dw, targets_dh], dim=1)
            regression_diff = torch.abs(targets - regressions)
            # Compute Smooth L1 loss
            regression_loss = torch.where(
                regression_diff &lt;= 1 / (self.sigma**2),
                0.5 * (self.sigma * regression_diff) ** 2,
                regression_diff - (0.5 / (self.sigma ** 2))
            )

            # Append the squared error
            regression_losses.append(regression_loss.mean())

        # Return the mean classification loss and the mean regression loss
        return torch.stack(classification_losses).mean(), torch.stack(regression_losses).mean()</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="torchsight.losses.focal.FocalLoss.__init__"><code class="name flex">
<span>def <span class="ident">__init__</span></span>(<span>self, alpha=0.25, gamma=2.0, sigma=3.0, iou_thresholds=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Initialize the loss.</p>
<p>Train as background (minimize all the probabilities of the classes) if the IoU is below the 'background'
threshold and train with the label of the object if the IoU is over the 'object' threshold.
Ignore the anchors between both thresholds.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float</code></dt>
<dd>Alpha parameter for the focal loss.</dd>
<dt><strong><code>gamma</code></strong> :&ensp;<code>float</code></dt>
<dd>Gamma parameter for the focal loss.</dd>
<dt><strong><code>sigma</code></strong> :&ensp;<code>float</code></dt>
<dd>Point that defines the change from L1 loss to L2 loss (smooth L1).</dd>
<dt><strong><code>iou_thresholds</code></strong> :&ensp;<code>dict</code></dt>
<dd>Indicates the thresholds to assign an anchor as background or object.</dd>
<dt><strong><code>device</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Indicates the device where to run the loss.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def __init__(self, alpha=0.25, gamma=2.0, sigma=3.0, iou_thresholds=None):
    &#34;&#34;&#34;Initialize the loss.

    Train as background (minimize all the probabilities of the classes) if the IoU is below the &#39;background&#39;
    threshold and train with the label of the object if the IoU is over the &#39;object&#39; threshold.
    Ignore the anchors between both thresholds.

    Args:
        alpha (float): Alpha parameter for the focal loss.
        gamma (float): Gamma parameter for the focal loss.
        sigma (float): Point that defines the change from L1 loss to L2 loss (smooth L1).
        iou_thresholds (dict): Indicates the thresholds to assign an anchor as background or object.
        device (str, optional): Indicates the device where to run the loss.
    &#34;&#34;&#34;
    super(FocalLoss, self).__init__()

    self.alpha = alpha
    self.gamma = gamma
    self.sigma = sigma
    if iou_thresholds is None:
        iou_thresholds = {&#39;background&#39;: 0.4, &#39;object&#39;: 0.5}
    self.iou_background = iou_thresholds[&#39;background&#39;]
    self.iou_object = iou_thresholds[&#39;object&#39;]
    self.device = &#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39;</code></pre>
</details>
</dd>
<dt id="torchsight.losses.focal.FocalLoss.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, anchors, regressions, classifications, annotations)</span>
</code></dt>
<dd>
<section class="desc"><p>Forward pass to get the loss.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>anchors</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The base anchors (without the transformation to adjust the
bounding boxes).
Shape:
(batch size, total boxes, 4)</dd>
<dt><strong><code>regressions</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The regression values to adjust the anchors to the predicted
bounding boxes.
Shape:
(batch size, total boxes, 4)</dd>
<dt><strong><code>classifications</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The probabilities for each class at each bounding box.
Shape:
(batch size, total boxes, number of classes)</dd>
<dt><strong><code>annotations</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>
<p>Ground truth. Tensor with the bounding boxes and the label for
the object. The values must be x1, y1 (top left corner), x2, y2 (bottom right corner)
and the last value is the label.
Shape:
(batch size, maximum objects in any image, 5).</p>
<p>Why maximum objects in any image? Because if we have more than one image, each image
could have different amounts of objects inside and have different dimensions in the
ground truth (dim 1 of the batch). So we could have the maximum amount of objects
inside any image and then the rest of the images ground truths could be populated
with -1.0. So if this loss finds a ground truth box populated with -1.0 it understands
that it was to match the dimensions and have only one tensor.</p>
</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>torch.Tensor: The mean classification loss.
torch.Tensor: The mean regression loss.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def forward(self, anchors, regressions, classifications, annotations):
    &#34;&#34;&#34;Forward pass to get the loss.

    Args:
        anchors (torch.Tensor): The base anchors (without the transformation to adjust the
            bounding boxes).
            Shape:
                (batch size, total boxes, 4)
        regressions (torch.Tensor): The regression values to adjust the anchors to the predicted
            bounding boxes.
            Shape:
                (batch size, total boxes, 4)
        classifications (torch.Tensor): The probabilities for each class at each bounding box.
            Shape:
                (batch size, total boxes, number of classes)
        annotations (torch.Tensor): Ground truth. Tensor with the bounding boxes and the label for
            the object. The values must be x1, y1 (top left corner), x2, y2 (bottom right corner)
            and the last value is the label.
            Shape:
                (batch size, maximum objects in any image, 5).

            Why maximum objects in any image? Because if we have more than one image, each image
            could have different amounts of objects inside and have different dimensions in the
            ground truth (dim 1 of the batch). So we could have the maximum amount of objects
            inside any image and then the rest of the images ground truths could be populated
            with -1.0. So if this loss finds a ground truth box populated with -1.0 it understands
            that it was to match the dimensions and have only one tensor.

    Returns:
        torch.Tensor: The mean classification loss.
        torch.Tensor: The mean regression loss.
    &#34;&#34;&#34;
    batch_size = anchors.shape[0]
    batch_anchors = anchors
    batch_regressions = regressions
    batch_classifications = classifications
    batch_annotations = annotations

    classification_losses = []
    regression_losses = []

    for index in range(batch_size):
        anchors = batch_anchors[index]
        regressions = batch_regressions[index]
        classifications = batch_classifications[index]
        annotations = batch_annotations[index]
        # Keep only the real labels
        annotations = annotations[annotations[:, -1] != -1]

        if annotations.shape[0] == 0:
            classification_losses.append(torch.zeros((1,)).mean().to(self.device))
            regression_losses.append(torch.zeros((1,)).mean().to(self.device))
            continue

        # Get assignations of the annotations to the anchors
        # Get the assigned annotations (the i-th assigned annotation is the annotation assigned to the i-th
        # anchor)
        # Get the masks to select the anchors assigned to an object (IoU bigger than iou_object threshold)
        # Get the mask to select the anchors assigned to background (IoU lower than iou_background)
        assignations = Anchors.assign(anchors,
                                      annotations,
                                      thresholds={&#39;object&#39;: self.iou_object, &#39;background&#39;: self.iou_background})
        assigned_annotations, selected_anchors_objects, selected_anchors_backgrounds = assignations

        # Compute classification loss

        # Create the target tensor. Shape (number anchors, number of classes) where the
        # index of the class for the annotation has a 1 and all the others zero.
        n_classes = classifications.shape[1]
        targets = torch.zeros((anchors.shape[0], n_classes)).to(self.device)
        # Get the label for each anchor based on its assigned annotation ant turn it on. Do this only
        # for the assigned anchors.
        targets[selected_anchors_objects, assigned_annotations[selected_anchors_objects, 4].long()] = 1.
        # Avoid NaN in log clamping the classifications probabilities
        classifications = classifications.clamp(min=1e-5, max=1 - 1e-5)
        # Generate the alpha factor
        alpha = self.alpha * torch.ones(targets.shape).to(self.device)
        # It must be alpha for the correct label and 1 - alpha for the others
        alpha = torch.where(targets == 1, alpha, 1. - alpha)
        # Generate the focal weight
        focal = torch.where(targets == 1, 1 - classifications, classifications)
        focal = alpha * (focal ** self.gamma)
        # Get the binary cross entropy
        bce = -(targets * torch.log(classifications) + (1. - targets) * torch.log(1 - classifications))
        # Free memory
        del targets
        # Remove the loss for the not assigned anchors (not background, not object)
        selected_anchors = selected_anchors_backgrounds + selected_anchors_objects
        ignored_anchors = 1 - selected_anchors
        bce[ignored_anchors, :] = 0
        # Append to the classification losses
        classification_losses.append((focal * bce).sum() / selected_anchors.sum().type(torch.float))
        # Free memory
        del alpha, focal, bce, ignored_anchors, classifications, selected_anchors_backgrounds

        # Compute regression loss

        # Append zero loss if there is no object
        if not selected_anchors_objects.sum() &gt; 0:
            regression_losses.append(torch.zeros((1,)).mean().to(self.device))
            continue

        # Get the assigned ground truths to each one of the selected anchors to train the regression
        assigned_annotations = assigned_annotations[selected_anchors_objects, :-1]  # Shape (n selected anchors, 4)
        # Keep only the regressions for the selected anchors
        regressions = regressions[selected_anchors_objects, :]  # Shape (n selected anchors, 4)
        # Keep only the selected anchors for regression
        anchors = anchors[selected_anchors_objects, :]  # Shape (n selected anchors, 4)
        # Free memory
        del selected_anchors_objects

        # Get the parameters from the anchors
        anchors_widths = anchors[:, 2] - anchors[:, 0]
        anchors_heights = anchors[:, 3] - anchors[:, 1]
        anchors_x = anchors[:, 0] + (anchors_widths / 2)
        anchors_y = anchors[:, 1] + (anchors_heights / 2)

        # Get the parameters from the ground truth
        gt_widths = (assigned_annotations[:, 2] - assigned_annotations[:, 0]).clamp(min=1)
        gt_heights = (assigned_annotations[:, 3] - assigned_annotations[:, 1]).clamp(min=1)
        gt_x = assigned_annotations[:, 0] + (gt_widths / 2)
        gt_y = assigned_annotations[:, 1] + (gt_heights / 2)

        # Generate the targets
        targets_dw = torch.log(gt_widths / anchors_widths)
        targets_dh = torch.log(gt_heights / anchors_heights)
        targets_dx = (gt_x - anchors_x) / anchors_widths
        targets_dy = (gt_y - anchors_y) / anchors_heights

        # Stack the targets as it could come from the regression module
        targets = torch.stack([targets_dx, targets_dy, targets_dw, targets_dh], dim=1)
        regression_diff = torch.abs(targets - regressions)
        # Compute Smooth L1 loss
        regression_loss = torch.where(
            regression_diff &lt;= 1 / (self.sigma**2),
            0.5 * (self.sigma * regression_diff) ** 2,
            regression_diff - (0.5 / (self.sigma ** 2))
        )

        # Append the squared error
        regression_losses.append(regression_loss.mean())

    # Return the mean classification loss and the mean regression loss
    return torch.stack(classification_losses).mean(), torch.stack(regression_losses).mean()</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="torchsight.losses" href="index.html">torchsight.losses</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="torchsight.losses.focal.FocalLoss" href="#torchsight.losses.focal.FocalLoss">FocalLoss</a></code></h4>
<ul class="">
<li><code><a title="torchsight.losses.focal.FocalLoss.__init__" href="#torchsight.losses.focal.FocalLoss.__init__">__init__</a></code></li>
<li><code><a title="torchsight.losses.focal.FocalLoss.forward" href="#torchsight.losses.focal.FocalLoss.forward">forward</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.5.3</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>