<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.5.4" />
<title>torchsight.evaluators.dlde API documentation</title>
<meta name="description" content="Evaluators for the DLDENet models." />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.name small{font-weight:normal}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase;cursor:pointer}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title"><code>torchsight.evaluators.dlde</code> module</h1>
</header>
<section id="section-intro">
<p>Evaluators for the DLDENet models.</p>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">&#34;&#34;&#34;Evaluators for the DLDENet models.&#34;&#34;&#34;
import json
import os

import torch
from torch.utils.data import DataLoader
from torchvision import transforms

from torchsight.datasets import CocoDataset
from torchsight.models import DLDENet, DLDENetWithTrackedMeans
from torchsight.transforms.detection import Normalize, Resize, ToTensor
from torchsight.utils import merge_dicts

from .evaluator import Evaluator


class DLDENetEvaluator(Evaluator):
    &#34;&#34;&#34;An evaluator for the DLDENet.

    It will evaluate the model computing the mAP over the coco valid dataset.
    &#34;&#34;&#34;
    params = {&#39;results&#39;: {&#39;dir&#39;: &#39;./evaluations/dldenet/coco&#39;, &#39;file&#39;: &#39;val2017.json&#39;},
              &#39;dataset&#39;: {&#39;root&#39;: &#39;./datasets/coco&#39;,
                          &#39;validation&#39;: &#39;val2017&#39;,
                          &#39;class_names&#39;: (),
                          # Try to load the classes names from the checkpoint file
                          &#39;class_names_from_checkpoint&#39;: True},
              &#39;dataloader&#39;: {&#39;batch_size&#39;: 8,
                             &#39;shuffle&#39;: False,
                             &#39;num_workers&#39;: 8},
              &#39;model&#39;: {&#39;with_tracked_means&#39;: False,
                        &#39;evaluation&#39;: {&#39;threshold&#39;: 0.5, &#39;iou_threshold&#39;: 0.5},
                        # As the tracked version was created when the trainer didn&#39;t save the hyperparameters
                        # we must provide them
                        &#39;tracked&#39;: {&#39;classes&#39;: 80,
                                    &#39;resnet&#39;: 50,
                                    &#39;features&#39;: {&#39;pyramid&#39;: 256,
                                                 &#39;regression&#39;: 256,
                                                 &#39;classification&#39;: 256},
                                    &#39;anchors&#39;: {&#39;sizes&#39;: [32, 64, 128, 256, 512],
                                                &#39;scales&#39;: [2 ** 0, 2 ** (1/3), 2 ** (2/3)],
                                                &#39;ratios&#39;: [0.5, 1, 2]},
                                    &#39;embedding_size&#39;: 256,
                                    &#39;concentration&#39;: 15,
                                    &#39;shift&#39;: 0.8}},
              # We don&#39;t have params from the transforms because we load the params from the checkpoint,
              # but we can edit this params writing them here
              &#39;transforms&#39;: {}}

    def __init__(self, *args, **kwargs):
        &#34;&#34;&#34;Initialize the evaluator.

        Set the initial list with the predictions.
        &#34;&#34;&#34;
        self.predictions = []

        super().__init__(*args, **kwargs)

    ###############################
    ###         GETTERS         ###
    ###############################

    def get_transform(self):
        &#34;&#34;&#34;Get the transformations to apply to the dataset.

        Returns:
            torchvision.transforms.Compose: A composition of the transformations to apply.
        &#34;&#34;&#34;
        params = torch.load(self.checkpoint)[&#39;hyperparameters&#39;][&#39;transforms&#39;]
        params = merge_dicts(params, self.params[&#39;transforms&#39;], verbose=True)

        return transforms.Compose([Resize(**params[&#39;resize&#39;]),
                                   ToTensor(),
                                   Normalize(**params[&#39;normalize&#39;])])

    def get_dataset(self):
        &#34;&#34;&#34;Get the COCO dataset for the evaluation.

        Returns:
            torch.utils.data.Dataset: The dataset to use for the evaluation.
        &#34;&#34;&#34;
        params = self.params[&#39;dataset&#39;]
        transform = self.get_transform()
        class_names = params[&#39;class_names&#39;]

        if params[&#39;class_names_from_checkpoint&#39;]:
            checkpoint = torch.load(self.checkpoint)
            if &#39;hyperparameters&#39; in checkpoint:
                class_names = checkpoint[&#39;hyperparameters&#39;][&#39;datasets&#39;][&#39;class_names&#39;]
            else:
                print(&#34;Couldn&#39;t load the class_names from the checkpoint, it doesn&#39;t have the hyperparameters.&#34;)

        return CocoDataset(
            root=params[&#39;root&#39;],
            dataset=params[&#39;validation&#39;],
            classes_names=class_names,
            transform=transform)

    def get_dataloader(self):
        &#34;&#34;&#34;Get the dataloader to use for the evaluation.

        Returns:
            torch.utils.data.Dataloader: The dataloader to use for the evaluation.
        &#34;&#34;&#34;
        def collate(data):
            &#34;&#34;&#34;Custom collate function to join the different images.

            It pads the images so all has the same size.

            Arguments:
                data (sequence): Sequence of tuples as (image, annotations, images&#39; infos, *_).

            Returns:
                torch.Tensor: The images.
                    Shape: (batch size, channels, height, width)
            &#34;&#34;&#34;
            images = [image for image, *_ in data]
            max_width = max([image.shape[-1] for image in images])
            max_height = max([image.shape[-2] for image in images])

            def pad_image(image):
                aux = torch.zeros((image.shape[0], max_height, max_width))
                aux[:, :image.shape[1], :image.shape[2]] = image
                return aux

            images = torch.stack([pad_image(image) for image, *_ in data], dim=0)
            infos = [info for _, _, info, *_ in data]

            return images, infos

        return DataLoader(dataset=self.dataset, collate_fn=collate, **self.params[&#39;dataloader&#39;])

    def get_model(self):
        &#34;&#34;&#34;Get the model to use to make the predictions.

        We can use the DLDENet with tracked means or the weighted version by changing
        the flag params[&#39;model&#39;][&#39;with_tracked_means&#39;].

        Returns:
            torch.nn.Module: The model to use to make the predictions over the data.
        &#34;&#34;&#34;
        if self.params[&#39;model&#39;][&#39;with_tracked_means&#39;]:
            params = {**self.params[&#39;model&#39;][&#39;tracked&#39;], &#39;device&#39;: self.device}
            state_dict = torch.load(self.checkpoint, map_location=self.device)[&#39;DLDENet&#39;]
            return DLDENetWithTrackedMeans(**params).load_state_dict(state_dict)

        return DLDENet.from_checkpoint(self.checkpoint, self.device)

    ###############################
    ###         METHODS         ###
    ###############################

    def eval(self):
        &#34;&#34;&#34;Put the model in evaluation mode and set the threshold for the detection.&#34;&#34;&#34;
        params = self.params[&#39;model&#39;][&#39;evaluation&#39;]
        self.model.eval(threshold=params[&#39;threshold&#39;], iou_threshold=params[&#39;iou_threshold&#39;])

    @staticmethod
    def transform_boxes(boxes, info):
        &#34;&#34;&#34;Transform the bounding boxes from x1, y1, x2, y2 to x, y, width, height.

        As the images were transformed using the Resize transformation we need to get the scale
        used to update the boxes to use the same scale to revert the prediction to the scale
        of the original annotations.
        That scale is stored in the info of the image as info[&#39;resize_scale&#39;].

        Arguments:
            boxes (torch.Tensor): A tensor with shape (n predictions, 4).
            info (dict): The information of the image that contains the original height and width.

        Returns:
            torch.Tensor: The transformed bounding boxes.
                Shape: (n predictions, 4)
        &#34;&#34;&#34;
        # Update the scale
        if &#39;resize_scale&#39; in info:
            boxes /= info[&#39;resize_scale&#39;]

        x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
        w, h = x2 - x1, y2 - y1

        return torch.stack([x1, y1, w, h], dim=1)

    def forward(self, images, infos, *_):
        &#34;&#34;&#34;Forward pass through the network.

        Here we make the predictions over the images.

        Arguments:
            images (torch.Tensor): The tensor with the batch of images where to make predictions.
            infos (list): A list with the info of each image.
        &#34;&#34;&#34;
        # Get the list of tuples (boxes, classifications)
        # With shapes (n predictions, 4) and (n predictions, n classes)
        predictions = self.model(images.to(self.device))
        for i, (boxes, classifications) in enumerate(predictions):
            # Check if there are no detections
            if boxes.shape[0] == 0:
                continue

            scores, labels = classifications.max(dim=1)
            boxes = self.transform_boxes(boxes, infos[i])
            image_id = infos[i][&#39;id&#39;]

            for j, box in enumerate(boxes):
                score, label = scores[j], labels[j]
                try:
                    category_id = self.dataset.classes[&#39;ids&#39;][int(label)]
                except KeyError:
                    # The model predicted a class that is not present in the dataset
                    continue
                self.predictions.append({&#39;image_id&#39;: image_id,
                                         &#39;category_id&#39;: category_id,
                                         &#39;bbox&#39;: [float(point) for point in box],
                                         &#39;score&#39;: float(score)})

    def evaluate_callback(self):
        &#34;&#34;&#34;After the finish of the evaluation store the predictions in the results directory
        and use the pycocotools to compute the mAP.
        &#34;&#34;&#34;
        result_dir = self.params[&#39;results&#39;][&#39;dir&#39;]
        file = self.params[&#39;results&#39;][&#39;file&#39;]
        file_path = os.path.join(result_dir, file)

        if not os.path.exists(result_dir):
            os.makedirs(result_dir)

        with open(file_path, &#39;w&#39;) as file:
            file.write(json.dumps(self.predictions))

        self.dataset.compute_map(file_path)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="torchsight.evaluators.dlde.DLDENetEvaluator"><code class="flex name class">
<span>class <span class="ident">DLDENetEvaluator</span></span>
<span>(</span><span><small>ancestors:</small> <a title="torchsight.evaluators.evaluator.Evaluator" href="evaluator.html#torchsight.evaluators.evaluator.Evaluator">Evaluator</a>)</span>
</code></dt>
<dd>
<section class="desc"><p>An evaluator for the DLDENet.</p>
<p>It will evaluate the model computing the mAP over the coco valid dataset.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class DLDENetEvaluator(Evaluator):
    &#34;&#34;&#34;An evaluator for the DLDENet.

    It will evaluate the model computing the mAP over the coco valid dataset.
    &#34;&#34;&#34;
    params = {&#39;results&#39;: {&#39;dir&#39;: &#39;./evaluations/dldenet/coco&#39;, &#39;file&#39;: &#39;val2017.json&#39;},
              &#39;dataset&#39;: {&#39;root&#39;: &#39;./datasets/coco&#39;,
                          &#39;validation&#39;: &#39;val2017&#39;,
                          &#39;class_names&#39;: (),
                          # Try to load the classes names from the checkpoint file
                          &#39;class_names_from_checkpoint&#39;: True},
              &#39;dataloader&#39;: {&#39;batch_size&#39;: 8,
                             &#39;shuffle&#39;: False,
                             &#39;num_workers&#39;: 8},
              &#39;model&#39;: {&#39;with_tracked_means&#39;: False,
                        &#39;evaluation&#39;: {&#39;threshold&#39;: 0.5, &#39;iou_threshold&#39;: 0.5},
                        # As the tracked version was created when the trainer didn&#39;t save the hyperparameters
                        # we must provide them
                        &#39;tracked&#39;: {&#39;classes&#39;: 80,
                                    &#39;resnet&#39;: 50,
                                    &#39;features&#39;: {&#39;pyramid&#39;: 256,
                                                 &#39;regression&#39;: 256,
                                                 &#39;classification&#39;: 256},
                                    &#39;anchors&#39;: {&#39;sizes&#39;: [32, 64, 128, 256, 512],
                                                &#39;scales&#39;: [2 ** 0, 2 ** (1/3), 2 ** (2/3)],
                                                &#39;ratios&#39;: [0.5, 1, 2]},
                                    &#39;embedding_size&#39;: 256,
                                    &#39;concentration&#39;: 15,
                                    &#39;shift&#39;: 0.8}},
              # We don&#39;t have params from the transforms because we load the params from the checkpoint,
              # but we can edit this params writing them here
              &#39;transforms&#39;: {}}

    def __init__(self, *args, **kwargs):
        &#34;&#34;&#34;Initialize the evaluator.

        Set the initial list with the predictions.
        &#34;&#34;&#34;
        self.predictions = []

        super().__init__(*args, **kwargs)

    ###############################
    ###         GETTERS         ###
    ###############################

    def get_transform(self):
        &#34;&#34;&#34;Get the transformations to apply to the dataset.

        Returns:
            torchvision.transforms.Compose: A composition of the transformations to apply.
        &#34;&#34;&#34;
        params = torch.load(self.checkpoint)[&#39;hyperparameters&#39;][&#39;transforms&#39;]
        params = merge_dicts(params, self.params[&#39;transforms&#39;], verbose=True)

        return transforms.Compose([Resize(**params[&#39;resize&#39;]),
                                   ToTensor(),
                                   Normalize(**params[&#39;normalize&#39;])])

    def get_dataset(self):
        &#34;&#34;&#34;Get the COCO dataset for the evaluation.

        Returns:
            torch.utils.data.Dataset: The dataset to use for the evaluation.
        &#34;&#34;&#34;
        params = self.params[&#39;dataset&#39;]
        transform = self.get_transform()
        class_names = params[&#39;class_names&#39;]

        if params[&#39;class_names_from_checkpoint&#39;]:
            checkpoint = torch.load(self.checkpoint)
            if &#39;hyperparameters&#39; in checkpoint:
                class_names = checkpoint[&#39;hyperparameters&#39;][&#39;datasets&#39;][&#39;class_names&#39;]
            else:
                print(&#34;Couldn&#39;t load the class_names from the checkpoint, it doesn&#39;t have the hyperparameters.&#34;)

        return CocoDataset(
            root=params[&#39;root&#39;],
            dataset=params[&#39;validation&#39;],
            classes_names=class_names,
            transform=transform)

    def get_dataloader(self):
        &#34;&#34;&#34;Get the dataloader to use for the evaluation.

        Returns:
            torch.utils.data.Dataloader: The dataloader to use for the evaluation.
        &#34;&#34;&#34;
        def collate(data):
            &#34;&#34;&#34;Custom collate function to join the different images.

            It pads the images so all has the same size.

            Arguments:
                data (sequence): Sequence of tuples as (image, annotations, images&#39; infos, *_).

            Returns:
                torch.Tensor: The images.
                    Shape: (batch size, channels, height, width)
            &#34;&#34;&#34;
            images = [image for image, *_ in data]
            max_width = max([image.shape[-1] for image in images])
            max_height = max([image.shape[-2] for image in images])

            def pad_image(image):
                aux = torch.zeros((image.shape[0], max_height, max_width))
                aux[:, :image.shape[1], :image.shape[2]] = image
                return aux

            images = torch.stack([pad_image(image) for image, *_ in data], dim=0)
            infos = [info for _, _, info, *_ in data]

            return images, infos

        return DataLoader(dataset=self.dataset, collate_fn=collate, **self.params[&#39;dataloader&#39;])

    def get_model(self):
        &#34;&#34;&#34;Get the model to use to make the predictions.

        We can use the DLDENet with tracked means or the weighted version by changing
        the flag params[&#39;model&#39;][&#39;with_tracked_means&#39;].

        Returns:
            torch.nn.Module: The model to use to make the predictions over the data.
        &#34;&#34;&#34;
        if self.params[&#39;model&#39;][&#39;with_tracked_means&#39;]:
            params = {**self.params[&#39;model&#39;][&#39;tracked&#39;], &#39;device&#39;: self.device}
            state_dict = torch.load(self.checkpoint, map_location=self.device)[&#39;DLDENet&#39;]
            return DLDENetWithTrackedMeans(**params).load_state_dict(state_dict)

        return DLDENet.from_checkpoint(self.checkpoint, self.device)

    ###############################
    ###         METHODS         ###
    ###############################

    def eval(self):
        &#34;&#34;&#34;Put the model in evaluation mode and set the threshold for the detection.&#34;&#34;&#34;
        params = self.params[&#39;model&#39;][&#39;evaluation&#39;]
        self.model.eval(threshold=params[&#39;threshold&#39;], iou_threshold=params[&#39;iou_threshold&#39;])

    @staticmethod
    def transform_boxes(boxes, info):
        &#34;&#34;&#34;Transform the bounding boxes from x1, y1, x2, y2 to x, y, width, height.

        As the images were transformed using the Resize transformation we need to get the scale
        used to update the boxes to use the same scale to revert the prediction to the scale
        of the original annotations.
        That scale is stored in the info of the image as info[&#39;resize_scale&#39;].

        Arguments:
            boxes (torch.Tensor): A tensor with shape (n predictions, 4).
            info (dict): The information of the image that contains the original height and width.

        Returns:
            torch.Tensor: The transformed bounding boxes.
                Shape: (n predictions, 4)
        &#34;&#34;&#34;
        # Update the scale
        if &#39;resize_scale&#39; in info:
            boxes /= info[&#39;resize_scale&#39;]

        x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
        w, h = x2 - x1, y2 - y1

        return torch.stack([x1, y1, w, h], dim=1)

    def forward(self, images, infos, *_):
        &#34;&#34;&#34;Forward pass through the network.

        Here we make the predictions over the images.

        Arguments:
            images (torch.Tensor): The tensor with the batch of images where to make predictions.
            infos (list): A list with the info of each image.
        &#34;&#34;&#34;
        # Get the list of tuples (boxes, classifications)
        # With shapes (n predictions, 4) and (n predictions, n classes)
        predictions = self.model(images.to(self.device))
        for i, (boxes, classifications) in enumerate(predictions):
            # Check if there are no detections
            if boxes.shape[0] == 0:
                continue

            scores, labels = classifications.max(dim=1)
            boxes = self.transform_boxes(boxes, infos[i])
            image_id = infos[i][&#39;id&#39;]

            for j, box in enumerate(boxes):
                score, label = scores[j], labels[j]
                try:
                    category_id = self.dataset.classes[&#39;ids&#39;][int(label)]
                except KeyError:
                    # The model predicted a class that is not present in the dataset
                    continue
                self.predictions.append({&#39;image_id&#39;: image_id,
                                         &#39;category_id&#39;: category_id,
                                         &#39;bbox&#39;: [float(point) for point in box],
                                         &#39;score&#39;: float(score)})

    def evaluate_callback(self):
        &#34;&#34;&#34;After the finish of the evaluation store the predictions in the results directory
        and use the pycocotools to compute the mAP.
        &#34;&#34;&#34;
        result_dir = self.params[&#39;results&#39;][&#39;dir&#39;]
        file = self.params[&#39;results&#39;][&#39;file&#39;]
        file_path = os.path.join(result_dir, file)

        if not os.path.exists(result_dir):
            os.makedirs(result_dir)

        with open(file_path, &#39;w&#39;) as file:
            file.write(json.dumps(self.predictions))

        self.dataset.compute_map(file_path)</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="torchsight.evaluators.dlde.DLDENetEvaluator.params"><code class="name">var <span class="ident">params</span></code></dt>
<dd>
<section class="desc"></section>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="torchsight.evaluators.dlde.DLDENetEvaluator.transform_boxes"><code class="name flex">
<span>def <span class="ident">transform_boxes</span></span>(<span>boxes, info)</span>
</code></dt>
<dd>
<section class="desc"><p>Transform the bounding boxes from x1, y1, x2, y2 to x, y, width, height.</p>
<p>As the images were transformed using the Resize transformation we need to get the scale
used to update the boxes to use the same scale to revert the prediction to the scale
of the original annotations.
That scale is stored in the info of the image as info['resize_scale'].</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>boxes</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>A tensor with shape (n predictions, 4).</dd>
<dt><strong><code>info</code></strong> :&ensp;<code>dict</code></dt>
<dd>The information of the image that contains the original height and width.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>torch.Tensor: The transformed bounding boxes.
Shape: (n predictions, 4)</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">@staticmethod
def transform_boxes(boxes, info):
    &#34;&#34;&#34;Transform the bounding boxes from x1, y1, x2, y2 to x, y, width, height.

    As the images were transformed using the Resize transformation we need to get the scale
    used to update the boxes to use the same scale to revert the prediction to the scale
    of the original annotations.
    That scale is stored in the info of the image as info[&#39;resize_scale&#39;].

    Arguments:
        boxes (torch.Tensor): A tensor with shape (n predictions, 4).
        info (dict): The information of the image that contains the original height and width.

    Returns:
        torch.Tensor: The transformed bounding boxes.
            Shape: (n predictions, 4)
    &#34;&#34;&#34;
    # Update the scale
    if &#39;resize_scale&#39; in info:
        boxes /= info[&#39;resize_scale&#39;]

    x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]
    w, h = x2 - x1, y2 - y1

    return torch.stack([x1, y1, w, h], dim=1)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="torchsight.evaluators.dlde.DLDENetEvaluator.__init__"><code class="name flex">
<span>def <span class="ident">__init__</span></span>(<span>self, *args, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Initialize the evaluator.</p>
<p>Set the initial list with the predictions.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def __init__(self, *args, **kwargs):
    &#34;&#34;&#34;Initialize the evaluator.

    Set the initial list with the predictions.
    &#34;&#34;&#34;
    self.predictions = []

    super().__init__(*args, **kwargs)</code></pre>
</details>
</dd>
<dt id="torchsight.evaluators.dlde.DLDENetEvaluator.eval"><code class="name flex">
<span>def <span class="ident">eval</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Put the model in evaluation mode and set the threshold for the detection.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def eval(self):
    &#34;&#34;&#34;Put the model in evaluation mode and set the threshold for the detection.&#34;&#34;&#34;
    params = self.params[&#39;model&#39;][&#39;evaluation&#39;]
    self.model.eval(threshold=params[&#39;threshold&#39;], iou_threshold=params[&#39;iou_threshold&#39;])</code></pre>
</details>
</dd>
<dt id="torchsight.evaluators.dlde.DLDENetEvaluator.evaluate_callback"><code class="name flex">
<span>def <span class="ident">evaluate_callback</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>After the finish of the evaluation store the predictions in the results directory
and use the pycocotools to compute the mAP.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def evaluate_callback(self):
    &#34;&#34;&#34;After the finish of the evaluation store the predictions in the results directory
    and use the pycocotools to compute the mAP.
    &#34;&#34;&#34;
    result_dir = self.params[&#39;results&#39;][&#39;dir&#39;]
    file = self.params[&#39;results&#39;][&#39;file&#39;]
    file_path = os.path.join(result_dir, file)

    if not os.path.exists(result_dir):
        os.makedirs(result_dir)

    with open(file_path, &#39;w&#39;) as file:
        file.write(json.dumps(self.predictions))

    self.dataset.compute_map(file_path)</code></pre>
</details>
</dd>
<dt id="torchsight.evaluators.dlde.DLDENetEvaluator.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, images, infos, *_)</span>
</code></dt>
<dd>
<section class="desc"><p>Forward pass through the network.</p>
<p>Here we make the predictions over the images.</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>images</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The tensor with the batch of images where to make predictions.</dd>
<dt><strong><code>infos</code></strong> :&ensp;<code>list</code></dt>
<dd>A list with the info of each image.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def forward(self, images, infos, *_):
    &#34;&#34;&#34;Forward pass through the network.

    Here we make the predictions over the images.

    Arguments:
        images (torch.Tensor): The tensor with the batch of images where to make predictions.
        infos (list): A list with the info of each image.
    &#34;&#34;&#34;
    # Get the list of tuples (boxes, classifications)
    # With shapes (n predictions, 4) and (n predictions, n classes)
    predictions = self.model(images.to(self.device))
    for i, (boxes, classifications) in enumerate(predictions):
        # Check if there are no detections
        if boxes.shape[0] == 0:
            continue

        scores, labels = classifications.max(dim=1)
        boxes = self.transform_boxes(boxes, infos[i])
        image_id = infos[i][&#39;id&#39;]

        for j, box in enumerate(boxes):
            score, label = scores[j], labels[j]
            try:
                category_id = self.dataset.classes[&#39;ids&#39;][int(label)]
            except KeyError:
                # The model predicted a class that is not present in the dataset
                continue
            self.predictions.append({&#39;image_id&#39;: image_id,
                                     &#39;category_id&#39;: category_id,
                                     &#39;bbox&#39;: [float(point) for point in box],
                                     &#39;score&#39;: float(score)})</code></pre>
</details>
</dd>
<dt id="torchsight.evaluators.dlde.DLDENetEvaluator.get_dataset"><code class="name flex">
<span>def <span class="ident">get_dataset</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Get the COCO dataset for the evaluation.</p>
<h2 id="returns">Returns</h2>
<p>torch.utils.data.Dataset: The dataset to use for the evaluation.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_dataset(self):
    &#34;&#34;&#34;Get the COCO dataset for the evaluation.

    Returns:
        torch.utils.data.Dataset: The dataset to use for the evaluation.
    &#34;&#34;&#34;
    params = self.params[&#39;dataset&#39;]
    transform = self.get_transform()
    class_names = params[&#39;class_names&#39;]

    if params[&#39;class_names_from_checkpoint&#39;]:
        checkpoint = torch.load(self.checkpoint)
        if &#39;hyperparameters&#39; in checkpoint:
            class_names = checkpoint[&#39;hyperparameters&#39;][&#39;datasets&#39;][&#39;class_names&#39;]
        else:
            print(&#34;Couldn&#39;t load the class_names from the checkpoint, it doesn&#39;t have the hyperparameters.&#34;)

    return CocoDataset(
        root=params[&#39;root&#39;],
        dataset=params[&#39;validation&#39;],
        classes_names=class_names,
        transform=transform)</code></pre>
</details>
</dd>
<dt id="torchsight.evaluators.dlde.DLDENetEvaluator.get_model"><code class="name flex">
<span>def <span class="ident">get_model</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Get the model to use to make the predictions.</p>
<p>We can use the DLDENet with tracked means or the weighted version by changing
the flag params['model']['with_tracked_means'].</p>
<h2 id="returns">Returns</h2>
<p>torch.nn.Module: The model to use to make the predictions over the data.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_model(self):
    &#34;&#34;&#34;Get the model to use to make the predictions.

    We can use the DLDENet with tracked means or the weighted version by changing
    the flag params[&#39;model&#39;][&#39;with_tracked_means&#39;].

    Returns:
        torch.nn.Module: The model to use to make the predictions over the data.
    &#34;&#34;&#34;
    if self.params[&#39;model&#39;][&#39;with_tracked_means&#39;]:
        params = {**self.params[&#39;model&#39;][&#39;tracked&#39;], &#39;device&#39;: self.device}
        state_dict = torch.load(self.checkpoint, map_location=self.device)[&#39;DLDENet&#39;]
        return DLDENetWithTrackedMeans(**params).load_state_dict(state_dict)

    return DLDENet.from_checkpoint(self.checkpoint, self.device)</code></pre>
</details>
</dd>
<dt id="torchsight.evaluators.dlde.DLDENetEvaluator.get_transform"><code class="name flex">
<span>def <span class="ident">get_transform</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Get the transformations to apply to the dataset.</p>
<h2 id="returns">Returns</h2>
<p>torchvision.transforms.Compose: A composition of the transformations to apply.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_transform(self):
    &#34;&#34;&#34;Get the transformations to apply to the dataset.

    Returns:
        torchvision.transforms.Compose: A composition of the transformations to apply.
    &#34;&#34;&#34;
    params = torch.load(self.checkpoint)[&#39;hyperparameters&#39;][&#39;transforms&#39;]
    params = merge_dicts(params, self.params[&#39;transforms&#39;], verbose=True)

    return transforms.Compose([Resize(**params[&#39;resize&#39;]),
                               ToTensor(),
                               Normalize(**params[&#39;normalize&#39;])])</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="torchsight.evaluators.evaluator.Evaluator" href="evaluator.html#torchsight.evaluators.evaluator.Evaluator">Evaluator</a></b></code>:
<ul class="hlist">
<li><code><a title="torchsight.evaluators.evaluator.Evaluator.batch_callback" href="evaluator.html#torchsight.evaluators.evaluator.Evaluator.batch_callback">batch_callback</a></code></li>
<li><code><a title="torchsight.evaluators.evaluator.Evaluator.evaluate" href="evaluator.html#torchsight.evaluators.evaluator.Evaluator.evaluate">evaluate</a></code></li>
<li><code><a title="torchsight.evaluators.evaluator.Evaluator.get_dataloader" href="evaluator.html#torchsight.evaluators.evaluator.Evaluator.get_dataloader">get_dataloader</a></code></li>
<li><code><a title="torchsight.evaluators.evaluator.Evaluator.get_logger" href="evaluator.html#torchsight.evaluators.evaluator.Evaluator.get_logger">get_logger</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="torchsight.evaluators" href="index.html">torchsight.evaluators</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="torchsight.evaluators.dlde.DLDENetEvaluator" href="#torchsight.evaluators.dlde.DLDENetEvaluator">DLDENetEvaluator</a></code></h4>
<ul class="two-column">
<li><code><a title="torchsight.evaluators.dlde.DLDENetEvaluator.__init__" href="#torchsight.evaluators.dlde.DLDENetEvaluator.__init__">__init__</a></code></li>
<li><code><a title="torchsight.evaluators.dlde.DLDENetEvaluator.eval" href="#torchsight.evaluators.dlde.DLDENetEvaluator.eval">eval</a></code></li>
<li><code><a title="torchsight.evaluators.dlde.DLDENetEvaluator.evaluate_callback" href="#torchsight.evaluators.dlde.DLDENetEvaluator.evaluate_callback">evaluate_callback</a></code></li>
<li><code><a title="torchsight.evaluators.dlde.DLDENetEvaluator.forward" href="#torchsight.evaluators.dlde.DLDENetEvaluator.forward">forward</a></code></li>
<li><code><a title="torchsight.evaluators.dlde.DLDENetEvaluator.get_dataset" href="#torchsight.evaluators.dlde.DLDENetEvaluator.get_dataset">get_dataset</a></code></li>
<li><code><a title="torchsight.evaluators.dlde.DLDENetEvaluator.get_model" href="#torchsight.evaluators.dlde.DLDENetEvaluator.get_model">get_model</a></code></li>
<li><code><a title="torchsight.evaluators.dlde.DLDENetEvaluator.get_transform" href="#torchsight.evaluators.dlde.DLDENetEvaluator.get_transform">get_transform</a></code></li>
<li><code><a title="torchsight.evaluators.dlde.DLDENetEvaluator.params" href="#torchsight.evaluators.dlde.DLDENetEvaluator.params">params</a></code></li>
<li><code><a title="torchsight.evaluators.dlde.DLDENetEvaluator.transform_boxes" href="#torchsight.evaluators.dlde.DLDENetEvaluator.transform_boxes">transform_boxes</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.5.4</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>