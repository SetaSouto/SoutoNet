<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.5.4" />
<title>torchsight.evaluators.flickr32.evaluator API documentation</title>
<meta name="description" content="Module with an evaluator for the Flickr32 dataset." />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.name small{font-weight:normal}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase;cursor:pointer}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title"><code>torchsight.evaluators.flickr32.evaluator</code> module</h1>
</header>
<section id="section-intro">
<p>Module with an evaluator for the Flickr32 dataset.</p>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">&#34;&#34;&#34;Module with an evaluator for the Flickr32 dataset.&#34;&#34;&#34;
import torch
from torch.utils.data import DataLoader

from torchsight.datasets import Flickr32Dataset
from torchsight.utils import merge_dicts

from ..evaluator import Evaluator
from .fl_eval_classification import fl_eval_classification


class Flickr32Evaluator(Evaluator):
    &#34;&#34;&#34;An evaluator for the Flickr32 dataset.

    You must extend this evaluator and override the `get_model()` method to use a custom model
    to perform the evaluation and `get_transform()` to use the transformation of the images
    as your model needs.

    The model should return the name of the brand for each image (or none if the image has
    no logo) and the probability of that prediction.
    You could override the method `predict()` to perform that task.
    &#34;&#34;&#34;

    def __init__(self, *args, **kwargs):
        &#34;&#34;&#34;Initialize the evaluator.&#34;&#34;&#34;
        self.processed = 0  # Number of processed images
        self.detected = 0  # Number of images with logos detected
        self.predictions = []

        super().__init__(*args, **kwargs)

    @staticmethod
    def get_base_params():
        &#34;&#34;&#34;Get the base parameters for the evaluator.&#34;&#34;&#34;
        return merge_dicts(
            super(Flickr32Evaluator, Flickr32Evaluator).get_base_params(),
            {
                &#39;root&#39;: &#39;./datasets/flickr32&#39;,
                &#39;file&#39;: &#39;./flickr32_predictions.csv&#39;,
                &#39;dataloader&#39;: {
                    &#39;num_workers&#39;: 8,
                    &#39;shuffle&#39;: False,
                    &#39;batch_size&#39;: 8
                }
            })

    ###############################
    ###         GETTERS         ###
    ###############################

    def get_transform(self):
        &#34;&#34;&#34;Get the transformation to applies to the dataset according to the model.&#34;&#34;&#34;
        raise NotImplementedError()

    def get_model(self):
        &#34;&#34;&#34;Get the model that makes the predictions.&#34;&#34;&#34;
        raise NotImplementedError()

    def get_dataset(self):
        &#34;&#34;&#34;Get the dataset for the evaluation.

        Returns:
            torch.utils.data.Dataset: The dataset to use for the evaluation.
        &#34;&#34;&#34;
        transform = self.get_transform()

        try:
            params = self.checkpoint[&#39;hyperparameters&#39;][&#39;datasets&#39;][&#39;flickr32&#39;]
        except KeyError:
            # The model was not trained over flickr32 dataset
            params = {&#39;classes&#39;: None}

        return Flickr32Dataset(root=self.params[&#39;root&#39;], classes=params[&#39;classes&#39;], only_boxes=False,
                               dataset=&#39;test&#39;, transform=transform)

    def get_dataloader(self):
        &#34;&#34;&#34;Generate the custom dataloaders for the evaluation.

        Returns:
            torch.utils.data.Dataloaders: The dataloader for the validation.
        &#34;&#34;&#34;
        def collate(data):
            &#34;&#34;&#34;Custom collate function to join the images and get the name of the images.

            Arguments:
                data (sequence): Sequence of tuples as (image, _, info).

            Returns:
                torch.Tensor: The images.
                    Shape:
                        (batch size, channels, height, width)
                list of dicts: The filename of the each image.
            &#34;&#34;&#34;
            images = [image for image, *_ in data]
            max_width = max([image.shape[-1] for image in images])
            max_height = max([image.shape[-2] for image in images])

            def pad_image(image):
                aux = torch.zeros((image.shape[0], max_height, max_width))
                aux[:, :image.shape[1], :image.shape[2]] = image
                return aux

            images = torch.stack([pad_image(image) for image, *_ in data], dim=0)
            files = [info[&#39;image&#39;].split(&#39;/&#39;)[-1].replace(&#39;.jpg&#39;, &#39;&#39;) for _, _, info in data]

            return images, files

        hyperparameters = {**self.params[&#39;dataloader&#39;], &#39;collate_fn&#39;: collate}
        return DataLoader(**hyperparameters, dataset=self.dataset)

    ###############################
    ###         METHODS         ###
    ###############################

    def predict(self, images, files):
        &#34;&#34;&#34;Make a predictions for the given images.

        It assumes that the model make predictions and returns a list of tensors with shape:
        `(num bounding boxes, 6)`.
        For each prediction contains x1, y1, x2, y2, label, probability.

        So this method keep only the maximum annotation and generates the tuples.

        If your model does not follow this structure you can override this method.

        Arguments:
            images (torch.Tensor): The batch of images to make predictions on.
            infos (list of dict): A list of the dicts generated by the dataset.
                See __getitem__ method in the dataste for more information.

        Returns:
            list of tuples: Each tuple contains the name of the brand and the probability of
                the prediction.
                If the prediction is that there is no logo in the image it returns None as brand.
        &#34;&#34;&#34;
        detections_list = self.model(images)
        predictions = []

        for i, detections in enumerate(detections_list):
            self.processed += 1
            if detections.shape[0] &gt; 0:
                self.detected += 1
                probs = detections[:, 5]
                prob, index = probs.max(dim=0)
                label = detections[index, 4]
                brand = self.dataset.label_to_class[int(label.long())]
            else:
                brand = &#39;no-logo&#39;
                prob = 1.0

            predictions.append((files[i], brand, &#39;{:.1f}&#39;.format(float(prob))))

        return predictions

    def forward(self, images, infos):
        &#34;&#34;&#34;Forward pass through the model.

        Make the predictions and add it to the predictions variable.
        &#34;&#34;&#34;
        images = images.to(self.device)
        self.predictions += self.predict(images, infos)
        self.current_log[&#39;Processed&#39;] = self.processed
        self.current_log[&#39;Detected&#39;] = self.detected

    def evaluate_callback(self):
        &#34;&#34;&#34;After all the predictions, use the evaluation kit to compute the metrics.

        The evaluation kit receives the root directory of the dataset and a CSV file
        with `\t` as separator with rows with `image - brand/no-logo - prob.
        So this method generates the CSV file and call the evaluation function.
        &#34;&#34;&#34;
        with open(self.params[&#39;file&#39;], &#39;w&#39;) as file:
            file.write(&#39;\n&#39;.join((&#39;\t&#39;.join(prediction) for prediction in self.predictions)))

        fl_eval_classification(self.params[&#39;root&#39;], self.params[&#39;file&#39;], verbose=True)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="torchsight.evaluators.flickr32.evaluator.Flickr32Evaluator"><code class="flex name class">
<span>class <span class="ident">Flickr32Evaluator</span></span>
<span>(</span><span><small>ancestors:</small> <a title="torchsight.evaluators.evaluator.Evaluator" href="../evaluator.html#torchsight.evaluators.evaluator.Evaluator">Evaluator</a>, <a title="torchsight.utils.print.PrintMixin" href="../../utils/print.html#torchsight.utils.print.PrintMixin">PrintMixin</a>)</span>
</code></dt>
<dd>
<section class="desc"><p>An evaluator for the Flickr32 dataset.</p>
<p>You must extend this evaluator and override the <code>get_model()</code> method to use a custom model
to perform the evaluation and <code>get_transform()</code> to use the transformation of the images
as your model needs.</p>
<p>The model should return the name of the brand for each image (or none if the image has
no logo) and the probability of that prediction.
You could override the method <code>predict()</code> to perform that task.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class Flickr32Evaluator(Evaluator):
    &#34;&#34;&#34;An evaluator for the Flickr32 dataset.

    You must extend this evaluator and override the `get_model()` method to use a custom model
    to perform the evaluation and `get_transform()` to use the transformation of the images
    as your model needs.

    The model should return the name of the brand for each image (or none if the image has
    no logo) and the probability of that prediction.
    You could override the method `predict()` to perform that task.
    &#34;&#34;&#34;

    def __init__(self, *args, **kwargs):
        &#34;&#34;&#34;Initialize the evaluator.&#34;&#34;&#34;
        self.processed = 0  # Number of processed images
        self.detected = 0  # Number of images with logos detected
        self.predictions = []

        super().__init__(*args, **kwargs)

    @staticmethod
    def get_base_params():
        &#34;&#34;&#34;Get the base parameters for the evaluator.&#34;&#34;&#34;
        return merge_dicts(
            super(Flickr32Evaluator, Flickr32Evaluator).get_base_params(),
            {
                &#39;root&#39;: &#39;./datasets/flickr32&#39;,
                &#39;file&#39;: &#39;./flickr32_predictions.csv&#39;,
                &#39;dataloader&#39;: {
                    &#39;num_workers&#39;: 8,
                    &#39;shuffle&#39;: False,
                    &#39;batch_size&#39;: 8
                }
            })

    ###############################
    ###         GETTERS         ###
    ###############################

    def get_transform(self):
        &#34;&#34;&#34;Get the transformation to applies to the dataset according to the model.&#34;&#34;&#34;
        raise NotImplementedError()

    def get_model(self):
        &#34;&#34;&#34;Get the model that makes the predictions.&#34;&#34;&#34;
        raise NotImplementedError()

    def get_dataset(self):
        &#34;&#34;&#34;Get the dataset for the evaluation.

        Returns:
            torch.utils.data.Dataset: The dataset to use for the evaluation.
        &#34;&#34;&#34;
        transform = self.get_transform()

        try:
            params = self.checkpoint[&#39;hyperparameters&#39;][&#39;datasets&#39;][&#39;flickr32&#39;]
        except KeyError:
            # The model was not trained over flickr32 dataset
            params = {&#39;classes&#39;: None}

        return Flickr32Dataset(root=self.params[&#39;root&#39;], classes=params[&#39;classes&#39;], only_boxes=False,
                               dataset=&#39;test&#39;, transform=transform)

    def get_dataloader(self):
        &#34;&#34;&#34;Generate the custom dataloaders for the evaluation.

        Returns:
            torch.utils.data.Dataloaders: The dataloader for the validation.
        &#34;&#34;&#34;
        def collate(data):
            &#34;&#34;&#34;Custom collate function to join the images and get the name of the images.

            Arguments:
                data (sequence): Sequence of tuples as (image, _, info).

            Returns:
                torch.Tensor: The images.
                    Shape:
                        (batch size, channels, height, width)
                list of dicts: The filename of the each image.
            &#34;&#34;&#34;
            images = [image for image, *_ in data]
            max_width = max([image.shape[-1] for image in images])
            max_height = max([image.shape[-2] for image in images])

            def pad_image(image):
                aux = torch.zeros((image.shape[0], max_height, max_width))
                aux[:, :image.shape[1], :image.shape[2]] = image
                return aux

            images = torch.stack([pad_image(image) for image, *_ in data], dim=0)
            files = [info[&#39;image&#39;].split(&#39;/&#39;)[-1].replace(&#39;.jpg&#39;, &#39;&#39;) for _, _, info in data]

            return images, files

        hyperparameters = {**self.params[&#39;dataloader&#39;], &#39;collate_fn&#39;: collate}
        return DataLoader(**hyperparameters, dataset=self.dataset)

    ###############################
    ###         METHODS         ###
    ###############################

    def predict(self, images, files):
        &#34;&#34;&#34;Make a predictions for the given images.

        It assumes that the model make predictions and returns a list of tensors with shape:
        `(num bounding boxes, 6)`.
        For each prediction contains x1, y1, x2, y2, label, probability.

        So this method keep only the maximum annotation and generates the tuples.

        If your model does not follow this structure you can override this method.

        Arguments:
            images (torch.Tensor): The batch of images to make predictions on.
            infos (list of dict): A list of the dicts generated by the dataset.
                See __getitem__ method in the dataste for more information.

        Returns:
            list of tuples: Each tuple contains the name of the brand and the probability of
                the prediction.
                If the prediction is that there is no logo in the image it returns None as brand.
        &#34;&#34;&#34;
        detections_list = self.model(images)
        predictions = []

        for i, detections in enumerate(detections_list):
            self.processed += 1
            if detections.shape[0] &gt; 0:
                self.detected += 1
                probs = detections[:, 5]
                prob, index = probs.max(dim=0)
                label = detections[index, 4]
                brand = self.dataset.label_to_class[int(label.long())]
            else:
                brand = &#39;no-logo&#39;
                prob = 1.0

            predictions.append((files[i], brand, &#39;{:.1f}&#39;.format(float(prob))))

        return predictions

    def forward(self, images, infos):
        &#34;&#34;&#34;Forward pass through the model.

        Make the predictions and add it to the predictions variable.
        &#34;&#34;&#34;
        images = images.to(self.device)
        self.predictions += self.predict(images, infos)
        self.current_log[&#39;Processed&#39;] = self.processed
        self.current_log[&#39;Detected&#39;] = self.detected

    def evaluate_callback(self):
        &#34;&#34;&#34;After all the predictions, use the evaluation kit to compute the metrics.

        The evaluation kit receives the root directory of the dataset and a CSV file
        with `\t` as separator with rows with `image - brand/no-logo - prob.
        So this method generates the CSV file and call the evaluation function.
        &#34;&#34;&#34;
        with open(self.params[&#39;file&#39;], &#39;w&#39;) as file:
            file.write(&#39;\n&#39;.join((&#39;\t&#39;.join(prediction) for prediction in self.predictions)))

        fl_eval_classification(self.params[&#39;root&#39;], self.params[&#39;file&#39;], verbose=True)</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="torchsight.evaluators.flickr32.evaluator.Flickr32Evaluator.get_base_params"><code class="name flex">
<span>def <span class="ident">get_base_params</span></span>(<span>)</span>
</code></dt>
<dd>
<section class="desc"><p>Get the base parameters for the evaluator.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">@staticmethod
def get_base_params():
    &#34;&#34;&#34;Get the base parameters for the evaluator.&#34;&#34;&#34;
    return merge_dicts(
        super(Flickr32Evaluator, Flickr32Evaluator).get_base_params(),
        {
            &#39;root&#39;: &#39;./datasets/flickr32&#39;,
            &#39;file&#39;: &#39;./flickr32_predictions.csv&#39;,
            &#39;dataloader&#39;: {
                &#39;num_workers&#39;: 8,
                &#39;shuffle&#39;: False,
                &#39;batch_size&#39;: 8
            }
        })</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="torchsight.evaluators.flickr32.evaluator.Flickr32Evaluator.__init__"><code class="name flex">
<span>def <span class="ident">__init__</span></span>(<span>self, *args, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Initialize the evaluator.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def __init__(self, *args, **kwargs):
    &#34;&#34;&#34;Initialize the evaluator.&#34;&#34;&#34;
    self.processed = 0  # Number of processed images
    self.detected = 0  # Number of images with logos detected
    self.predictions = []

    super().__init__(*args, **kwargs)</code></pre>
</details>
</dd>
<dt id="torchsight.evaluators.flickr32.evaluator.Flickr32Evaluator.evaluate_callback"><code class="name flex">
<span>def <span class="ident">evaluate_callback</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>After all the predictions, use the evaluation kit to compute the metrics.</p>
<p>The evaluation kit receives the root directory of the dataset and a CSV file
with <code></code> as separator with rows with `image - brand/no-logo - prob.
So this method generates the CSV file and call the evaluation function.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def evaluate_callback(self):
    &#34;&#34;&#34;After all the predictions, use the evaluation kit to compute the metrics.

    The evaluation kit receives the root directory of the dataset and a CSV file
    with `\t` as separator with rows with `image - brand/no-logo - prob.
    So this method generates the CSV file and call the evaluation function.
    &#34;&#34;&#34;
    with open(self.params[&#39;file&#39;], &#39;w&#39;) as file:
        file.write(&#39;\n&#39;.join((&#39;\t&#39;.join(prediction) for prediction in self.predictions)))

    fl_eval_classification(self.params[&#39;root&#39;], self.params[&#39;file&#39;], verbose=True)</code></pre>
</details>
</dd>
<dt id="torchsight.evaluators.flickr32.evaluator.Flickr32Evaluator.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, images, infos)</span>
</code></dt>
<dd>
<section class="desc"><p>Forward pass through the model.</p>
<p>Make the predictions and add it to the predictions variable.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def forward(self, images, infos):
    &#34;&#34;&#34;Forward pass through the model.

    Make the predictions and add it to the predictions variable.
    &#34;&#34;&#34;
    images = images.to(self.device)
    self.predictions += self.predict(images, infos)
    self.current_log[&#39;Processed&#39;] = self.processed
    self.current_log[&#39;Detected&#39;] = self.detected</code></pre>
</details>
</dd>
<dt id="torchsight.evaluators.flickr32.evaluator.Flickr32Evaluator.get_dataloader"><code class="name flex">
<span>def <span class="ident">get_dataloader</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Generate the custom dataloaders for the evaluation.</p>
<h2 id="returns">Returns</h2>
<p>torch.utils.data.Dataloaders: The dataloader for the validation.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_dataloader(self):
    &#34;&#34;&#34;Generate the custom dataloaders for the evaluation.

    Returns:
        torch.utils.data.Dataloaders: The dataloader for the validation.
    &#34;&#34;&#34;
    def collate(data):
        &#34;&#34;&#34;Custom collate function to join the images and get the name of the images.

        Arguments:
            data (sequence): Sequence of tuples as (image, _, info).

        Returns:
            torch.Tensor: The images.
                Shape:
                    (batch size, channels, height, width)
            list of dicts: The filename of the each image.
        &#34;&#34;&#34;
        images = [image for image, *_ in data]
        max_width = max([image.shape[-1] for image in images])
        max_height = max([image.shape[-2] for image in images])

        def pad_image(image):
            aux = torch.zeros((image.shape[0], max_height, max_width))
            aux[:, :image.shape[1], :image.shape[2]] = image
            return aux

        images = torch.stack([pad_image(image) for image, *_ in data], dim=0)
        files = [info[&#39;image&#39;].split(&#39;/&#39;)[-1].replace(&#39;.jpg&#39;, &#39;&#39;) for _, _, info in data]

        return images, files

    hyperparameters = {**self.params[&#39;dataloader&#39;], &#39;collate_fn&#39;: collate}
    return DataLoader(**hyperparameters, dataset=self.dataset)</code></pre>
</details>
</dd>
<dt id="torchsight.evaluators.flickr32.evaluator.Flickr32Evaluator.get_model"><code class="name flex">
<span>def <span class="ident">get_model</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Get the model that makes the predictions.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_model(self):
    &#34;&#34;&#34;Get the model that makes the predictions.&#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="torchsight.evaluators.flickr32.evaluator.Flickr32Evaluator.get_transform"><code class="name flex">
<span>def <span class="ident">get_transform</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Get the transformation to applies to the dataset according to the model.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_transform(self):
    &#34;&#34;&#34;Get the transformation to applies to the dataset according to the model.&#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
</dd>
<dt id="torchsight.evaluators.flickr32.evaluator.Flickr32Evaluator.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, images, files)</span>
</code></dt>
<dd>
<section class="desc"><p>Make a predictions for the given images.</p>
<p>It assumes that the model make predictions and returns a list of tensors with shape:
<code>(num bounding boxes, 6)</code>.
For each prediction contains x1, y1, x2, y2, label, probability.</p>
<p>So this method keep only the maximum annotation and generates the tuples.</p>
<p>If your model does not follow this structure you can override this method.</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>images</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The batch of images to make predictions on.</dd>
<dt><strong><code>infos</code></strong> :&ensp;<code>list</code> of <code>dict</code></dt>
<dd>A list of the dicts generated by the dataset.
See <strong>getitem</strong> method in the dataste for more information.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>list of tuples: Each tuple contains the name of the brand and the probability of
the prediction.
If the prediction is that there is no logo in the image it returns None as brand.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def predict(self, images, files):
    &#34;&#34;&#34;Make a predictions for the given images.

    It assumes that the model make predictions and returns a list of tensors with shape:
    `(num bounding boxes, 6)`.
    For each prediction contains x1, y1, x2, y2, label, probability.

    So this method keep only the maximum annotation and generates the tuples.

    If your model does not follow this structure you can override this method.

    Arguments:
        images (torch.Tensor): The batch of images to make predictions on.
        infos (list of dict): A list of the dicts generated by the dataset.
            See __getitem__ method in the dataste for more information.

    Returns:
        list of tuples: Each tuple contains the name of the brand and the probability of
            the prediction.
            If the prediction is that there is no logo in the image it returns None as brand.
    &#34;&#34;&#34;
    detections_list = self.model(images)
    predictions = []

    for i, detections in enumerate(detections_list):
        self.processed += 1
        if detections.shape[0] &gt; 0:
            self.detected += 1
            probs = detections[:, 5]
            prob, index = probs.max(dim=0)
            label = detections[index, 4]
            brand = self.dataset.label_to_class[int(label.long())]
        else:
            brand = &#39;no-logo&#39;
            prob = 1.0

        predictions.append((files[i], brand, &#39;{:.1f}&#39;.format(float(prob))))

    return predictions</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="torchsight.evaluators.evaluator.Evaluator" href="../evaluator.html#torchsight.evaluators.evaluator.Evaluator">Evaluator</a></b></code>:
<ul class="hlist">
<li><code><a title="torchsight.evaluators.evaluator.Evaluator.batch_callback" href="../evaluator.html#torchsight.evaluators.evaluator.Evaluator.batch_callback">batch_callback</a></code></li>
<li><code><a title="torchsight.evaluators.evaluator.Evaluator.eval_mode" href="../evaluator.html#torchsight.evaluators.evaluator.Evaluator.eval_mode">eval_mode</a></code></li>
<li><code><a title="torchsight.evaluators.evaluator.Evaluator.evaluate" href="../evaluator.html#torchsight.evaluators.evaluator.Evaluator.evaluate">evaluate</a></code></li>
<li><code><a title="torchsight.evaluators.evaluator.Evaluator.get_dataset" href="../evaluator.html#torchsight.evaluators.evaluator.Evaluator.get_dataset">get_dataset</a></code></li>
<li><code><a title="torchsight.evaluators.evaluator.Evaluator.get_logger" href="../evaluator.html#torchsight.evaluators.evaluator.Evaluator.get_logger">get_logger</a></code></li>
<li><code><a title="torchsight.evaluators.evaluator.Evaluator.print" href="../../utils/print.html#torchsight.utils.print.PrintMixin.print">print</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="torchsight.evaluators.flickr32" href="index.html">torchsight.evaluators.flickr32</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="torchsight.evaluators.flickr32.evaluator.Flickr32Evaluator" href="#torchsight.evaluators.flickr32.evaluator.Flickr32Evaluator">Flickr32Evaluator</a></code></h4>
<ul class="two-column">
<li><code><a title="torchsight.evaluators.flickr32.evaluator.Flickr32Evaluator.__init__" href="#torchsight.evaluators.flickr32.evaluator.Flickr32Evaluator.__init__">__init__</a></code></li>
<li><code><a title="torchsight.evaluators.flickr32.evaluator.Flickr32Evaluator.evaluate_callback" href="#torchsight.evaluators.flickr32.evaluator.Flickr32Evaluator.evaluate_callback">evaluate_callback</a></code></li>
<li><code><a title="torchsight.evaluators.flickr32.evaluator.Flickr32Evaluator.forward" href="#torchsight.evaluators.flickr32.evaluator.Flickr32Evaluator.forward">forward</a></code></li>
<li><code><a title="torchsight.evaluators.flickr32.evaluator.Flickr32Evaluator.get_base_params" href="#torchsight.evaluators.flickr32.evaluator.Flickr32Evaluator.get_base_params">get_base_params</a></code></li>
<li><code><a title="torchsight.evaluators.flickr32.evaluator.Flickr32Evaluator.get_dataloader" href="#torchsight.evaluators.flickr32.evaluator.Flickr32Evaluator.get_dataloader">get_dataloader</a></code></li>
<li><code><a title="torchsight.evaluators.flickr32.evaluator.Flickr32Evaluator.get_model" href="#torchsight.evaluators.flickr32.evaluator.Flickr32Evaluator.get_model">get_model</a></code></li>
<li><code><a title="torchsight.evaluators.flickr32.evaluator.Flickr32Evaluator.get_transform" href="#torchsight.evaluators.flickr32.evaluator.Flickr32Evaluator.get_transform">get_transform</a></code></li>
<li><code><a title="torchsight.evaluators.flickr32.evaluator.Flickr32Evaluator.predict" href="#torchsight.evaluators.flickr32.evaluator.Flickr32Evaluator.predict">predict</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.5.4</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>