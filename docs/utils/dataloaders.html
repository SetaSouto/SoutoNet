<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.5.4" />
<title>torchsight.utils.dataloaders API documentation</title>
<meta name="description" content="Functional module to get dataloaders." />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.name small{font-weight:normal}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase;cursor:pointer}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title"><code>torchsight.utils.dataloaders</code> module</h1>
</header>
<section id="section-intro">
<p>Functional module to get dataloaders.</p>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">&#34;&#34;&#34;Functional module to get dataloaders.&#34;&#34;&#34;
import torch
from torch.utils.data import DataLoader


def get_dataloaders(hyperparameters, train_dataset, valid_dataset=None):
    &#34;&#34;&#34;Initialize and get the dataloaders for the given datasets.

    This is function is here to avoid duplication of code, as almost all the datasets
    returns tuples of (images, batches, *_) we generate dataloaders for that datasets
    with a single function.

    Arguments:
        hyperparameters (dict): A dict with the keyword arguments for the DataLoader.
        train_dataset (torch.utils.data.Dataset): The dataset for training.
        valid_dataset (torch.utils.data.Dataset, optional): An optional dataset for validation.

    Returns:
        torch.utils.data.Dataloaders: The dataloader for the training dataset.
        torch.utils.data.Dataloaders: The dataloader for the validation dataset.
    &#34;&#34;&#34;
    def collate(data):
        &#34;&#34;&#34;Custom collate function to join the different images with its different annotations.

        Why is this important?
        Because as each image could contain different amounts of annotated objects the tensor
        for the batch could not be created, so we need to &#34;fill&#34; the annotations tensors with -1
        to give them the same shapes and stack them.
        Why -1?
        Because the FocalLoss could interpret that label and ingore it for the loss.

        Also it pads the images so all has the same size.

        Arguments:
            data (sequence): Sequence of tuples as (image, annotations, *_).

        Returns:
            torch.Tensor: The images.
                Shape:
                    (batch size, channels, height, width)
            torch.Tensor: The annotations.
                Shape:
                    (batch size, biggest amount of annotations, 5)
        &#34;&#34;&#34;
        images = [image for image, *_ in data]
        max_width = max([image.shape[-1] for image in images])
        max_height = max([image.shape[-2] for image in images])

        def pad_image(image):
            aux = torch.zeros((image.shape[0], max_height, max_width))
            aux[:, :image.shape[1], :image.shape[2]] = image
            return aux

        images = torch.stack([pad_image(image) for image, *_ in data], dim=0)

        max_annotations = max([annotations.shape[0] for _, annotations, *_ in data])

        def fill_annotations(annotations):
            aux = torch.ones((max_annotations, 5))
            aux *= -1
            aux[:annotations.shape[0], :] = annotations
            return aux

        annotations = torch.stack([fill_annotations(a) for _, a, *_ in data], dim=0)
        return images, annotations

    hyperparameters = {**hyperparameters, &#39;collate_fn&#39;: collate}
    train_dataloader = DataLoader(**hyperparameters, dataset=train_dataset)

    if valid_dataset is not None:
        return (train_dataloader,
                DataLoader(**hyperparameters, dataset=valid_dataset))

    return train_dataloader</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="torchsight.utils.dataloaders.get_dataloaders"><code class="name flex">
<span>def <span class="ident">get_dataloaders</span></span>(<span>hyperparameters, train_dataset, valid_dataset=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Initialize and get the dataloaders for the given datasets.</p>
<p>This is function is here to avoid duplication of code, as almost all the datasets
returns tuples of (images, batches, *_) we generate dataloaders for that datasets
with a single function.</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>hyperparameters</code></strong> :&ensp;<code>dict</code></dt>
<dd>A dict with the keyword arguments for the DataLoader.</dd>
<dt><strong><code>train_dataset</code></strong> :&ensp;<code>torch.utils.data.Dataset</code></dt>
<dd>The dataset for training.</dd>
<dt><strong><code>valid_dataset</code></strong> :&ensp;<code>torch.utils.data.Dataset</code>, optional</dt>
<dd>An optional dataset for validation.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>torch.utils.data.Dataloaders: The dataloader for the training dataset.
torch.utils.data.Dataloaders: The dataloader for the validation dataset.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_dataloaders(hyperparameters, train_dataset, valid_dataset=None):
    &#34;&#34;&#34;Initialize and get the dataloaders for the given datasets.

    This is function is here to avoid duplication of code, as almost all the datasets
    returns tuples of (images, batches, *_) we generate dataloaders for that datasets
    with a single function.

    Arguments:
        hyperparameters (dict): A dict with the keyword arguments for the DataLoader.
        train_dataset (torch.utils.data.Dataset): The dataset for training.
        valid_dataset (torch.utils.data.Dataset, optional): An optional dataset for validation.

    Returns:
        torch.utils.data.Dataloaders: The dataloader for the training dataset.
        torch.utils.data.Dataloaders: The dataloader for the validation dataset.
    &#34;&#34;&#34;
    def collate(data):
        &#34;&#34;&#34;Custom collate function to join the different images with its different annotations.

        Why is this important?
        Because as each image could contain different amounts of annotated objects the tensor
        for the batch could not be created, so we need to &#34;fill&#34; the annotations tensors with -1
        to give them the same shapes and stack them.
        Why -1?
        Because the FocalLoss could interpret that label and ingore it for the loss.

        Also it pads the images so all has the same size.

        Arguments:
            data (sequence): Sequence of tuples as (image, annotations, *_).

        Returns:
            torch.Tensor: The images.
                Shape:
                    (batch size, channels, height, width)
            torch.Tensor: The annotations.
                Shape:
                    (batch size, biggest amount of annotations, 5)
        &#34;&#34;&#34;
        images = [image for image, *_ in data]
        max_width = max([image.shape[-1] for image in images])
        max_height = max([image.shape[-2] for image in images])

        def pad_image(image):
            aux = torch.zeros((image.shape[0], max_height, max_width))
            aux[:, :image.shape[1], :image.shape[2]] = image
            return aux

        images = torch.stack([pad_image(image) for image, *_ in data], dim=0)

        max_annotations = max([annotations.shape[0] for _, annotations, *_ in data])

        def fill_annotations(annotations):
            aux = torch.ones((max_annotations, 5))
            aux *= -1
            aux[:annotations.shape[0], :] = annotations
            return aux

        annotations = torch.stack([fill_annotations(a) for _, a, *_ in data], dim=0)
        return images, annotations

    hyperparameters = {**hyperparameters, &#39;collate_fn&#39;: collate}
    train_dataloader = DataLoader(**hyperparameters, dataset=train_dataset)

    if valid_dataset is not None:
        return (train_dataloader,
                DataLoader(**hyperparameters, dataset=valid_dataset))

    return train_dataloader</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="torchsight.utils" href="index.html">torchsight.utils</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="torchsight.utils.dataloaders.get_dataloaders" href="#torchsight.utils.dataloaders.get_dataloaders">get_dataloaders</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.5.4</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>