<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.5.4" />
<title>torchsight.trainers.retinanet API documentation</title>
<meta name="description" content="RetinaNet trainer." />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.name small{font-weight:normal}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase;cursor:pointer}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title"><code>torchsight.trainers.retinanet</code> module</h1>
</header>
<section id="section-intro">
<p>RetinaNet trainer.</p>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">&#34;&#34;&#34;RetinaNet trainer.&#34;&#34;&#34;
import time

import torch
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch.utils.data import DataLoader

from torchvision import transforms

from ..datasets import CocoDataset
from ..losses import FocalLoss
from ..metrics import MeanAP
from ..models import RetinaNet
from ..transforms.detection import Normalize, Resize, ToTensor
from .abstract import AbstractTrainer


class RetinaNetTrainer(AbstractTrainer):
    &#34;&#34;&#34;Trainer for the RetinaNet model.

    For each one of the hyperparameters please visit the class docstring.
    &#34;&#34;&#34;
    # Base hyperparameters, can be replaced in the initialization of the trainer:
    # &gt;&gt;&gt; RetinaNetTrainer(hyperparameters={&#39;RetinaNet&#39;: {&#39;classes&#39;: 1}})
    hyperparameters = {
        &#39;RetinaNet&#39;: {
            &#39;classes&#39;: 80,
            &#39;resnet&#39;: 50,
            &#39;features&#39;: {
                &#39;pyramid&#39;: 256,
                &#39;regression&#39;: 256,
                &#39;classification&#39;: 256
            },
            &#39;anchors&#39;: {
                &#39;sizes&#39;: [32, 64, 128, 256, 512],
                &#39;scales&#39;: [2 ** 0, 2 ** (1/3), 2 ** (2/3)],
                &#39;ratios&#39;: [0.5, 1, 2]
            },
            &#39;pretrained&#39;: True,
            &#39;evaluation&#39;: {
                &#39;threshold&#39;: 0.5,
                &#39;iou_threshold&#39;: 0.5
            }
        },
        &#39;FocalLoss&#39;: {
            &#39;alpha&#39;: 0.25,
            &#39;gamma&#39;: 2.0,
            &#39;iou_thresholds&#39;: {
                &#39;background&#39;: 0.4,
                &#39;object&#39;: 0.5
            },
            # Weight of each loss. See train method.
            &#39;weights&#39;: {&#39;classification&#39;: 1e5, &#39;regression&#39;: 1}
        },
        &#39;datasets&#39;: {
            &#39;root&#39;: &#39;./datasets/coco&#39;,
            &#39;class_names&#39;: (),  # () indicates all classes
            &#39;train&#39;: &#39;train2017&#39;,
            &#39;validation&#39;: &#39;val2017&#39;
        },
        &#39;dataloaders&#39;: {
            &#39;batch_size&#39;: 1,
            &#39;shuffle&#39;: True,
            &#39;num_workers&#39;: 1
        },
        &#39;optimizer&#39;: {
            &#39;learning_rate&#39;: 1e-2,
            &#39;momentum&#39;: 0.9,
            &#39;weight_decay&#39;: 1e-4
        },
        &#39;scheduler&#39;: {
            &#39;factor&#39;: 0.1,
            &#39;patience&#39;: 2,
            &#39;threshold&#39;: 0.1
        },
        &#39;transforms&#39;: {
            &#39;resize&#39;: {
                &#39;min_side&#39;: 640,
                &#39;max_side&#39;: 1024,
                &#39;stride&#39;: 128
            },
            &#39;normalize&#39;: {
                &#39;mean&#39;: [0.485, 0.456, 0.406],
                &#39;std&#39;: [0.229, 0.224, 0.225]
            }
        }
    }

    def __init__(self, *args, **kwargs):
        &#34;&#34;&#34;Initialize the trainer.&#34;&#34;&#34;
        self.compute_map = MeanAP()
        super(RetinaNetTrainer, self).__init__(*args, **kwargs)

    def get_model_hyperparameters(self):
        return self.hyperparameters[&#39;RetinaNet&#39;]

    def forward(self, images, *args, **kwargs):
        &#34;&#34;&#34;Forward pass through the network.

        Why this method? Is a software decision, that gives the freedom to override the forward pass
        through the network and reuse all the other code. See the DLDE trainer for an example.
        &#34;&#34;&#34;
        return self.model(images)

    def train(self, epochs=100, validate=True, epoch_callback=None):
        &#34;&#34;&#34;Train the model for the given epochs.

        Arguments:
            epochs (int): The number of epochs to train.
            validate (bool): If true it validates the model after each epoch using the validate method.
            epoch_callback (function, optional): An optional function to call after each epoch.
        &#34;&#34;&#34;
        self.model.to(self.device)
        self.criterion.to(self.device)

        # Weights for each loss, to increase or decrease their values
        weights = self.hyperparameters[&#39;FocalLoss&#39;][&#39;weights&#39;]

        print(&#39;----- Training started ------&#39;)
        print(&#39;Using device: {}&#39;.format(self.device))

        n_batches = len(self.dataloader)
        start_time = time.time()

        for epoch in range(epochs):
            epoch = epoch + 1 + self.checkpoint_epoch
            last_endtime = time.time()

            # Set model to train mode, useful for batch normalization or dropouts modules. For more info see:
            # https://discuss.pytorch.org/t/trying-to-understand-the-meaning-of-model-train-and-model-eval/20158
            self.model.train()

            for batch_index, (images, annotations, *_) in enumerate(self.dataloader):
                images, annotations = images.to(self.device), annotations.to(self.device)

                # Optimize
                self.optimizer.zero_grad()
                anchors, regressions, classifications = self.forward(images, annotations)
                del images
                classification_loss, regression_loss = self.criterion(anchors, regressions, classifications,
                                                                      annotations)
                del anchors, regressions, classifications, annotations

                classification_loss *= weights[&#39;classification&#39;]
                regression_loss *= weights[&#39;regression&#39;]
                loss = classification_loss + regression_loss
                # Set as float to free memory
                classification_loss = float(classification_loss)
                regression_loss = float(regression_loss)
                # Optimize
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.1)
                self.optimizer.step()

                # Log the batch
                total_time = time.time() - start_time
                batch_time = time.time() - last_endtime
                last_endtime = time.time()
                # Get the actual learning rate (modified by the scheduler)
                learning_rates = [str(param_group[&#39;lr&#39;]) for i, param_group in enumerate(self.optimizer.param_groups)]
                self.logger.log({
                    &#39;Training&#39;: None,
                    &#39;Epoch&#39;: epoch,
                    &#39;Batch&#39;: &#39;{}/{}&#39;.format(batch_index + 1, n_batches),
                    &#39;LR&#39;: &#39; &#39;.join(learning_rates),
                    &#39;Class.&#39;: &#39;{:.7f}&#39;.format(classification_loss),
                    &#39;Regr.&#39;: &#39;{:.7f}&#39;.format(regression_loss),
                    &#39;Loss&#39;: &#39;{:.7f}&#39;.format(loss),
                    &#39;Time&#39;: &#39;{:.3f} s&#39;.format(batch_time),
                    &#39;Total&#39;: &#39;{:.3f} s&#39;.format(total_time)
                })

                # Save the weights for this epoch every some batches
                if batch_index % 100 == 0:
                    self.save_checkpoint(epoch)

            # Call the epoch callback
            if epoch_callback is not None:
                epoch_callback(epoch)

            # Save the weights at the end of the epoch
            self.save_checkpoint(epoch)

            if validate:
                validation_loss = self.validate(epoch)
                self.scheduler.step(validation_loss)

    def validate(self, epoch):
        &#34;&#34;&#34;Compute the loss over the validation dataset.&#34;&#34;&#34;
        self.model.to(self.device)
        hyperparameters = self.get_model_hyperparameters()[&#39;evaluation&#39;]
        self.model.eval(threshold=hyperparameters[&#39;threshold&#39;],
                        iou_threshold=hyperparameters[&#39;iou_threshold&#39;],
                        loss=True)

        weights = self.hyperparameters[&#39;FocalLoss&#39;][&#39;weights&#39;]

        classification_losses = []
        regression_losses = []
        losses = []

        start_time = time.time()
        last_endtime = time.time()
        n_batches = len(self.valid_dataloader)

        for batch, (images, annotations) in enumerate(self.valid_dataloader):
            images, annotations = images.to(self.device), annotations.to(self.device)

            anchors, regressions, classifications = self.model(images)
            del images

            classification_loss, regression_loss = self.criterion(anchors, regressions, classifications, annotations)
            del anchors, regressions, classifications, annotations

            classification_loss *= weights[&#39;classification&#39;]
            regression_loss *= weights[&#39;regression&#39;]
            classification_loss = float(classification_loss)
            regression_loss = float(regression_loss)

            classification_losses.append(classification_loss)
            regression_losses.append(regression_loss)
            losses.append(classification_loss + regression_loss)

            batch_time = time.time() - last_endtime
            last_endtime = time.time()
            total_time = time.time() - start_time

            self.logger.log({
                &#39;Validating&#39;: None,
                &#39;Epoch&#39;: epoch,
                &#39;Batch&#39;: &#39;{}/{}&#39;.format(batch + 1, n_batches),
                &#39;Class.&#39;: &#39;{:.7f}&#39;.format(classification_loss),
                &#39;Regr.&#39;: &#39;{:.7f}&#39;.format(regression_loss),
                &#39;Loss&#39;: &#39;{:.7f}&#39;.format(losses[-1]),
                &#39;Time&#39;: &#39;{:.3f}&#39;.format(batch_time),
                &#39;Total&#39;: &#39;{:.3f}&#39;.format(total_time)
            })

        return torch.Tensor(losses).mean()

    def validate_map(self):
        &#34;&#34;&#34;Compute mAP over validation dataset.

        We iterate over the images in the validation dataset, compute the detections using the current
        state of the model, generate the detections tensor and compute the mAP using the MeanAP class.

        As the MeanAP class does not computes the mAP for each class (it computes for the entire image),
        we must filter the annotations by each class, compute mAP and store the value for the class and
        continue.

        Returns:
            torch.Tensor: The mAP averaged over all the classes.
        &#34;&#34;&#34;
        print(&#39;--------- VALIDATING --------&#39;)

        self.model.to(self.device)
        self.model.eval(**self.hyperparameters[&#39;RetinaNet&#39;][&#39;evaluation&#39;])

        mAP = {}  # The array of mAP per each class per each image
        # mAP is something like {&#39;0&#39;: [0.15, ..., 0.87]} where the length of the list is the number of images
        # in the validation dataset
        aps = {}  # The array with the Average Precision for each IoU threshold, for each image, for each class
        # aps is something like {&#39;0&#39;: [[0.67, ..., 0.54], ..., []]} where the length of the bigger array is the
        # number of images that contains that label and the inner arrays is the number of IoU thresholds to compute
        # the Average Precisions

        for batch_index, (images, annotations) in enumerate(self.valid_dataloader):
            images, annotations = images.to(self.device), annotations.to(self.device)
            for index, (boxes, classifications) in enumerate(self.model(images)):
                # Generate the detections
                detections = torch.zeros((boxes.shape[0], 6)).to(self.device)
                detections[:, :4] = boxes
                prob, label = classifications.max(dim=1)
                detections[:, 4] = label
                detections[:, 5] = prob

                # Get the actual annotations, clean and iterate over each unique label
                actual_annotations = annotations[index].clone()
                # Remove dummy annotations created by the data loader (label == -1)
                mask = actual_annotations[:, -1] == -1
                actual_annotations = actual_annotations[mask]
                # Get the true labels in the actual annotation
                labels = [int(label) for label in actual_annotations[:, -1].unique()]
                # Iterate over each label to compute mAP per class
                for label in labels:
                    if label not in mAP:
                        mAP[label] = []
                    if label not in aps:
                        aps[label] = []
                    # Add zero values if there are no detections
                    if not boxes.shape[0] &gt; 0:
                        mAP[label].append(torch.zeros((1)).mean().to(self.device))
                        aps[label].append(torch.zeros((self.compute_map.iou_thresholds.shape[0])).to(self.device))
                        continue
                    # Compute mAP
                    actual_map, actual_aps = self.compute_map(actual_annotations, detections)
                    mAP[label].append(actual_map)
                    aps[label].append(actual_aps)

            print(&#39;[Validating] [Batch {}]&#39;.format(batch_index))

        # Set the model to train again
        self.model.train()
        # Compute the average of the map over all the classes
        final_map = [torch.stack(mAP[label]).mean() for label in mAP]
        final_aps = {label: torch.stack(aps[label]).mean(dim=0) for label in aps}
        return torch.stack(final_map).mean(), final_aps

    def get_model(self):
        &#34;&#34;&#34;Initialize and get the RetinaNet.

        Returns:
            torch.nn.Module: The RetinaNet model.
        &#34;&#34;&#34;
        hyperparameters = self.hyperparameters[&#39;RetinaNet&#39;]
        return RetinaNet(
            classes=hyperparameters[&#39;classes&#39;],
            resnet=hyperparameters[&#39;resnet&#39;],
            features=hyperparameters[&#39;features&#39;],
            anchors=hyperparameters[&#39;anchors&#39;],
            pretrained=hyperparameters[&#39;pretrained&#39;]
        )

    def get_criterion(self):
        &#34;&#34;&#34;Initialize and get the FocalLoss.

        Returns:
            torch.nn.Module: The FocalLoss.
        &#34;&#34;&#34;
        hyperparameters = self.hyperparameters[&#39;FocalLoss&#39;]
        return FocalLoss(
            alpha=hyperparameters[&#39;alpha&#39;],
            gamma=hyperparameters[&#39;gamma&#39;],
            iou_thresholds=hyperparameters[&#39;iou_thresholds&#39;],
            device=self.device
        )

    def get_transform(self):
        &#34;&#34;&#34;Initialize and get the transforms for the images.

        Returns:
            torchvision.transform.Compose: The Compose of the transformations.
        &#34;&#34;&#34;
        hyperparameters = self.hyperparameters[&#39;transforms&#39;]
        return transforms.Compose([
            Resize(**hyperparameters[&#39;resize&#39;]),
            ToTensor(),
            Normalize(**hyperparameters[&#39;normalize&#39;])
        ])

    def get_datasets(self):
        &#34;&#34;&#34;Initialize and get the Coco dataset for training and evaluation.

        Returns:
            torch.utils.data.Dataset: The Coco dataset.
        &#34;&#34;&#34;
        transform = self.get_transform()
        hyperparameters = self.hyperparameters[&#39;datasets&#39;]

        model_classes = self.get_model_hyperparameters()[&#39;classes&#39;]
        dataset_classes = len(hyperparameters[&#39;class_names&#39;])
        if dataset_classes &gt; 0 and model_classes != dataset_classes:
            raise ValueError(&#39; &#39;.join([&#39;The amount of classes for the model ({})&#39;.format(model_classes),
                                       &#39;must be the same to the length of &#34;class_names&#34; in&#39;,
                                       &#39;the &#34;dataset&#34; hyperparameters ({}).&#39;.format(dataset_classes)]))

        return (
            CocoDataset(
                root=hyperparameters[&#39;root&#39;],
                dataset=hyperparameters[&#39;train&#39;],
                classes_names=hyperparameters[&#39;class_names&#39;],
                transform=transform
            ),
            CocoDataset(
                root=hyperparameters[&#39;root&#39;],
                dataset=hyperparameters[&#39;validation&#39;],
                classes_names=hyperparameters[&#39;class_names&#39;],
                transform=transform
            )
        )

    def get_dataloaders(self):
        &#34;&#34;&#34;Initialize and get the dataloaders for the datasets.

        Returns:
            torch.utils.data.Dataloaders: The dataloader for the training dataset.
            torch.utils.data.Dataloaders: The dataloader for the validation dataset.
        &#34;&#34;&#34;
        def collate(data):
            &#34;&#34;&#34;Custom collate function to join the different images with its different annotations.

            Why is this important?
            Because as each image could contain different amounts of annotated objects the tensor
            for the batch could not be created, so we need to &#34;fill&#34; the annotations tensors with -1
            to give them the same shapes and stack them.
            Why -1?
            Because the FocalLoss could interpret that label and ingore it for the loss.

            Also it pads the images so all has the same size.

            Arguments:
                data (sequence): Sequence of tuples as (image, annotations).

            Returns:
                torch.Tensor: The images.
                    Shape:
                        (batch size, channels, height, width)
                torch.Tensor: The annotations.
                    Shape:
                        (batch size, biggest amount of annotations, 5)
            &#34;&#34;&#34;
            images = [image for image, _ in data]
            max_width = max([image.shape[-1] for image in images])
            max_height = max([image.shape[-2] for image in images])

            def pad_image(image):
                aux = torch.zeros((image.shape[0], max_height, max_width))
                aux[:, :image.shape[1], :image.shape[2]] = image
                return aux

            images = torch.stack([pad_image(image) for image, _ in data], dim=0)

            max_annotations = max([annotations.shape[0] for _, annotations in data])

            def fill_annotations(annotations):
                aux = torch.ones((max_annotations, 5))
                aux *= -1
                aux[:annotations.shape[0], :] = annotations
                return aux

            annotations = torch.stack([fill_annotations(a) for _, a in data], dim=0)
            return images, annotations

        hyperparameters = {**self.hyperparameters[&#39;dataloaders&#39;], &#39;collate_fn&#39;: collate}

        return (
            DataLoader(dataset=self.dataset, **hyperparameters),
            DataLoader(dataset=self.valid_dataset, **hyperparameters)
        )

    def get_optimizer(self):
        &#34;&#34;&#34;Returns the optimizer of the training.

        Stochastic Gradient Descent:
        https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html

        Returns:
            optimizer (torch.optim.Optimizer): The optimizer of the training.
                For the optimizer package see: https://pytorch.org/docs/stable/optim.html
        &#34;&#34;&#34;
        hyperparameters = self.hyperparameters[&#39;optimizer&#39;]
        return torch.optim.SGD(
            self.model.parameters(),
            lr=hyperparameters[&#39;learning_rate&#39;],
            momentum=hyperparameters[&#39;momentum&#39;],
            weight_decay=hyperparameters[&#39;weight_decay&#39;]
        )

    def get_scheduler(self):
        &#34;&#34;&#34;Get the learning rate scheduler.

        Returns:
            torch.optim.lr_scheduler.ReduceLROnPlateau: The learning rate scheduler.
        &#34;&#34;&#34;
        hyperparameters = self.hyperparameters[&#39;scheduler&#39;]
        return ReduceLROnPlateau(
            optimizer=self.optimizer,
            mode=&#39;min&#39;,
            factor=hyperparameters[&#39;factor&#39;],
            patience=hyperparameters[&#39;patience&#39;],
            verbose=True,
            threshold=hyperparameters[&#39;threshold&#39;]
        )</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="torchsight.trainers.retinanet.RetinaNetTrainer"><code class="flex name class">
<span>class <span class="ident">RetinaNetTrainer</span></span>
<span>(</span><span><small>ancestors:</small> <a title="torchsight.trainers.abstract.AbstractTrainer" href="abstract.html#torchsight.trainers.abstract.AbstractTrainer">AbstractTrainer</a>)</span>
</code></dt>
<dd>
<section class="desc"><p>Trainer for the RetinaNet model.</p>
<p>For each one of the hyperparameters please visit the class docstring.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class RetinaNetTrainer(AbstractTrainer):
    &#34;&#34;&#34;Trainer for the RetinaNet model.

    For each one of the hyperparameters please visit the class docstring.
    &#34;&#34;&#34;
    # Base hyperparameters, can be replaced in the initialization of the trainer:
    # &gt;&gt;&gt; RetinaNetTrainer(hyperparameters={&#39;RetinaNet&#39;: {&#39;classes&#39;: 1}})
    hyperparameters = {
        &#39;RetinaNet&#39;: {
            &#39;classes&#39;: 80,
            &#39;resnet&#39;: 50,
            &#39;features&#39;: {
                &#39;pyramid&#39;: 256,
                &#39;regression&#39;: 256,
                &#39;classification&#39;: 256
            },
            &#39;anchors&#39;: {
                &#39;sizes&#39;: [32, 64, 128, 256, 512],
                &#39;scales&#39;: [2 ** 0, 2 ** (1/3), 2 ** (2/3)],
                &#39;ratios&#39;: [0.5, 1, 2]
            },
            &#39;pretrained&#39;: True,
            &#39;evaluation&#39;: {
                &#39;threshold&#39;: 0.5,
                &#39;iou_threshold&#39;: 0.5
            }
        },
        &#39;FocalLoss&#39;: {
            &#39;alpha&#39;: 0.25,
            &#39;gamma&#39;: 2.0,
            &#39;iou_thresholds&#39;: {
                &#39;background&#39;: 0.4,
                &#39;object&#39;: 0.5
            },
            # Weight of each loss. See train method.
            &#39;weights&#39;: {&#39;classification&#39;: 1e5, &#39;regression&#39;: 1}
        },
        &#39;datasets&#39;: {
            &#39;root&#39;: &#39;./datasets/coco&#39;,
            &#39;class_names&#39;: (),  # () indicates all classes
            &#39;train&#39;: &#39;train2017&#39;,
            &#39;validation&#39;: &#39;val2017&#39;
        },
        &#39;dataloaders&#39;: {
            &#39;batch_size&#39;: 1,
            &#39;shuffle&#39;: True,
            &#39;num_workers&#39;: 1
        },
        &#39;optimizer&#39;: {
            &#39;learning_rate&#39;: 1e-2,
            &#39;momentum&#39;: 0.9,
            &#39;weight_decay&#39;: 1e-4
        },
        &#39;scheduler&#39;: {
            &#39;factor&#39;: 0.1,
            &#39;patience&#39;: 2,
            &#39;threshold&#39;: 0.1
        },
        &#39;transforms&#39;: {
            &#39;resize&#39;: {
                &#39;min_side&#39;: 640,
                &#39;max_side&#39;: 1024,
                &#39;stride&#39;: 128
            },
            &#39;normalize&#39;: {
                &#39;mean&#39;: [0.485, 0.456, 0.406],
                &#39;std&#39;: [0.229, 0.224, 0.225]
            }
        }
    }

    def __init__(self, *args, **kwargs):
        &#34;&#34;&#34;Initialize the trainer.&#34;&#34;&#34;
        self.compute_map = MeanAP()
        super(RetinaNetTrainer, self).__init__(*args, **kwargs)

    def get_model_hyperparameters(self):
        return self.hyperparameters[&#39;RetinaNet&#39;]

    def forward(self, images, *args, **kwargs):
        &#34;&#34;&#34;Forward pass through the network.

        Why this method? Is a software decision, that gives the freedom to override the forward pass
        through the network and reuse all the other code. See the DLDE trainer for an example.
        &#34;&#34;&#34;
        return self.model(images)

    def train(self, epochs=100, validate=True, epoch_callback=None):
        &#34;&#34;&#34;Train the model for the given epochs.

        Arguments:
            epochs (int): The number of epochs to train.
            validate (bool): If true it validates the model after each epoch using the validate method.
            epoch_callback (function, optional): An optional function to call after each epoch.
        &#34;&#34;&#34;
        self.model.to(self.device)
        self.criterion.to(self.device)

        # Weights for each loss, to increase or decrease their values
        weights = self.hyperparameters[&#39;FocalLoss&#39;][&#39;weights&#39;]

        print(&#39;----- Training started ------&#39;)
        print(&#39;Using device: {}&#39;.format(self.device))

        n_batches = len(self.dataloader)
        start_time = time.time()

        for epoch in range(epochs):
            epoch = epoch + 1 + self.checkpoint_epoch
            last_endtime = time.time()

            # Set model to train mode, useful for batch normalization or dropouts modules. For more info see:
            # https://discuss.pytorch.org/t/trying-to-understand-the-meaning-of-model-train-and-model-eval/20158
            self.model.train()

            for batch_index, (images, annotations, *_) in enumerate(self.dataloader):
                images, annotations = images.to(self.device), annotations.to(self.device)

                # Optimize
                self.optimizer.zero_grad()
                anchors, regressions, classifications = self.forward(images, annotations)
                del images
                classification_loss, regression_loss = self.criterion(anchors, regressions, classifications,
                                                                      annotations)
                del anchors, regressions, classifications, annotations

                classification_loss *= weights[&#39;classification&#39;]
                regression_loss *= weights[&#39;regression&#39;]
                loss = classification_loss + regression_loss
                # Set as float to free memory
                classification_loss = float(classification_loss)
                regression_loss = float(regression_loss)
                # Optimize
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.1)
                self.optimizer.step()

                # Log the batch
                total_time = time.time() - start_time
                batch_time = time.time() - last_endtime
                last_endtime = time.time()
                # Get the actual learning rate (modified by the scheduler)
                learning_rates = [str(param_group[&#39;lr&#39;]) for i, param_group in enumerate(self.optimizer.param_groups)]
                self.logger.log({
                    &#39;Training&#39;: None,
                    &#39;Epoch&#39;: epoch,
                    &#39;Batch&#39;: &#39;{}/{}&#39;.format(batch_index + 1, n_batches),
                    &#39;LR&#39;: &#39; &#39;.join(learning_rates),
                    &#39;Class.&#39;: &#39;{:.7f}&#39;.format(classification_loss),
                    &#39;Regr.&#39;: &#39;{:.7f}&#39;.format(regression_loss),
                    &#39;Loss&#39;: &#39;{:.7f}&#39;.format(loss),
                    &#39;Time&#39;: &#39;{:.3f} s&#39;.format(batch_time),
                    &#39;Total&#39;: &#39;{:.3f} s&#39;.format(total_time)
                })

                # Save the weights for this epoch every some batches
                if batch_index % 100 == 0:
                    self.save_checkpoint(epoch)

            # Call the epoch callback
            if epoch_callback is not None:
                epoch_callback(epoch)

            # Save the weights at the end of the epoch
            self.save_checkpoint(epoch)

            if validate:
                validation_loss = self.validate(epoch)
                self.scheduler.step(validation_loss)

    def validate(self, epoch):
        &#34;&#34;&#34;Compute the loss over the validation dataset.&#34;&#34;&#34;
        self.model.to(self.device)
        hyperparameters = self.get_model_hyperparameters()[&#39;evaluation&#39;]
        self.model.eval(threshold=hyperparameters[&#39;threshold&#39;],
                        iou_threshold=hyperparameters[&#39;iou_threshold&#39;],
                        loss=True)

        weights = self.hyperparameters[&#39;FocalLoss&#39;][&#39;weights&#39;]

        classification_losses = []
        regression_losses = []
        losses = []

        start_time = time.time()
        last_endtime = time.time()
        n_batches = len(self.valid_dataloader)

        for batch, (images, annotations) in enumerate(self.valid_dataloader):
            images, annotations = images.to(self.device), annotations.to(self.device)

            anchors, regressions, classifications = self.model(images)
            del images

            classification_loss, regression_loss = self.criterion(anchors, regressions, classifications, annotations)
            del anchors, regressions, classifications, annotations

            classification_loss *= weights[&#39;classification&#39;]
            regression_loss *= weights[&#39;regression&#39;]
            classification_loss = float(classification_loss)
            regression_loss = float(regression_loss)

            classification_losses.append(classification_loss)
            regression_losses.append(regression_loss)
            losses.append(classification_loss + regression_loss)

            batch_time = time.time() - last_endtime
            last_endtime = time.time()
            total_time = time.time() - start_time

            self.logger.log({
                &#39;Validating&#39;: None,
                &#39;Epoch&#39;: epoch,
                &#39;Batch&#39;: &#39;{}/{}&#39;.format(batch + 1, n_batches),
                &#39;Class.&#39;: &#39;{:.7f}&#39;.format(classification_loss),
                &#39;Regr.&#39;: &#39;{:.7f}&#39;.format(regression_loss),
                &#39;Loss&#39;: &#39;{:.7f}&#39;.format(losses[-1]),
                &#39;Time&#39;: &#39;{:.3f}&#39;.format(batch_time),
                &#39;Total&#39;: &#39;{:.3f}&#39;.format(total_time)
            })

        return torch.Tensor(losses).mean()

    def validate_map(self):
        &#34;&#34;&#34;Compute mAP over validation dataset.

        We iterate over the images in the validation dataset, compute the detections using the current
        state of the model, generate the detections tensor and compute the mAP using the MeanAP class.

        As the MeanAP class does not computes the mAP for each class (it computes for the entire image),
        we must filter the annotations by each class, compute mAP and store the value for the class and
        continue.

        Returns:
            torch.Tensor: The mAP averaged over all the classes.
        &#34;&#34;&#34;
        print(&#39;--------- VALIDATING --------&#39;)

        self.model.to(self.device)
        self.model.eval(**self.hyperparameters[&#39;RetinaNet&#39;][&#39;evaluation&#39;])

        mAP = {}  # The array of mAP per each class per each image
        # mAP is something like {&#39;0&#39;: [0.15, ..., 0.87]} where the length of the list is the number of images
        # in the validation dataset
        aps = {}  # The array with the Average Precision for each IoU threshold, for each image, for each class
        # aps is something like {&#39;0&#39;: [[0.67, ..., 0.54], ..., []]} where the length of the bigger array is the
        # number of images that contains that label and the inner arrays is the number of IoU thresholds to compute
        # the Average Precisions

        for batch_index, (images, annotations) in enumerate(self.valid_dataloader):
            images, annotations = images.to(self.device), annotations.to(self.device)
            for index, (boxes, classifications) in enumerate(self.model(images)):
                # Generate the detections
                detections = torch.zeros((boxes.shape[0], 6)).to(self.device)
                detections[:, :4] = boxes
                prob, label = classifications.max(dim=1)
                detections[:, 4] = label
                detections[:, 5] = prob

                # Get the actual annotations, clean and iterate over each unique label
                actual_annotations = annotations[index].clone()
                # Remove dummy annotations created by the data loader (label == -1)
                mask = actual_annotations[:, -1] == -1
                actual_annotations = actual_annotations[mask]
                # Get the true labels in the actual annotation
                labels = [int(label) for label in actual_annotations[:, -1].unique()]
                # Iterate over each label to compute mAP per class
                for label in labels:
                    if label not in mAP:
                        mAP[label] = []
                    if label not in aps:
                        aps[label] = []
                    # Add zero values if there are no detections
                    if not boxes.shape[0] &gt; 0:
                        mAP[label].append(torch.zeros((1)).mean().to(self.device))
                        aps[label].append(torch.zeros((self.compute_map.iou_thresholds.shape[0])).to(self.device))
                        continue
                    # Compute mAP
                    actual_map, actual_aps = self.compute_map(actual_annotations, detections)
                    mAP[label].append(actual_map)
                    aps[label].append(actual_aps)

            print(&#39;[Validating] [Batch {}]&#39;.format(batch_index))

        # Set the model to train again
        self.model.train()
        # Compute the average of the map over all the classes
        final_map = [torch.stack(mAP[label]).mean() for label in mAP]
        final_aps = {label: torch.stack(aps[label]).mean(dim=0) for label in aps}
        return torch.stack(final_map).mean(), final_aps

    def get_model(self):
        &#34;&#34;&#34;Initialize and get the RetinaNet.

        Returns:
            torch.nn.Module: The RetinaNet model.
        &#34;&#34;&#34;
        hyperparameters = self.hyperparameters[&#39;RetinaNet&#39;]
        return RetinaNet(
            classes=hyperparameters[&#39;classes&#39;],
            resnet=hyperparameters[&#39;resnet&#39;],
            features=hyperparameters[&#39;features&#39;],
            anchors=hyperparameters[&#39;anchors&#39;],
            pretrained=hyperparameters[&#39;pretrained&#39;]
        )

    def get_criterion(self):
        &#34;&#34;&#34;Initialize and get the FocalLoss.

        Returns:
            torch.nn.Module: The FocalLoss.
        &#34;&#34;&#34;
        hyperparameters = self.hyperparameters[&#39;FocalLoss&#39;]
        return FocalLoss(
            alpha=hyperparameters[&#39;alpha&#39;],
            gamma=hyperparameters[&#39;gamma&#39;],
            iou_thresholds=hyperparameters[&#39;iou_thresholds&#39;],
            device=self.device
        )

    def get_transform(self):
        &#34;&#34;&#34;Initialize and get the transforms for the images.

        Returns:
            torchvision.transform.Compose: The Compose of the transformations.
        &#34;&#34;&#34;
        hyperparameters = self.hyperparameters[&#39;transforms&#39;]
        return transforms.Compose([
            Resize(**hyperparameters[&#39;resize&#39;]),
            ToTensor(),
            Normalize(**hyperparameters[&#39;normalize&#39;])
        ])

    def get_datasets(self):
        &#34;&#34;&#34;Initialize and get the Coco dataset for training and evaluation.

        Returns:
            torch.utils.data.Dataset: The Coco dataset.
        &#34;&#34;&#34;
        transform = self.get_transform()
        hyperparameters = self.hyperparameters[&#39;datasets&#39;]

        model_classes = self.get_model_hyperparameters()[&#39;classes&#39;]
        dataset_classes = len(hyperparameters[&#39;class_names&#39;])
        if dataset_classes &gt; 0 and model_classes != dataset_classes:
            raise ValueError(&#39; &#39;.join([&#39;The amount of classes for the model ({})&#39;.format(model_classes),
                                       &#39;must be the same to the length of &#34;class_names&#34; in&#39;,
                                       &#39;the &#34;dataset&#34; hyperparameters ({}).&#39;.format(dataset_classes)]))

        return (
            CocoDataset(
                root=hyperparameters[&#39;root&#39;],
                dataset=hyperparameters[&#39;train&#39;],
                classes_names=hyperparameters[&#39;class_names&#39;],
                transform=transform
            ),
            CocoDataset(
                root=hyperparameters[&#39;root&#39;],
                dataset=hyperparameters[&#39;validation&#39;],
                classes_names=hyperparameters[&#39;class_names&#39;],
                transform=transform
            )
        )

    def get_dataloaders(self):
        &#34;&#34;&#34;Initialize and get the dataloaders for the datasets.

        Returns:
            torch.utils.data.Dataloaders: The dataloader for the training dataset.
            torch.utils.data.Dataloaders: The dataloader for the validation dataset.
        &#34;&#34;&#34;
        def collate(data):
            &#34;&#34;&#34;Custom collate function to join the different images with its different annotations.

            Why is this important?
            Because as each image could contain different amounts of annotated objects the tensor
            for the batch could not be created, so we need to &#34;fill&#34; the annotations tensors with -1
            to give them the same shapes and stack them.
            Why -1?
            Because the FocalLoss could interpret that label and ingore it for the loss.

            Also it pads the images so all has the same size.

            Arguments:
                data (sequence): Sequence of tuples as (image, annotations).

            Returns:
                torch.Tensor: The images.
                    Shape:
                        (batch size, channels, height, width)
                torch.Tensor: The annotations.
                    Shape:
                        (batch size, biggest amount of annotations, 5)
            &#34;&#34;&#34;
            images = [image for image, _ in data]
            max_width = max([image.shape[-1] for image in images])
            max_height = max([image.shape[-2] for image in images])

            def pad_image(image):
                aux = torch.zeros((image.shape[0], max_height, max_width))
                aux[:, :image.shape[1], :image.shape[2]] = image
                return aux

            images = torch.stack([pad_image(image) for image, _ in data], dim=0)

            max_annotations = max([annotations.shape[0] for _, annotations in data])

            def fill_annotations(annotations):
                aux = torch.ones((max_annotations, 5))
                aux *= -1
                aux[:annotations.shape[0], :] = annotations
                return aux

            annotations = torch.stack([fill_annotations(a) for _, a in data], dim=0)
            return images, annotations

        hyperparameters = {**self.hyperparameters[&#39;dataloaders&#39;], &#39;collate_fn&#39;: collate}

        return (
            DataLoader(dataset=self.dataset, **hyperparameters),
            DataLoader(dataset=self.valid_dataset, **hyperparameters)
        )

    def get_optimizer(self):
        &#34;&#34;&#34;Returns the optimizer of the training.

        Stochastic Gradient Descent:
        https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html

        Returns:
            optimizer (torch.optim.Optimizer): The optimizer of the training.
                For the optimizer package see: https://pytorch.org/docs/stable/optim.html
        &#34;&#34;&#34;
        hyperparameters = self.hyperparameters[&#39;optimizer&#39;]
        return torch.optim.SGD(
            self.model.parameters(),
            lr=hyperparameters[&#39;learning_rate&#39;],
            momentum=hyperparameters[&#39;momentum&#39;],
            weight_decay=hyperparameters[&#39;weight_decay&#39;]
        )

    def get_scheduler(self):
        &#34;&#34;&#34;Get the learning rate scheduler.

        Returns:
            torch.optim.lr_scheduler.ReduceLROnPlateau: The learning rate scheduler.
        &#34;&#34;&#34;
        hyperparameters = self.hyperparameters[&#39;scheduler&#39;]
        return ReduceLROnPlateau(
            optimizer=self.optimizer,
            mode=&#39;min&#39;,
            factor=hyperparameters[&#39;factor&#39;],
            patience=hyperparameters[&#39;patience&#39;],
            verbose=True,
            threshold=hyperparameters[&#39;threshold&#39;]
        )</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="torchsight.trainers.retinanet.RetinaNetTrainer.hyperparameters"><code class="name">var <span class="ident">hyperparameters</span></code></dt>
<dd>
<section class="desc"></section>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="torchsight.trainers.retinanet.RetinaNetTrainer.__init__"><code class="name flex">
<span>def <span class="ident">__init__</span></span>(<span>self, *args, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Initialize the trainer.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def __init__(self, *args, **kwargs):
    &#34;&#34;&#34;Initialize the trainer.&#34;&#34;&#34;
    self.compute_map = MeanAP()
    super(RetinaNetTrainer, self).__init__(*args, **kwargs)</code></pre>
</details>
</dd>
<dt id="torchsight.trainers.retinanet.RetinaNetTrainer.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, images, *args, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Forward pass through the network.</p>
<p>Why this method? Is a software decision, that gives the freedom to override the forward pass
through the network and reuse all the other code. See the DLDE trainer for an example.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def forward(self, images, *args, **kwargs):
    &#34;&#34;&#34;Forward pass through the network.

    Why this method? Is a software decision, that gives the freedom to override the forward pass
    through the network and reuse all the other code. See the DLDE trainer for an example.
    &#34;&#34;&#34;
    return self.model(images)</code></pre>
</details>
</dd>
<dt id="torchsight.trainers.retinanet.RetinaNetTrainer.get_criterion"><code class="name flex">
<span>def <span class="ident">get_criterion</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Initialize and get the FocalLoss.</p>
<h2 id="returns">Returns</h2>
<p>torch.nn.Module: The FocalLoss.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_criterion(self):
    &#34;&#34;&#34;Initialize and get the FocalLoss.

    Returns:
        torch.nn.Module: The FocalLoss.
    &#34;&#34;&#34;
    hyperparameters = self.hyperparameters[&#39;FocalLoss&#39;]
    return FocalLoss(
        alpha=hyperparameters[&#39;alpha&#39;],
        gamma=hyperparameters[&#39;gamma&#39;],
        iou_thresholds=hyperparameters[&#39;iou_thresholds&#39;],
        device=self.device
    )</code></pre>
</details>
</dd>
<dt id="torchsight.trainers.retinanet.RetinaNetTrainer.get_dataloaders"><code class="name flex">
<span>def <span class="ident">get_dataloaders</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Initialize and get the dataloaders for the datasets.</p>
<h2 id="returns">Returns</h2>
<p>torch.utils.data.Dataloaders: The dataloader for the training dataset.
torch.utils.data.Dataloaders: The dataloader for the validation dataset.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_dataloaders(self):
    &#34;&#34;&#34;Initialize and get the dataloaders for the datasets.

    Returns:
        torch.utils.data.Dataloaders: The dataloader for the training dataset.
        torch.utils.data.Dataloaders: The dataloader for the validation dataset.
    &#34;&#34;&#34;
    def collate(data):
        &#34;&#34;&#34;Custom collate function to join the different images with its different annotations.

        Why is this important?
        Because as each image could contain different amounts of annotated objects the tensor
        for the batch could not be created, so we need to &#34;fill&#34; the annotations tensors with -1
        to give them the same shapes and stack them.
        Why -1?
        Because the FocalLoss could interpret that label and ingore it for the loss.

        Also it pads the images so all has the same size.

        Arguments:
            data (sequence): Sequence of tuples as (image, annotations).

        Returns:
            torch.Tensor: The images.
                Shape:
                    (batch size, channels, height, width)
            torch.Tensor: The annotations.
                Shape:
                    (batch size, biggest amount of annotations, 5)
        &#34;&#34;&#34;
        images = [image for image, _ in data]
        max_width = max([image.shape[-1] for image in images])
        max_height = max([image.shape[-2] for image in images])

        def pad_image(image):
            aux = torch.zeros((image.shape[0], max_height, max_width))
            aux[:, :image.shape[1], :image.shape[2]] = image
            return aux

        images = torch.stack([pad_image(image) for image, _ in data], dim=0)

        max_annotations = max([annotations.shape[0] for _, annotations in data])

        def fill_annotations(annotations):
            aux = torch.ones((max_annotations, 5))
            aux *= -1
            aux[:annotations.shape[0], :] = annotations
            return aux

        annotations = torch.stack([fill_annotations(a) for _, a in data], dim=0)
        return images, annotations

    hyperparameters = {**self.hyperparameters[&#39;dataloaders&#39;], &#39;collate_fn&#39;: collate}

    return (
        DataLoader(dataset=self.dataset, **hyperparameters),
        DataLoader(dataset=self.valid_dataset, **hyperparameters)
    )</code></pre>
</details>
</dd>
<dt id="torchsight.trainers.retinanet.RetinaNetTrainer.get_datasets"><code class="name flex">
<span>def <span class="ident">get_datasets</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Initialize and get the Coco dataset for training and evaluation.</p>
<h2 id="returns">Returns</h2>
<p>torch.utils.data.Dataset: The Coco dataset.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_datasets(self):
    &#34;&#34;&#34;Initialize and get the Coco dataset for training and evaluation.

    Returns:
        torch.utils.data.Dataset: The Coco dataset.
    &#34;&#34;&#34;
    transform = self.get_transform()
    hyperparameters = self.hyperparameters[&#39;datasets&#39;]

    model_classes = self.get_model_hyperparameters()[&#39;classes&#39;]
    dataset_classes = len(hyperparameters[&#39;class_names&#39;])
    if dataset_classes &gt; 0 and model_classes != dataset_classes:
        raise ValueError(&#39; &#39;.join([&#39;The amount of classes for the model ({})&#39;.format(model_classes),
                                   &#39;must be the same to the length of &#34;class_names&#34; in&#39;,
                                   &#39;the &#34;dataset&#34; hyperparameters ({}).&#39;.format(dataset_classes)]))

    return (
        CocoDataset(
            root=hyperparameters[&#39;root&#39;],
            dataset=hyperparameters[&#39;train&#39;],
            classes_names=hyperparameters[&#39;class_names&#39;],
            transform=transform
        ),
        CocoDataset(
            root=hyperparameters[&#39;root&#39;],
            dataset=hyperparameters[&#39;validation&#39;],
            classes_names=hyperparameters[&#39;class_names&#39;],
            transform=transform
        )
    )</code></pre>
</details>
</dd>
<dt id="torchsight.trainers.retinanet.RetinaNetTrainer.get_model"><code class="name flex">
<span>def <span class="ident">get_model</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Initialize and get the RetinaNet.</p>
<h2 id="returns">Returns</h2>
<p>torch.nn.Module: The RetinaNet model.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_model(self):
    &#34;&#34;&#34;Initialize and get the RetinaNet.

    Returns:
        torch.nn.Module: The RetinaNet model.
    &#34;&#34;&#34;
    hyperparameters = self.hyperparameters[&#39;RetinaNet&#39;]
    return RetinaNet(
        classes=hyperparameters[&#39;classes&#39;],
        resnet=hyperparameters[&#39;resnet&#39;],
        features=hyperparameters[&#39;features&#39;],
        anchors=hyperparameters[&#39;anchors&#39;],
        pretrained=hyperparameters[&#39;pretrained&#39;]
    )</code></pre>
</details>
</dd>
<dt id="torchsight.trainers.retinanet.RetinaNetTrainer.get_model_hyperparameters"><code class="name flex">
<span>def <span class="ident">get_model_hyperparameters</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_model_hyperparameters(self):
    return self.hyperparameters[&#39;RetinaNet&#39;]</code></pre>
</details>
</dd>
<dt id="torchsight.trainers.retinanet.RetinaNetTrainer.get_optimizer"><code class="name flex">
<span>def <span class="ident">get_optimizer</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the optimizer of the training.</p>
<p>Stochastic Gradient Descent:
<a href="https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html">https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html</a></p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>optimizer</code></strong> :&ensp;<code>torch.optim.Optimizer</code></dt>
<dd>The optimizer of the training.
For the optimizer package see: <a href="https://pytorch.org/docs/stable/optim.html">https://pytorch.org/docs/stable/optim.html</a></dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_optimizer(self):
    &#34;&#34;&#34;Returns the optimizer of the training.

    Stochastic Gradient Descent:
    https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html

    Returns:
        optimizer (torch.optim.Optimizer): The optimizer of the training.
            For the optimizer package see: https://pytorch.org/docs/stable/optim.html
    &#34;&#34;&#34;
    hyperparameters = self.hyperparameters[&#39;optimizer&#39;]
    return torch.optim.SGD(
        self.model.parameters(),
        lr=hyperparameters[&#39;learning_rate&#39;],
        momentum=hyperparameters[&#39;momentum&#39;],
        weight_decay=hyperparameters[&#39;weight_decay&#39;]
    )</code></pre>
</details>
</dd>
<dt id="torchsight.trainers.retinanet.RetinaNetTrainer.get_scheduler"><code class="name flex">
<span>def <span class="ident">get_scheduler</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Get the learning rate scheduler.</p>
<h2 id="returns">Returns</h2>
<p>torch.optim.lr_scheduler.ReduceLROnPlateau: The learning rate scheduler.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_scheduler(self):
    &#34;&#34;&#34;Get the learning rate scheduler.

    Returns:
        torch.optim.lr_scheduler.ReduceLROnPlateau: The learning rate scheduler.
    &#34;&#34;&#34;
    hyperparameters = self.hyperparameters[&#39;scheduler&#39;]
    return ReduceLROnPlateau(
        optimizer=self.optimizer,
        mode=&#39;min&#39;,
        factor=hyperparameters[&#39;factor&#39;],
        patience=hyperparameters[&#39;patience&#39;],
        verbose=True,
        threshold=hyperparameters[&#39;threshold&#39;]
    )</code></pre>
</details>
</dd>
<dt id="torchsight.trainers.retinanet.RetinaNetTrainer.get_transform"><code class="name flex">
<span>def <span class="ident">get_transform</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Initialize and get the transforms for the images.</p>
<h2 id="returns">Returns</h2>
<p>torchvision.transform.Compose: The Compose of the transformations.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_transform(self):
    &#34;&#34;&#34;Initialize and get the transforms for the images.

    Returns:
        torchvision.transform.Compose: The Compose of the transformations.
    &#34;&#34;&#34;
    hyperparameters = self.hyperparameters[&#39;transforms&#39;]
    return transforms.Compose([
        Resize(**hyperparameters[&#39;resize&#39;]),
        ToTensor(),
        Normalize(**hyperparameters[&#39;normalize&#39;])
    ])</code></pre>
</details>
</dd>
<dt id="torchsight.trainers.retinanet.RetinaNetTrainer.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self, epochs=100, validate=True, epoch_callback=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Train the model for the given epochs.</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>epochs</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of epochs to train.</dd>
<dt><strong><code>validate</code></strong> :&ensp;<code>bool</code></dt>
<dd>If true it validates the model after each epoch using the validate method.</dd>
<dt><strong><code>epoch_callback</code></strong> :&ensp;<code>function</code>, optional</dt>
<dd>An optional function to call after each epoch.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def train(self, epochs=100, validate=True, epoch_callback=None):
    &#34;&#34;&#34;Train the model for the given epochs.

    Arguments:
        epochs (int): The number of epochs to train.
        validate (bool): If true it validates the model after each epoch using the validate method.
        epoch_callback (function, optional): An optional function to call after each epoch.
    &#34;&#34;&#34;
    self.model.to(self.device)
    self.criterion.to(self.device)

    # Weights for each loss, to increase or decrease their values
    weights = self.hyperparameters[&#39;FocalLoss&#39;][&#39;weights&#39;]

    print(&#39;----- Training started ------&#39;)
    print(&#39;Using device: {}&#39;.format(self.device))

    n_batches = len(self.dataloader)
    start_time = time.time()

    for epoch in range(epochs):
        epoch = epoch + 1 + self.checkpoint_epoch
        last_endtime = time.time()

        # Set model to train mode, useful for batch normalization or dropouts modules. For more info see:
        # https://discuss.pytorch.org/t/trying-to-understand-the-meaning-of-model-train-and-model-eval/20158
        self.model.train()

        for batch_index, (images, annotations, *_) in enumerate(self.dataloader):
            images, annotations = images.to(self.device), annotations.to(self.device)

            # Optimize
            self.optimizer.zero_grad()
            anchors, regressions, classifications = self.forward(images, annotations)
            del images
            classification_loss, regression_loss = self.criterion(anchors, regressions, classifications,
                                                                  annotations)
            del anchors, regressions, classifications, annotations

            classification_loss *= weights[&#39;classification&#39;]
            regression_loss *= weights[&#39;regression&#39;]
            loss = classification_loss + regression_loss
            # Set as float to free memory
            classification_loss = float(classification_loss)
            regression_loss = float(regression_loss)
            # Optimize
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.1)
            self.optimizer.step()

            # Log the batch
            total_time = time.time() - start_time
            batch_time = time.time() - last_endtime
            last_endtime = time.time()
            # Get the actual learning rate (modified by the scheduler)
            learning_rates = [str(param_group[&#39;lr&#39;]) for i, param_group in enumerate(self.optimizer.param_groups)]
            self.logger.log({
                &#39;Training&#39;: None,
                &#39;Epoch&#39;: epoch,
                &#39;Batch&#39;: &#39;{}/{}&#39;.format(batch_index + 1, n_batches),
                &#39;LR&#39;: &#39; &#39;.join(learning_rates),
                &#39;Class.&#39;: &#39;{:.7f}&#39;.format(classification_loss),
                &#39;Regr.&#39;: &#39;{:.7f}&#39;.format(regression_loss),
                &#39;Loss&#39;: &#39;{:.7f}&#39;.format(loss),
                &#39;Time&#39;: &#39;{:.3f} s&#39;.format(batch_time),
                &#39;Total&#39;: &#39;{:.3f} s&#39;.format(total_time)
            })

            # Save the weights for this epoch every some batches
            if batch_index % 100 == 0:
                self.save_checkpoint(epoch)

        # Call the epoch callback
        if epoch_callback is not None:
            epoch_callback(epoch)

        # Save the weights at the end of the epoch
        self.save_checkpoint(epoch)

        if validate:
            validation_loss = self.validate(epoch)
            self.scheduler.step(validation_loss)</code></pre>
</details>
</dd>
<dt id="torchsight.trainers.retinanet.RetinaNetTrainer.validate"><code class="name flex">
<span>def <span class="ident">validate</span></span>(<span>self, epoch)</span>
</code></dt>
<dd>
<section class="desc"><p>Compute the loss over the validation dataset.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def validate(self, epoch):
    &#34;&#34;&#34;Compute the loss over the validation dataset.&#34;&#34;&#34;
    self.model.to(self.device)
    hyperparameters = self.get_model_hyperparameters()[&#39;evaluation&#39;]
    self.model.eval(threshold=hyperparameters[&#39;threshold&#39;],
                    iou_threshold=hyperparameters[&#39;iou_threshold&#39;],
                    loss=True)

    weights = self.hyperparameters[&#39;FocalLoss&#39;][&#39;weights&#39;]

    classification_losses = []
    regression_losses = []
    losses = []

    start_time = time.time()
    last_endtime = time.time()
    n_batches = len(self.valid_dataloader)

    for batch, (images, annotations) in enumerate(self.valid_dataloader):
        images, annotations = images.to(self.device), annotations.to(self.device)

        anchors, regressions, classifications = self.model(images)
        del images

        classification_loss, regression_loss = self.criterion(anchors, regressions, classifications, annotations)
        del anchors, regressions, classifications, annotations

        classification_loss *= weights[&#39;classification&#39;]
        regression_loss *= weights[&#39;regression&#39;]
        classification_loss = float(classification_loss)
        regression_loss = float(regression_loss)

        classification_losses.append(classification_loss)
        regression_losses.append(regression_loss)
        losses.append(classification_loss + regression_loss)

        batch_time = time.time() - last_endtime
        last_endtime = time.time()
        total_time = time.time() - start_time

        self.logger.log({
            &#39;Validating&#39;: None,
            &#39;Epoch&#39;: epoch,
            &#39;Batch&#39;: &#39;{}/{}&#39;.format(batch + 1, n_batches),
            &#39;Class.&#39;: &#39;{:.7f}&#39;.format(classification_loss),
            &#39;Regr.&#39;: &#39;{:.7f}&#39;.format(regression_loss),
            &#39;Loss&#39;: &#39;{:.7f}&#39;.format(losses[-1]),
            &#39;Time&#39;: &#39;{:.3f}&#39;.format(batch_time),
            &#39;Total&#39;: &#39;{:.3f}&#39;.format(total_time)
        })

    return torch.Tensor(losses).mean()</code></pre>
</details>
</dd>
<dt id="torchsight.trainers.retinanet.RetinaNetTrainer.validate_map"><code class="name flex">
<span>def <span class="ident">validate_map</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Compute mAP over validation dataset.</p>
<p>We iterate over the images in the validation dataset, compute the detections using the current
state of the model, generate the detections tensor and compute the mAP using the MeanAP class.</p>
<p>As the MeanAP class does not computes the mAP for each class (it computes for the entire image),
we must filter the annotations by each class, compute mAP and store the value for the class and
continue.</p>
<h2 id="returns">Returns</h2>
<p>torch.Tensor: The mAP averaged over all the classes.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def validate_map(self):
    &#34;&#34;&#34;Compute mAP over validation dataset.

    We iterate over the images in the validation dataset, compute the detections using the current
    state of the model, generate the detections tensor and compute the mAP using the MeanAP class.

    As the MeanAP class does not computes the mAP for each class (it computes for the entire image),
    we must filter the annotations by each class, compute mAP and store the value for the class and
    continue.

    Returns:
        torch.Tensor: The mAP averaged over all the classes.
    &#34;&#34;&#34;
    print(&#39;--------- VALIDATING --------&#39;)

    self.model.to(self.device)
    self.model.eval(**self.hyperparameters[&#39;RetinaNet&#39;][&#39;evaluation&#39;])

    mAP = {}  # The array of mAP per each class per each image
    # mAP is something like {&#39;0&#39;: [0.15, ..., 0.87]} where the length of the list is the number of images
    # in the validation dataset
    aps = {}  # The array with the Average Precision for each IoU threshold, for each image, for each class
    # aps is something like {&#39;0&#39;: [[0.67, ..., 0.54], ..., []]} where the length of the bigger array is the
    # number of images that contains that label and the inner arrays is the number of IoU thresholds to compute
    # the Average Precisions

    for batch_index, (images, annotations) in enumerate(self.valid_dataloader):
        images, annotations = images.to(self.device), annotations.to(self.device)
        for index, (boxes, classifications) in enumerate(self.model(images)):
            # Generate the detections
            detections = torch.zeros((boxes.shape[0], 6)).to(self.device)
            detections[:, :4] = boxes
            prob, label = classifications.max(dim=1)
            detections[:, 4] = label
            detections[:, 5] = prob

            # Get the actual annotations, clean and iterate over each unique label
            actual_annotations = annotations[index].clone()
            # Remove dummy annotations created by the data loader (label == -1)
            mask = actual_annotations[:, -1] == -1
            actual_annotations = actual_annotations[mask]
            # Get the true labels in the actual annotation
            labels = [int(label) for label in actual_annotations[:, -1].unique()]
            # Iterate over each label to compute mAP per class
            for label in labels:
                if label not in mAP:
                    mAP[label] = []
                if label not in aps:
                    aps[label] = []
                # Add zero values if there are no detections
                if not boxes.shape[0] &gt; 0:
                    mAP[label].append(torch.zeros((1)).mean().to(self.device))
                    aps[label].append(torch.zeros((self.compute_map.iou_thresholds.shape[0])).to(self.device))
                    continue
                # Compute mAP
                actual_map, actual_aps = self.compute_map(actual_annotations, detections)
                mAP[label].append(actual_map)
                aps[label].append(actual_aps)

        print(&#39;[Validating] [Batch {}]&#39;.format(batch_index))

    # Set the model to train again
    self.model.train()
    # Compute the average of the map over all the classes
    final_map = [torch.stack(mAP[label]).mean() for label in mAP]
    final_aps = {label: torch.stack(aps[label]).mean(dim=0) for label in aps}
    return torch.stack(final_map).mean(), final_aps</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="torchsight.trainers.abstract.AbstractTrainer" href="abstract.html#torchsight.trainers.abstract.AbstractTrainer">AbstractTrainer</a></b></code>:
<ul class="hlist">
<li><code><a title="torchsight.trainers.abstract.AbstractTrainer.merge_hyperparameters" href="abstract.html#torchsight.trainers.abstract.AbstractTrainer.merge_hyperparameters">merge_hyperparameters</a></code></li>
<li><code><a title="torchsight.trainers.abstract.AbstractTrainer.resume" href="abstract.html#torchsight.trainers.abstract.AbstractTrainer.resume">resume</a></code></li>
<li><code><a title="torchsight.trainers.abstract.AbstractTrainer.save_checkpoint" href="abstract.html#torchsight.trainers.abstract.AbstractTrainer.save_checkpoint">save_checkpoint</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="torchsight.trainers" href="index.html">torchsight.trainers</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="torchsight.trainers.retinanet.RetinaNetTrainer" href="#torchsight.trainers.retinanet.RetinaNetTrainer">RetinaNetTrainer</a></code></h4>
<ul class="">
<li><code><a title="torchsight.trainers.retinanet.RetinaNetTrainer.__init__" href="#torchsight.trainers.retinanet.RetinaNetTrainer.__init__">__init__</a></code></li>
<li><code><a title="torchsight.trainers.retinanet.RetinaNetTrainer.forward" href="#torchsight.trainers.retinanet.RetinaNetTrainer.forward">forward</a></code></li>
<li><code><a title="torchsight.trainers.retinanet.RetinaNetTrainer.get_criterion" href="#torchsight.trainers.retinanet.RetinaNetTrainer.get_criterion">get_criterion</a></code></li>
<li><code><a title="torchsight.trainers.retinanet.RetinaNetTrainer.get_dataloaders" href="#torchsight.trainers.retinanet.RetinaNetTrainer.get_dataloaders">get_dataloaders</a></code></li>
<li><code><a title="torchsight.trainers.retinanet.RetinaNetTrainer.get_datasets" href="#torchsight.trainers.retinanet.RetinaNetTrainer.get_datasets">get_datasets</a></code></li>
<li><code><a title="torchsight.trainers.retinanet.RetinaNetTrainer.get_model" href="#torchsight.trainers.retinanet.RetinaNetTrainer.get_model">get_model</a></code></li>
<li><code><a title="torchsight.trainers.retinanet.RetinaNetTrainer.get_model_hyperparameters" href="#torchsight.trainers.retinanet.RetinaNetTrainer.get_model_hyperparameters">get_model_hyperparameters</a></code></li>
<li><code><a title="torchsight.trainers.retinanet.RetinaNetTrainer.get_optimizer" href="#torchsight.trainers.retinanet.RetinaNetTrainer.get_optimizer">get_optimizer</a></code></li>
<li><code><a title="torchsight.trainers.retinanet.RetinaNetTrainer.get_scheduler" href="#torchsight.trainers.retinanet.RetinaNetTrainer.get_scheduler">get_scheduler</a></code></li>
<li><code><a title="torchsight.trainers.retinanet.RetinaNetTrainer.get_transform" href="#torchsight.trainers.retinanet.RetinaNetTrainer.get_transform">get_transform</a></code></li>
<li><code><a title="torchsight.trainers.retinanet.RetinaNetTrainer.hyperparameters" href="#torchsight.trainers.retinanet.RetinaNetTrainer.hyperparameters">hyperparameters</a></code></li>
<li><code><a title="torchsight.trainers.retinanet.RetinaNetTrainer.train" href="#torchsight.trainers.retinanet.RetinaNetTrainer.train">train</a></code></li>
<li><code><a title="torchsight.trainers.retinanet.RetinaNetTrainer.validate" href="#torchsight.trainers.retinanet.RetinaNetTrainer.validate">validate</a></code></li>
<li><code><a title="torchsight.trainers.retinanet.RetinaNetTrainer.validate_map" href="#torchsight.trainers.retinanet.RetinaNetTrainer.validate_map">validate_map</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.5.4</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>