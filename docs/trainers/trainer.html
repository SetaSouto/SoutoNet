<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.5.4" />
<title>torchsight.trainers.trainer API documentation</title>
<meta name="description" content="Abstract trainer module â€¦" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.name small{font-weight:normal}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase;cursor:pointer}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title"><code>torchsight.trainers.trainer</code> module</h1>
</header>
<section id="section-intro">
<p>Abstract trainer module.</p>
<h2 id="quick-start">Quick start</h2>
<p>A trainer has getters and methods:
- The getters are for get the different modules necessary for the training like <em>datasets</em>, <em>dataloaders</em>,
<em>model</em>, <em>criterion</em> and <em>optimizer</em>. Optionally you can provide a <em>logger</em> and a <em>learning rate scheduler</em>.
- The methods are used to optimize and evaluate the model.
- The <strong>train</strong> method does the classic training algorithm.
- The <strong>validate</strong> method does the validation of the model over the validation dataset.
- The <strong>eval</strong> method puts the model into evaluation mode.
- The <strong>forward</strong> method does a forward pass over the model and returns the loss tensor. <strong>You must implement
this method</strong>.
- The <strong>backward</strong> method does the backward propagation of the loss.</p>
<p>To use a trainer you must implement the getters methods and the <em>forward</em> method.</p>
<p>A good practice is to use the hyperparameters dict to store the parameters for the getters, so anyone can change
the hyperparameters without changing the code.</p>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">&#34;&#34;&#34;Abstract trainer module.

## Quick start

A trainer has getters and methods:
- The getters are for get the different modules necessary for the training like *datasets*, *dataloaders*,
*model*, *criterion* and *optimizer*. Optionally you can provide a *logger* and a *learning rate scheduler*.
- The methods are used to optimize and evaluate the model.
  - The **train** method does the classic training algorithm.
  - The **validate** method does the validation of the model over the validation dataset.
  - The **eval** method puts the model into evaluation mode.
  - The **forward** method does a forward pass over the model and returns the loss tensor. **You must implement
    this method**.
  - The **backward** method does the backward propagation of the loss.

To use a trainer you must implement the getters methods and the *forward* method.

A good practice is to use the hyperparameters dict to store the parameters for the getters, so anyone can change
the hyperparameters without changing the code.
&#34;&#34;&#34;
import json
import os
import time

import torch

from torchsight.loggers import PrintLogger
from torchsight.utils import merge_dicts

LOGS_DIR = &#39;./logs&#39;


class Trainer():
    &#34;&#34;&#34;Base Trainer class, all the trainers must extend this class.&#34;&#34;&#34;
    # A dict with all the hyperparameters for the different components of the training
    hyperparameters = {}

    def __init__(self, hyperparameters=None, checkpoint=None, device=None):
        &#34;&#34;&#34;Initialize the trainer.

        Arguments:
            hyperparameters (dict, optional): A dict to change the base hyperparameters.
                If it&#39;s present, it will be deeply merged with the base hyperparameters.
        &#34;&#34;&#34;
        base_hyperparameters = {&#39;checkpoint&#39;: {&#39;dir&#39;: LOGS_DIR, &#39;verbose&#39;: True},
                                &#39;logger&#39;: {&#39;dir&#39;: LOGS_DIR}}
        # Add the base hyperparameters to the trainer hyperparameters
        self.hyperparameters = merge_dicts(self.hyperparameters, base_hyperparameters)
        # Add the modified hyperparameters given in the initialization
        self.hyperparameters = merge_dicts(self.hyperparameters, hyperparameters, verbose=True)
        # Set the device of the trainer
        self.device = device if device is not None else &#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39;

        # Get the modules for the training
        self.dataset, self.valid_dataset = self.get_datasets()
        self.dataloader, self.valid_dataloader = self.get_dataloaders()
        self.model = self.get_model()
        self.criterion = self.get_criterion()
        self.optimizer = self.get_optimizer()
        self.scheduler = self.get_scheduler()

        # Load the checkpoint
        self.checkpoint = self.resume(checkpoint)
        # Get the logger
        self.logger = self.get_logger()
        # As we only log one time per batch we need to keep the state of all the elements that we want to log
        self.current_log = {}

    ####################################
    ###           GETTERS            ###
    ####################################

    def get_datasets(self):
        &#34;&#34;&#34;Get the training and validation datasets.

        Returns:
            tuple: A Tuple with the torch.utils.data.Datasets for training and validation.
        &#34;&#34;&#34;
        raise NotImplementedError(&#39;You must provide your own datasets.&#39;)

    def get_dataloaders(self):
        &#34;&#34;&#34;Get the dataloaders for training and validation.

        Returns:
            tuple: The tuple with the torch.utils.data.DataLoader for training and validation.
        &#34;&#34;&#34;
        raise NotImplementedError(&#39;You must provide the dataloaders for the datasets.&#39;)

    def get_model(self):
        &#34;&#34;&#34;Get the model to train.

        Returns:
            torch.nn.Module: The model to train.
        &#34;&#34;&#34;
        raise NotImplementedError(&#39;You must provide the model to train.&#39;)

    def get_criterion(self):
        &#34;&#34;&#34;Get the criterion to use to train the model.

        Returns:
            torch.nn.Module: The criterion to use in the training.
        &#34;&#34;&#34;
        return NotImplementedError(&#39;You must provide the criterion to train the model.&#39;)

    def get_optimizer(self):
        &#34;&#34;&#34;Get the optimizer of the model.

        Returns:
            torch.optim.Optimizer: The optimizer of the model&#39;s parameters.
        &#34;&#34;&#34;
        raise NotImplementedError(&#39;You must provide the optimizer to use during the training.&#39;)

    def get_scheduler(self):
        &#34;&#34;&#34;Get the (optional) scheduler for the learning rate of the optimizer.

        Returns:
            torch.optim.lr_scheduler._LRScheduler: A scheduler for the learning rate.
        &#34;&#34;&#34;
        # No error because the scheduler is optional.

    def get_logger(self):
        &#34;&#34;&#34;Get the (optional) logger to use during the training to show the information about the process.

        This base implementation uses the PrintLogger that will print the log to the console.

        Returns:
            pymatch.loggers.Logger: A Logger to use during the training.
        &#34;&#34;&#34;
        description = &#39;Hyperparameters:\n{}&#39;.format(json.dumps(self.hyperparameters, indent=2))
        return PrintLogger(description, self.hyperparameters[&#39;logger&#39;][&#39;dir&#39;])

    ####################################
    ###           METHODS            ###
    ####################################

    def train(self, epochs=100, validate=True):
        &#34;&#34;&#34;Train the model during the giving epochs.

        Arguments:
            epochs (int, optional): The number of epochs to run the model.
            validate (bool, optional): If it&#39;s True the trainer will validate the training using
                the validate() method. And if there is a scheduler it gives the validation loss
                generated by the validate() method to the scheduler to adjust the learning rate.
        &#34;&#34;&#34;
        self.model.to(self.device)

        # The criterion could be inside the model for example and in that case it could be None
        if self.criterion is not None:
            self.criterion.to(self.device)

        # The number of batches that the training dataset have
        n_batches = len(self.dataloader)

        # The start time of the training and the last batch&#39;s end time
        start_time = time.time()
        last_endtime = start_time

        # We start from the next epoch of the checkpoint (if there is any)
        start_epoch = 1 if self.checkpoint is None else self.checkpoint[&#39;epoch&#39;] + 1

        for epoch in range(start_epoch, start_epoch + epochs):
            # Indicate to the model that we are in training mode, useful for batch normalization or dropouts modules.
            # For more info see:
            # https://discuss.pytorch.org/t/trying-to-understand-the-meaning-of-model-train-and-model-eval/20158
            self.model.train()

            for batch, data in enumerate(self.dataloader):
                # Optimize
                self.optimizer.zero_grad()
                loss = self.forward(*data)
                self.backward(loss)
                self.optimizer.step()

                # Log the batch
                learning_rates = [str(param_group[&#39;lr&#39;])
                                  for i, param_group in enumerate(self.optimizer.param_groups)]

                total_time = time.time() - start_time
                batch_time = time.time() - last_endtime
                last_endtime = time.time()

                self.logger.log(merge_dicts({
                    &#39;Training&#39;: None,
                    &#39;Epoch&#39;: epoch,
                    &#39;Batch&#39;: &#39;{}/{}&#39;.format(batch + 1, n_batches),
                    &#39;LR&#39;: &#39; &#39;.join(learning_rates),
                    &#39;Loss&#39;: &#39;{:.7f}&#39;.format(float(loss)),
                    &#39;Time&#39;: &#39;{:.3f} s&#39;.format(batch_time),
                    &#39;Total&#39;: &#39;{:.1f} s&#39;.format(total_time)
                }, self.current_log))
                self.current_log = {}  # Restart the log dict for the next batch

                # Call the callback for the batch
                self.batch_callback(batch, epoch)

            # Call the callback for the epoch
            self.epoch_callback(epoch)

            # Save the checkpoint for this epoch
            self.save(epoch)

            if validate:
                loss = self.validate(epoch)
                if self.scheduler is not None:
                    self.scheduler.step(loss)

    def forward(self, *args):
        &#34;&#34;&#34;Do a forward pass over the model with the model and get the loss value.

        Arguments:
            *args: All the data that the dataloader generates while iterating over it.

        Returns:
            torch.Tensor: The loss value of the forward pass.
        &#34;&#34;&#34;
        raise NotImplementedError(&#39;You must implement the forward pass over the model.&#39;)

    def backward(self, loss):
        &#34;&#34;&#34;Do the backward pass over the network.

        There is a method for this because each experiment could do different things during the backward like:
        ```python
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.1)
        ```

        But in this case it only does the backward of the loss.

        Arguments:
            loss (torch.Tensor): The loss value computed during the forward pass.
        &#34;&#34;&#34;
        loss.backward()

    def eval(self):
        &#34;&#34;&#34;Set the model into evaluation mode.

        It&#39;s a method to override this and provide a custom eval() call if you want.
        &#34;&#34;&#34;
        self.model.eval()

    def validate(self, epoch):
        &#34;&#34;&#34;Run the model over the validation dataset and return the mean loss over it.&#34;&#34;&#34;
        self.model.to(self.device)
        self.eval()

        start_time = time.time()
        last_endtime = start_time

        n_batches = len(self.valid_dataloader)

        losses = []

        with torch.no_grad():
            for batch, data in enumerate(self.valid_dataloader):
                loss = float(self.forward(*data))

                batch_time = time.time() - last_endtime
                last_endtime = time.time()
                total_time = time.time() - start_time

                self.logger.log(merge_dicts({
                    &#39;Validating&#39;: None,
                    &#39;Epoch&#39;: epoch,
                    &#39;Batch&#39;: &#39;{}/{}&#39;.format(batch + 1, n_batches),
                    &#39;Loss&#39;: &#39;{:.7f}&#39;.format(float(loss)),
                    &#39;Time&#39;: &#39;{:.3f} s&#39;.format(batch_time),
                    &#39;Total&#39;: &#39;{:.1f} s&#39;.format(total_time)
                }, self.current_log))
                self.current_log = {}  # Restart the log dict for the next batch

                losses.append(loss)

        return torch.Tensor(losses).mean()

    def batch_callback(self, batch, epoch):
        &#34;&#34;&#34;Method that is called after a batch has finished its process.&#34;&#34;&#34;

    def epoch_callback(self, epoch):
        &#34;&#34;&#34;Method that is called after an epoch has finished its process.&#34;&#34;&#34;

    def save(self, epoch):
        &#34;&#34;&#34;Save the checkpoint of the trainer.

        The checkpoint is a dict like:
        {&#39;epoch&#39;: int, &#39;model&#39;: state_dict, &#39;optimizer&#39;: state_dict, &#39;scheduler&#39;: state_dict}
        where the scheduler is optional.

        Arguments:
            epoch (int): The epoch that has finished.
        &#34;&#34;&#34;
        params = self.hyperparameters[&#39;checkpoint&#39;]
        path = os.path.join(params[&#39;dir&#39;], &#39;checkpoint_epoch_{}.pth.tar&#39;.format(epoch))

        if params[&#39;verbose&#39;]:
            print(&#39;[Epoch {}] Saving checkpoint to: {}&#39;.format(epoch, path))

        checkpoint = {&#39;epoch&#39;: epoch,
                      &#39;model&#39;: self.model.state_dict(),
                      &#39;optimizer&#39;: self.optimizer.state_dict(),
                      &#39;hyperparameters&#39;: self.hyperparameters}

        if self.scheduler is not None:
            checkpoint[&#39;scheduler&#39;] = self.scheduler.state_dict()

        torch.save(checkpoint, path)

    def resume(self, checkpoint):
        &#34;&#34;&#34;Resume the training based on a last checkpoint and get the checkpoint dict.

        This method does only return the epoch value in the dict to avoid memory leaks, we don&#39;t need
        to keep the state_dicts in memory.
        You can customize your own trainer and return more values.

        Arguments:
            checkpoint (str): The path to the checkpoint file.

        Returns:
            dict: A dict with the epoch only. The state dict are not returned to not keep them
                in memory.
        &#34;&#34;&#34;
        if checkpoint is None:
            return None

        verbose = self.hyperparameters[&#39;checkpoint&#39;][&#39;verbose&#39;]

        if verbose:
            print(&#39;Loading checkpoint from {}&#39;.format(checkpoint))

        checkpoint = torch.load(checkpoint, map_location=self.device)

        self.model.load_state_dict(checkpoint[&#39;model&#39;])
        self.optimizer.load_state_dict(checkpoint[&#39;optimizer&#39;])
        for state in self.optimizer.state.values():
            for k, val in state.items():
                if torch.is_tensor(val):
                    state[k] = val.to(self.device)

        if &#39;scheduler&#39; in checkpoint:
            self.scheduler.load_state_dict(checkpoint[&#39;scheduler&#39;])

        return {&#39;epoch&#39;: checkpoint[&#39;epoch&#39;]}

    @classmethod
    def from_checkpoint(cls, checkpoint, new_params=None, device=None, verbose=True):
        &#34;&#34;&#34;Get an instance of the trainer based on the given checkpoint file.

        This is very useful because the checkpoint saves the hyperparameters too,
        so you have a trainer with the same hyperparameters that one from the checkpoint.

        Also, you can use this method to load the model, because you can do
        `trainer.model` to get the model instance.

        Arguments:
            checkpoint (str): The path to the file that contains the checkpoint file.
            new_params (dict, optional): A dict with new hyperparameters to change the ones
                in the checkpoint. Useful for example to change the batch size, the dataset root,
                etc.

        Returns:
            Trainer: An instance of the trainer with the exact same hyperparameters and with
                the modules with their state_dicts from the checkpoint too.
        &#34;&#34;&#34;
        hyperparameters = merge_dicts(torch.load(checkpoint)[&#39;hyperparameters&#39;], new_params, verbose)

        return cls(hyperparameters=hyperparameters, checkpoint=checkpoint, device=device)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="torchsight.trainers.trainer.Trainer"><code class="flex name class">
<span>class <span class="ident">Trainer</span></span>
</code></dt>
<dd>
<section class="desc"><p>Base Trainer class, all the trainers must extend this class.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class Trainer():
    &#34;&#34;&#34;Base Trainer class, all the trainers must extend this class.&#34;&#34;&#34;
    # A dict with all the hyperparameters for the different components of the training
    hyperparameters = {}

    def __init__(self, hyperparameters=None, checkpoint=None, device=None):
        &#34;&#34;&#34;Initialize the trainer.

        Arguments:
            hyperparameters (dict, optional): A dict to change the base hyperparameters.
                If it&#39;s present, it will be deeply merged with the base hyperparameters.
        &#34;&#34;&#34;
        base_hyperparameters = {&#39;checkpoint&#39;: {&#39;dir&#39;: LOGS_DIR, &#39;verbose&#39;: True},
                                &#39;logger&#39;: {&#39;dir&#39;: LOGS_DIR}}
        # Add the base hyperparameters to the trainer hyperparameters
        self.hyperparameters = merge_dicts(self.hyperparameters, base_hyperparameters)
        # Add the modified hyperparameters given in the initialization
        self.hyperparameters = merge_dicts(self.hyperparameters, hyperparameters, verbose=True)
        # Set the device of the trainer
        self.device = device if device is not None else &#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39;

        # Get the modules for the training
        self.dataset, self.valid_dataset = self.get_datasets()
        self.dataloader, self.valid_dataloader = self.get_dataloaders()
        self.model = self.get_model()
        self.criterion = self.get_criterion()
        self.optimizer = self.get_optimizer()
        self.scheduler = self.get_scheduler()

        # Load the checkpoint
        self.checkpoint = self.resume(checkpoint)
        # Get the logger
        self.logger = self.get_logger()
        # As we only log one time per batch we need to keep the state of all the elements that we want to log
        self.current_log = {}

    ####################################
    ###           GETTERS            ###
    ####################################

    def get_datasets(self):
        &#34;&#34;&#34;Get the training and validation datasets.

        Returns:
            tuple: A Tuple with the torch.utils.data.Datasets for training and validation.
        &#34;&#34;&#34;
        raise NotImplementedError(&#39;You must provide your own datasets.&#39;)

    def get_dataloaders(self):
        &#34;&#34;&#34;Get the dataloaders for training and validation.

        Returns:
            tuple: The tuple with the torch.utils.data.DataLoader for training and validation.
        &#34;&#34;&#34;
        raise NotImplementedError(&#39;You must provide the dataloaders for the datasets.&#39;)

    def get_model(self):
        &#34;&#34;&#34;Get the model to train.

        Returns:
            torch.nn.Module: The model to train.
        &#34;&#34;&#34;
        raise NotImplementedError(&#39;You must provide the model to train.&#39;)

    def get_criterion(self):
        &#34;&#34;&#34;Get the criterion to use to train the model.

        Returns:
            torch.nn.Module: The criterion to use in the training.
        &#34;&#34;&#34;
        return NotImplementedError(&#39;You must provide the criterion to train the model.&#39;)

    def get_optimizer(self):
        &#34;&#34;&#34;Get the optimizer of the model.

        Returns:
            torch.optim.Optimizer: The optimizer of the model&#39;s parameters.
        &#34;&#34;&#34;
        raise NotImplementedError(&#39;You must provide the optimizer to use during the training.&#39;)

    def get_scheduler(self):
        &#34;&#34;&#34;Get the (optional) scheduler for the learning rate of the optimizer.

        Returns:
            torch.optim.lr_scheduler._LRScheduler: A scheduler for the learning rate.
        &#34;&#34;&#34;
        # No error because the scheduler is optional.

    def get_logger(self):
        &#34;&#34;&#34;Get the (optional) logger to use during the training to show the information about the process.

        This base implementation uses the PrintLogger that will print the log to the console.

        Returns:
            pymatch.loggers.Logger: A Logger to use during the training.
        &#34;&#34;&#34;
        description = &#39;Hyperparameters:\n{}&#39;.format(json.dumps(self.hyperparameters, indent=2))
        return PrintLogger(description, self.hyperparameters[&#39;logger&#39;][&#39;dir&#39;])

    ####################################
    ###           METHODS            ###
    ####################################

    def train(self, epochs=100, validate=True):
        &#34;&#34;&#34;Train the model during the giving epochs.

        Arguments:
            epochs (int, optional): The number of epochs to run the model.
            validate (bool, optional): If it&#39;s True the trainer will validate the training using
                the validate() method. And if there is a scheduler it gives the validation loss
                generated by the validate() method to the scheduler to adjust the learning rate.
        &#34;&#34;&#34;
        self.model.to(self.device)

        # The criterion could be inside the model for example and in that case it could be None
        if self.criterion is not None:
            self.criterion.to(self.device)

        # The number of batches that the training dataset have
        n_batches = len(self.dataloader)

        # The start time of the training and the last batch&#39;s end time
        start_time = time.time()
        last_endtime = start_time

        # We start from the next epoch of the checkpoint (if there is any)
        start_epoch = 1 if self.checkpoint is None else self.checkpoint[&#39;epoch&#39;] + 1

        for epoch in range(start_epoch, start_epoch + epochs):
            # Indicate to the model that we are in training mode, useful for batch normalization or dropouts modules.
            # For more info see:
            # https://discuss.pytorch.org/t/trying-to-understand-the-meaning-of-model-train-and-model-eval/20158
            self.model.train()

            for batch, data in enumerate(self.dataloader):
                # Optimize
                self.optimizer.zero_grad()
                loss = self.forward(*data)
                self.backward(loss)
                self.optimizer.step()

                # Log the batch
                learning_rates = [str(param_group[&#39;lr&#39;])
                                  for i, param_group in enumerate(self.optimizer.param_groups)]

                total_time = time.time() - start_time
                batch_time = time.time() - last_endtime
                last_endtime = time.time()

                self.logger.log(merge_dicts({
                    &#39;Training&#39;: None,
                    &#39;Epoch&#39;: epoch,
                    &#39;Batch&#39;: &#39;{}/{}&#39;.format(batch + 1, n_batches),
                    &#39;LR&#39;: &#39; &#39;.join(learning_rates),
                    &#39;Loss&#39;: &#39;{:.7f}&#39;.format(float(loss)),
                    &#39;Time&#39;: &#39;{:.3f} s&#39;.format(batch_time),
                    &#39;Total&#39;: &#39;{:.1f} s&#39;.format(total_time)
                }, self.current_log))
                self.current_log = {}  # Restart the log dict for the next batch

                # Call the callback for the batch
                self.batch_callback(batch, epoch)

            # Call the callback for the epoch
            self.epoch_callback(epoch)

            # Save the checkpoint for this epoch
            self.save(epoch)

            if validate:
                loss = self.validate(epoch)
                if self.scheduler is not None:
                    self.scheduler.step(loss)

    def forward(self, *args):
        &#34;&#34;&#34;Do a forward pass over the model with the model and get the loss value.

        Arguments:
            *args: All the data that the dataloader generates while iterating over it.

        Returns:
            torch.Tensor: The loss value of the forward pass.
        &#34;&#34;&#34;
        raise NotImplementedError(&#39;You must implement the forward pass over the model.&#39;)

    def backward(self, loss):
        &#34;&#34;&#34;Do the backward pass over the network.

        There is a method for this because each experiment could do different things during the backward like:
        ```python
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.1)
        ```

        But in this case it only does the backward of the loss.

        Arguments:
            loss (torch.Tensor): The loss value computed during the forward pass.
        &#34;&#34;&#34;
        loss.backward()

    def eval(self):
        &#34;&#34;&#34;Set the model into evaluation mode.

        It&#39;s a method to override this and provide a custom eval() call if you want.
        &#34;&#34;&#34;
        self.model.eval()

    def validate(self, epoch):
        &#34;&#34;&#34;Run the model over the validation dataset and return the mean loss over it.&#34;&#34;&#34;
        self.model.to(self.device)
        self.eval()

        start_time = time.time()
        last_endtime = start_time

        n_batches = len(self.valid_dataloader)

        losses = []

        with torch.no_grad():
            for batch, data in enumerate(self.valid_dataloader):
                loss = float(self.forward(*data))

                batch_time = time.time() - last_endtime
                last_endtime = time.time()
                total_time = time.time() - start_time

                self.logger.log(merge_dicts({
                    &#39;Validating&#39;: None,
                    &#39;Epoch&#39;: epoch,
                    &#39;Batch&#39;: &#39;{}/{}&#39;.format(batch + 1, n_batches),
                    &#39;Loss&#39;: &#39;{:.7f}&#39;.format(float(loss)),
                    &#39;Time&#39;: &#39;{:.3f} s&#39;.format(batch_time),
                    &#39;Total&#39;: &#39;{:.1f} s&#39;.format(total_time)
                }, self.current_log))
                self.current_log = {}  # Restart the log dict for the next batch

                losses.append(loss)

        return torch.Tensor(losses).mean()

    def batch_callback(self, batch, epoch):
        &#34;&#34;&#34;Method that is called after a batch has finished its process.&#34;&#34;&#34;

    def epoch_callback(self, epoch):
        &#34;&#34;&#34;Method that is called after an epoch has finished its process.&#34;&#34;&#34;

    def save(self, epoch):
        &#34;&#34;&#34;Save the checkpoint of the trainer.

        The checkpoint is a dict like:
        {&#39;epoch&#39;: int, &#39;model&#39;: state_dict, &#39;optimizer&#39;: state_dict, &#39;scheduler&#39;: state_dict}
        where the scheduler is optional.

        Arguments:
            epoch (int): The epoch that has finished.
        &#34;&#34;&#34;
        params = self.hyperparameters[&#39;checkpoint&#39;]
        path = os.path.join(params[&#39;dir&#39;], &#39;checkpoint_epoch_{}.pth.tar&#39;.format(epoch))

        if params[&#39;verbose&#39;]:
            print(&#39;[Epoch {}] Saving checkpoint to: {}&#39;.format(epoch, path))

        checkpoint = {&#39;epoch&#39;: epoch,
                      &#39;model&#39;: self.model.state_dict(),
                      &#39;optimizer&#39;: self.optimizer.state_dict(),
                      &#39;hyperparameters&#39;: self.hyperparameters}

        if self.scheduler is not None:
            checkpoint[&#39;scheduler&#39;] = self.scheduler.state_dict()

        torch.save(checkpoint, path)

    def resume(self, checkpoint):
        &#34;&#34;&#34;Resume the training based on a last checkpoint and get the checkpoint dict.

        This method does only return the epoch value in the dict to avoid memory leaks, we don&#39;t need
        to keep the state_dicts in memory.
        You can customize your own trainer and return more values.

        Arguments:
            checkpoint (str): The path to the checkpoint file.

        Returns:
            dict: A dict with the epoch only. The state dict are not returned to not keep them
                in memory.
        &#34;&#34;&#34;
        if checkpoint is None:
            return None

        verbose = self.hyperparameters[&#39;checkpoint&#39;][&#39;verbose&#39;]

        if verbose:
            print(&#39;Loading checkpoint from {}&#39;.format(checkpoint))

        checkpoint = torch.load(checkpoint, map_location=self.device)

        self.model.load_state_dict(checkpoint[&#39;model&#39;])
        self.optimizer.load_state_dict(checkpoint[&#39;optimizer&#39;])
        for state in self.optimizer.state.values():
            for k, val in state.items():
                if torch.is_tensor(val):
                    state[k] = val.to(self.device)

        if &#39;scheduler&#39; in checkpoint:
            self.scheduler.load_state_dict(checkpoint[&#39;scheduler&#39;])

        return {&#39;epoch&#39;: checkpoint[&#39;epoch&#39;]}

    @classmethod
    def from_checkpoint(cls, checkpoint, new_params=None, device=None, verbose=True):
        &#34;&#34;&#34;Get an instance of the trainer based on the given checkpoint file.

        This is very useful because the checkpoint saves the hyperparameters too,
        so you have a trainer with the same hyperparameters that one from the checkpoint.

        Also, you can use this method to load the model, because you can do
        `trainer.model` to get the model instance.

        Arguments:
            checkpoint (str): The path to the file that contains the checkpoint file.
            new_params (dict, optional): A dict with new hyperparameters to change the ones
                in the checkpoint. Useful for example to change the batch size, the dataset root,
                etc.

        Returns:
            Trainer: An instance of the trainer with the exact same hyperparameters and with
                the modules with their state_dicts from the checkpoint too.
        &#34;&#34;&#34;
        hyperparameters = merge_dicts(torch.load(checkpoint)[&#39;hyperparameters&#39;], new_params, verbose)

        return cls(hyperparameters=hyperparameters, checkpoint=checkpoint, device=device)</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="torchsight.trainers.trainer.Trainer.hyperparameters"><code class="name">var <span class="ident">hyperparameters</span></code></dt>
<dd>
<section class="desc"></section>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="torchsight.trainers.trainer.Trainer.from_checkpoint"><code class="name flex">
<span>def <span class="ident">from_checkpoint</span></span>(<span>cls, checkpoint, new_params=None, device=None, verbose=True)</span>
</code></dt>
<dd>
<section class="desc"><p>Get an instance of the trainer based on the given checkpoint file.</p>
<p>This is very useful because the checkpoint saves the hyperparameters too,
so you have a trainer with the same hyperparameters that one from the checkpoint.</p>
<p>Also, you can use this method to load the model, because you can do
<code>trainer.model</code> to get the model instance.</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>checkpoint</code></strong> :&ensp;<code>str</code></dt>
<dd>The path to the file that contains the checkpoint file.</dd>
<dt><strong><code>new_params</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>A dict with new hyperparameters to change the ones
in the checkpoint. Useful for example to change the batch size, the dataset root,
etc.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><a title="torchsight.trainers.trainer.Trainer" href="#torchsight.trainers.trainer.Trainer"><code>Trainer</code></a></strong></dt>
<dd>An instance of the trainer with the exact same hyperparameters and with
the modules with their state_dicts from the checkpoint too.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">@classmethod
def from_checkpoint(cls, checkpoint, new_params=None, device=None, verbose=True):
    &#34;&#34;&#34;Get an instance of the trainer based on the given checkpoint file.

    This is very useful because the checkpoint saves the hyperparameters too,
    so you have a trainer with the same hyperparameters that one from the checkpoint.

    Also, you can use this method to load the model, because you can do
    `trainer.model` to get the model instance.

    Arguments:
        checkpoint (str): The path to the file that contains the checkpoint file.
        new_params (dict, optional): A dict with new hyperparameters to change the ones
            in the checkpoint. Useful for example to change the batch size, the dataset root,
            etc.

    Returns:
        Trainer: An instance of the trainer with the exact same hyperparameters and with
            the modules with their state_dicts from the checkpoint too.
    &#34;&#34;&#34;
    hyperparameters = merge_dicts(torch.load(checkpoint)[&#39;hyperparameters&#39;], new_params, verbose)

    return cls(hyperparameters=hyperparameters, checkpoint=checkpoint, device=device)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="torchsight.trainers.trainer.Trainer.__init__"><code class="name flex">
<span>def <span class="ident">__init__</span></span>(<span>self, hyperparameters=None, checkpoint=None, device=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Initialize the trainer.</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>hyperparameters</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>A dict to change the base hyperparameters.
If it's present, it will be deeply merged with the base hyperparameters.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def __init__(self, hyperparameters=None, checkpoint=None, device=None):
    &#34;&#34;&#34;Initialize the trainer.

    Arguments:
        hyperparameters (dict, optional): A dict to change the base hyperparameters.
            If it&#39;s present, it will be deeply merged with the base hyperparameters.
    &#34;&#34;&#34;
    base_hyperparameters = {&#39;checkpoint&#39;: {&#39;dir&#39;: LOGS_DIR, &#39;verbose&#39;: True},
                            &#39;logger&#39;: {&#39;dir&#39;: LOGS_DIR}}
    # Add the base hyperparameters to the trainer hyperparameters
    self.hyperparameters = merge_dicts(self.hyperparameters, base_hyperparameters)
    # Add the modified hyperparameters given in the initialization
    self.hyperparameters = merge_dicts(self.hyperparameters, hyperparameters, verbose=True)
    # Set the device of the trainer
    self.device = device if device is not None else &#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39;

    # Get the modules for the training
    self.dataset, self.valid_dataset = self.get_datasets()
    self.dataloader, self.valid_dataloader = self.get_dataloaders()
    self.model = self.get_model()
    self.criterion = self.get_criterion()
    self.optimizer = self.get_optimizer()
    self.scheduler = self.get_scheduler()

    # Load the checkpoint
    self.checkpoint = self.resume(checkpoint)
    # Get the logger
    self.logger = self.get_logger()
    # As we only log one time per batch we need to keep the state of all the elements that we want to log
    self.current_log = {}</code></pre>
</details>
</dd>
<dt id="torchsight.trainers.trainer.Trainer.backward"><code class="name flex">
<span>def <span class="ident">backward</span></span>(<span>self, loss)</span>
</code></dt>
<dd>
<section class="desc"><p>Do the backward pass over the network.</p>
<p>There is a method for this because each experiment could do different things during the backward like:</p>
<pre><code class="python">loss.backward()
torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.1)
</code></pre>
<p>But in this case it only does the backward of the loss.</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>loss</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The loss value computed during the forward pass.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def backward(self, loss):
    &#34;&#34;&#34;Do the backward pass over the network.

    There is a method for this because each experiment could do different things during the backward like:
    ```python
    loss.backward()
    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.1)
    ```

    But in this case it only does the backward of the loss.

    Arguments:
        loss (torch.Tensor): The loss value computed during the forward pass.
    &#34;&#34;&#34;
    loss.backward()</code></pre>
</details>
</dd>
<dt id="torchsight.trainers.trainer.Trainer.batch_callback"><code class="name flex">
<span>def <span class="ident">batch_callback</span></span>(<span>self, batch, epoch)</span>
</code></dt>
<dd>
<section class="desc"><p>Method that is called after a batch has finished its process.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def batch_callback(self, batch, epoch):
    &#34;&#34;&#34;Method that is called after a batch has finished its process.&#34;&#34;&#34;</code></pre>
</details>
</dd>
<dt id="torchsight.trainers.trainer.Trainer.epoch_callback"><code class="name flex">
<span>def <span class="ident">epoch_callback</span></span>(<span>self, epoch)</span>
</code></dt>
<dd>
<section class="desc"><p>Method that is called after an epoch has finished its process.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def epoch_callback(self, epoch):
    &#34;&#34;&#34;Method that is called after an epoch has finished its process.&#34;&#34;&#34;</code></pre>
</details>
</dd>
<dt id="torchsight.trainers.trainer.Trainer.eval"><code class="name flex">
<span>def <span class="ident">eval</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Set the model into evaluation mode.</p>
<p>It's a method to override this and provide a custom eval() call if you want.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def eval(self):
    &#34;&#34;&#34;Set the model into evaluation mode.

    It&#39;s a method to override this and provide a custom eval() call if you want.
    &#34;&#34;&#34;
    self.model.eval()</code></pre>
</details>
</dd>
<dt id="torchsight.trainers.trainer.Trainer.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, *args)</span>
</code></dt>
<dd>
<section class="desc"><p>Do a forward pass over the model with the model and get the loss value.</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>*args</code></strong></dt>
<dd>All the data that the dataloader generates while iterating over it.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>torch.Tensor: The loss value of the forward pass.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def forward(self, *args):
    &#34;&#34;&#34;Do a forward pass over the model with the model and get the loss value.

    Arguments:
        *args: All the data that the dataloader generates while iterating over it.

    Returns:
        torch.Tensor: The loss value of the forward pass.
    &#34;&#34;&#34;
    raise NotImplementedError(&#39;You must implement the forward pass over the model.&#39;)</code></pre>
</details>
</dd>
<dt id="torchsight.trainers.trainer.Trainer.get_criterion"><code class="name flex">
<span>def <span class="ident">get_criterion</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Get the criterion to use to train the model.</p>
<h2 id="returns">Returns</h2>
<p>torch.nn.Module: The criterion to use in the training.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_criterion(self):
    &#34;&#34;&#34;Get the criterion to use to train the model.

    Returns:
        torch.nn.Module: The criterion to use in the training.
    &#34;&#34;&#34;
    return NotImplementedError(&#39;You must provide the criterion to train the model.&#39;)</code></pre>
</details>
</dd>
<dt id="torchsight.trainers.trainer.Trainer.get_dataloaders"><code class="name flex">
<span>def <span class="ident">get_dataloaders</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Get the dataloaders for training and validation.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>tuple</code></strong></dt>
<dd>The tuple with the torch.utils.data.DataLoader for training and validation.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_dataloaders(self):
    &#34;&#34;&#34;Get the dataloaders for training and validation.

    Returns:
        tuple: The tuple with the torch.utils.data.DataLoader for training and validation.
    &#34;&#34;&#34;
    raise NotImplementedError(&#39;You must provide the dataloaders for the datasets.&#39;)</code></pre>
</details>
</dd>
<dt id="torchsight.trainers.trainer.Trainer.get_datasets"><code class="name flex">
<span>def <span class="ident">get_datasets</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Get the training and validation datasets.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>tuple</code></strong></dt>
<dd>A Tuple with the torch.utils.data.Datasets for training and validation.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_datasets(self):
    &#34;&#34;&#34;Get the training and validation datasets.

    Returns:
        tuple: A Tuple with the torch.utils.data.Datasets for training and validation.
    &#34;&#34;&#34;
    raise NotImplementedError(&#39;You must provide your own datasets.&#39;)</code></pre>
</details>
</dd>
<dt id="torchsight.trainers.trainer.Trainer.get_logger"><code class="name flex">
<span>def <span class="ident">get_logger</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Get the (optional) logger to use during the training to show the information about the process.</p>
<p>This base implementation uses the PrintLogger that will print the log to the console.</p>
<h2 id="returns">Returns</h2>
<p>pymatch.loggers.Logger: A Logger to use during the training.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_logger(self):
    &#34;&#34;&#34;Get the (optional) logger to use during the training to show the information about the process.

    This base implementation uses the PrintLogger that will print the log to the console.

    Returns:
        pymatch.loggers.Logger: A Logger to use during the training.
    &#34;&#34;&#34;
    description = &#39;Hyperparameters:\n{}&#39;.format(json.dumps(self.hyperparameters, indent=2))
    return PrintLogger(description, self.hyperparameters[&#39;logger&#39;][&#39;dir&#39;])</code></pre>
</details>
</dd>
<dt id="torchsight.trainers.trainer.Trainer.get_model"><code class="name flex">
<span>def <span class="ident">get_model</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Get the model to train.</p>
<h2 id="returns">Returns</h2>
<p>torch.nn.Module: The model to train.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_model(self):
    &#34;&#34;&#34;Get the model to train.

    Returns:
        torch.nn.Module: The model to train.
    &#34;&#34;&#34;
    raise NotImplementedError(&#39;You must provide the model to train.&#39;)</code></pre>
</details>
</dd>
<dt id="torchsight.trainers.trainer.Trainer.get_optimizer"><code class="name flex">
<span>def <span class="ident">get_optimizer</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Get the optimizer of the model.</p>
<h2 id="returns">Returns</h2>
<p>torch.optim.Optimizer: The optimizer of the model's parameters.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_optimizer(self):
    &#34;&#34;&#34;Get the optimizer of the model.

    Returns:
        torch.optim.Optimizer: The optimizer of the model&#39;s parameters.
    &#34;&#34;&#34;
    raise NotImplementedError(&#39;You must provide the optimizer to use during the training.&#39;)</code></pre>
</details>
</dd>
<dt id="torchsight.trainers.trainer.Trainer.get_scheduler"><code class="name flex">
<span>def <span class="ident">get_scheduler</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Get the (optional) scheduler for the learning rate of the optimizer.</p>
<h2 id="returns">Returns</h2>
<p>torch.optim.lr_scheduler._LRScheduler: A scheduler for the learning rate.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_scheduler(self):
    &#34;&#34;&#34;Get the (optional) scheduler for the learning rate of the optimizer.

    Returns:
        torch.optim.lr_scheduler._LRScheduler: A scheduler for the learning rate.
    &#34;&#34;&#34;</code></pre>
</details>
</dd>
<dt id="torchsight.trainers.trainer.Trainer.resume"><code class="name flex">
<span>def <span class="ident">resume</span></span>(<span>self, checkpoint)</span>
</code></dt>
<dd>
<section class="desc"><p>Resume the training based on a last checkpoint and get the checkpoint dict.</p>
<p>This method does only return the epoch value in the dict to avoid memory leaks, we don't need
to keep the state_dicts in memory.
You can customize your own trainer and return more values.</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>checkpoint</code></strong> :&ensp;<code>str</code></dt>
<dd>The path to the checkpoint file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dict</code></strong></dt>
<dd>A dict with the epoch only. The state dict are not returned to not keep them
in memory.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def resume(self, checkpoint):
    &#34;&#34;&#34;Resume the training based on a last checkpoint and get the checkpoint dict.

    This method does only return the epoch value in the dict to avoid memory leaks, we don&#39;t need
    to keep the state_dicts in memory.
    You can customize your own trainer and return more values.

    Arguments:
        checkpoint (str): The path to the checkpoint file.

    Returns:
        dict: A dict with the epoch only. The state dict are not returned to not keep them
            in memory.
    &#34;&#34;&#34;
    if checkpoint is None:
        return None

    verbose = self.hyperparameters[&#39;checkpoint&#39;][&#39;verbose&#39;]

    if verbose:
        print(&#39;Loading checkpoint from {}&#39;.format(checkpoint))

    checkpoint = torch.load(checkpoint, map_location=self.device)

    self.model.load_state_dict(checkpoint[&#39;model&#39;])
    self.optimizer.load_state_dict(checkpoint[&#39;optimizer&#39;])
    for state in self.optimizer.state.values():
        for k, val in state.items():
            if torch.is_tensor(val):
                state[k] = val.to(self.device)

    if &#39;scheduler&#39; in checkpoint:
        self.scheduler.load_state_dict(checkpoint[&#39;scheduler&#39;])

    return {&#39;epoch&#39;: checkpoint[&#39;epoch&#39;]}</code></pre>
</details>
</dd>
<dt id="torchsight.trainers.trainer.Trainer.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self, epoch)</span>
</code></dt>
<dd>
<section class="desc"><p>Save the checkpoint of the trainer.</p>
<p>The checkpoint is a dict like:
{'epoch': int, 'model': state_dict, 'optimizer': state_dict, 'scheduler': state_dict}
where the scheduler is optional.</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>epoch</code></strong> :&ensp;<code>int</code></dt>
<dd>The epoch that has finished.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def save(self, epoch):
    &#34;&#34;&#34;Save the checkpoint of the trainer.

    The checkpoint is a dict like:
    {&#39;epoch&#39;: int, &#39;model&#39;: state_dict, &#39;optimizer&#39;: state_dict, &#39;scheduler&#39;: state_dict}
    where the scheduler is optional.

    Arguments:
        epoch (int): The epoch that has finished.
    &#34;&#34;&#34;
    params = self.hyperparameters[&#39;checkpoint&#39;]
    path = os.path.join(params[&#39;dir&#39;], &#39;checkpoint_epoch_{}.pth.tar&#39;.format(epoch))

    if params[&#39;verbose&#39;]:
        print(&#39;[Epoch {}] Saving checkpoint to: {}&#39;.format(epoch, path))

    checkpoint = {&#39;epoch&#39;: epoch,
                  &#39;model&#39;: self.model.state_dict(),
                  &#39;optimizer&#39;: self.optimizer.state_dict(),
                  &#39;hyperparameters&#39;: self.hyperparameters}

    if self.scheduler is not None:
        checkpoint[&#39;scheduler&#39;] = self.scheduler.state_dict()

    torch.save(checkpoint, path)</code></pre>
</details>
</dd>
<dt id="torchsight.trainers.trainer.Trainer.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self, epochs=100, validate=True)</span>
</code></dt>
<dd>
<section class="desc"><p>Train the model during the giving epochs.</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>epochs</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The number of epochs to run the model.</dd>
<dt><strong><code>validate</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If it's True the trainer will validate the training using
the validate() method. And if there is a scheduler it gives the validation loss
generated by the validate() method to the scheduler to adjust the learning rate.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def train(self, epochs=100, validate=True):
    &#34;&#34;&#34;Train the model during the giving epochs.

    Arguments:
        epochs (int, optional): The number of epochs to run the model.
        validate (bool, optional): If it&#39;s True the trainer will validate the training using
            the validate() method. And if there is a scheduler it gives the validation loss
            generated by the validate() method to the scheduler to adjust the learning rate.
    &#34;&#34;&#34;
    self.model.to(self.device)

    # The criterion could be inside the model for example and in that case it could be None
    if self.criterion is not None:
        self.criterion.to(self.device)

    # The number of batches that the training dataset have
    n_batches = len(self.dataloader)

    # The start time of the training and the last batch&#39;s end time
    start_time = time.time()
    last_endtime = start_time

    # We start from the next epoch of the checkpoint (if there is any)
    start_epoch = 1 if self.checkpoint is None else self.checkpoint[&#39;epoch&#39;] + 1

    for epoch in range(start_epoch, start_epoch + epochs):
        # Indicate to the model that we are in training mode, useful for batch normalization or dropouts modules.
        # For more info see:
        # https://discuss.pytorch.org/t/trying-to-understand-the-meaning-of-model-train-and-model-eval/20158
        self.model.train()

        for batch, data in enumerate(self.dataloader):
            # Optimize
            self.optimizer.zero_grad()
            loss = self.forward(*data)
            self.backward(loss)
            self.optimizer.step()

            # Log the batch
            learning_rates = [str(param_group[&#39;lr&#39;])
                              for i, param_group in enumerate(self.optimizer.param_groups)]

            total_time = time.time() - start_time
            batch_time = time.time() - last_endtime
            last_endtime = time.time()

            self.logger.log(merge_dicts({
                &#39;Training&#39;: None,
                &#39;Epoch&#39;: epoch,
                &#39;Batch&#39;: &#39;{}/{}&#39;.format(batch + 1, n_batches),
                &#39;LR&#39;: &#39; &#39;.join(learning_rates),
                &#39;Loss&#39;: &#39;{:.7f}&#39;.format(float(loss)),
                &#39;Time&#39;: &#39;{:.3f} s&#39;.format(batch_time),
                &#39;Total&#39;: &#39;{:.1f} s&#39;.format(total_time)
            }, self.current_log))
            self.current_log = {}  # Restart the log dict for the next batch

            # Call the callback for the batch
            self.batch_callback(batch, epoch)

        # Call the callback for the epoch
        self.epoch_callback(epoch)

        # Save the checkpoint for this epoch
        self.save(epoch)

        if validate:
            loss = self.validate(epoch)
            if self.scheduler is not None:
                self.scheduler.step(loss)</code></pre>
</details>
</dd>
<dt id="torchsight.trainers.trainer.Trainer.validate"><code class="name flex">
<span>def <span class="ident">validate</span></span>(<span>self, epoch)</span>
</code></dt>
<dd>
<section class="desc"><p>Run the model over the validation dataset and return the mean loss over it.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def validate(self, epoch):
    &#34;&#34;&#34;Run the model over the validation dataset and return the mean loss over it.&#34;&#34;&#34;
    self.model.to(self.device)
    self.eval()

    start_time = time.time()
    last_endtime = start_time

    n_batches = len(self.valid_dataloader)

    losses = []

    with torch.no_grad():
        for batch, data in enumerate(self.valid_dataloader):
            loss = float(self.forward(*data))

            batch_time = time.time() - last_endtime
            last_endtime = time.time()
            total_time = time.time() - start_time

            self.logger.log(merge_dicts({
                &#39;Validating&#39;: None,
                &#39;Epoch&#39;: epoch,
                &#39;Batch&#39;: &#39;{}/{}&#39;.format(batch + 1, n_batches),
                &#39;Loss&#39;: &#39;{:.7f}&#39;.format(float(loss)),
                &#39;Time&#39;: &#39;{:.3f} s&#39;.format(batch_time),
                &#39;Total&#39;: &#39;{:.1f} s&#39;.format(total_time)
            }, self.current_log))
            self.current_log = {}  # Restart the log dict for the next batch

            losses.append(loss)

    return torch.Tensor(losses).mean()</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#quick-start">Quick start</a></li>
</ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="torchsight.trainers" href="index.html">torchsight.trainers</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="torchsight.trainers.trainer.Trainer" href="#torchsight.trainers.trainer.Trainer">Trainer</a></code></h4>
<ul class="two-column">
<li><code><a title="torchsight.trainers.trainer.Trainer.__init__" href="#torchsight.trainers.trainer.Trainer.__init__">__init__</a></code></li>
<li><code><a title="torchsight.trainers.trainer.Trainer.backward" href="#torchsight.trainers.trainer.Trainer.backward">backward</a></code></li>
<li><code><a title="torchsight.trainers.trainer.Trainer.batch_callback" href="#torchsight.trainers.trainer.Trainer.batch_callback">batch_callback</a></code></li>
<li><code><a title="torchsight.trainers.trainer.Trainer.epoch_callback" href="#torchsight.trainers.trainer.Trainer.epoch_callback">epoch_callback</a></code></li>
<li><code><a title="torchsight.trainers.trainer.Trainer.eval" href="#torchsight.trainers.trainer.Trainer.eval">eval</a></code></li>
<li><code><a title="torchsight.trainers.trainer.Trainer.forward" href="#torchsight.trainers.trainer.Trainer.forward">forward</a></code></li>
<li><code><a title="torchsight.trainers.trainer.Trainer.from_checkpoint" href="#torchsight.trainers.trainer.Trainer.from_checkpoint">from_checkpoint</a></code></li>
<li><code><a title="torchsight.trainers.trainer.Trainer.get_criterion" href="#torchsight.trainers.trainer.Trainer.get_criterion">get_criterion</a></code></li>
<li><code><a title="torchsight.trainers.trainer.Trainer.get_dataloaders" href="#torchsight.trainers.trainer.Trainer.get_dataloaders">get_dataloaders</a></code></li>
<li><code><a title="torchsight.trainers.trainer.Trainer.get_datasets" href="#torchsight.trainers.trainer.Trainer.get_datasets">get_datasets</a></code></li>
<li><code><a title="torchsight.trainers.trainer.Trainer.get_logger" href="#torchsight.trainers.trainer.Trainer.get_logger">get_logger</a></code></li>
<li><code><a title="torchsight.trainers.trainer.Trainer.get_model" href="#torchsight.trainers.trainer.Trainer.get_model">get_model</a></code></li>
<li><code><a title="torchsight.trainers.trainer.Trainer.get_optimizer" href="#torchsight.trainers.trainer.Trainer.get_optimizer">get_optimizer</a></code></li>
<li><code><a title="torchsight.trainers.trainer.Trainer.get_scheduler" href="#torchsight.trainers.trainer.Trainer.get_scheduler">get_scheduler</a></code></li>
<li><code><a title="torchsight.trainers.trainer.Trainer.hyperparameters" href="#torchsight.trainers.trainer.Trainer.hyperparameters">hyperparameters</a></code></li>
<li><code><a title="torchsight.trainers.trainer.Trainer.resume" href="#torchsight.trainers.trainer.Trainer.resume">resume</a></code></li>
<li><code><a title="torchsight.trainers.trainer.Trainer.save" href="#torchsight.trainers.trainer.Trainer.save">save</a></code></li>
<li><code><a title="torchsight.trainers.trainer.Trainer.train" href="#torchsight.trainers.trainer.Trainer.train">train</a></code></li>
<li><code><a title="torchsight.trainers.trainer.Trainer.validate" href="#torchsight.trainers.trainer.Trainer.validate">validate</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.5.4</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>