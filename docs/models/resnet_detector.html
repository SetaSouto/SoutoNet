<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.5.4" />
<title>torchsight.models.resnet_detector API documentation</title>
<meta name="description" content="Module with a dummy object detector using a ResNet." />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.name small{font-weight:normal}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase;cursor:pointer}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title"><code>torchsight.models.resnet_detector</code> module</h1>
</header>
<section id="section-intro">
<p>Module with a dummy object detector using a ResNet.</p>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">&#34;&#34;&#34;Module with a dummy object detector using a ResNet.&#34;&#34;&#34;
import torch

from .resnet import resnet18, resnet34, resnet50, resnet101, resnet152


class ResnetDetector(torch.nn.Module):
    &#34;&#34;&#34;A dummy detector based on the features extracted from a pretrained ResNet.

    The model generates a feature map based on an image.
    Let&#39;s call &#39;embedding&#39; all the features for a location in the feature map.
    Using a pooling strategy we can reduce the embedding size and get an embedding
    over a kernel size in the feature map.

    So, for example, if we have an image of 512x512, the ResNet has an stride of 32,
    we get a feature map of 16x16 locations. Using a ResNet50, there are 2048 features
    generated per location. We can reduce them to 256 by pooling them.
    Now, we can apply kernels of size 2x2, 4x4 and 8x8 to get other embeddings for
    &#34;bigger&#34; objects.
    &#34;&#34;&#34;

    def __init__(self, resnet=18, dim=256, pool=&#39;avg&#39;, kernels=None):
        &#34;&#34;&#34;Initialize the model.

        Arguments:
            resnet (int, optional): The ResNet to use as feature extractor.
            dim (int, optional): The dimension of the embeddings to generate.
            pool (str, optional): The pool strategy to use. Options: &#39;avg&#39; or &#39;max&#39;.
            kernels (list of int, optional): The size of the kernels to use.
        &#34;&#34;&#34;
        super().__init__()

        if resnet == 18:
            self.resnet = resnet18(pretrained=True)
        elif resnet == 34:
            self.resnet = resnet34(pretrained=True)
        elif resnet == 50:
            self.resnet = resnet50(pretrained=True)
        elif resnet == 101:
            self.resnet = resnet101(pretrained=True)
        elif resnet == 152:
            self.resnet = resnet152(pretrained=True)
        else:
            raise ValueError(&#39;There is no resnet &#34;{}&#34;&#39;.format(resnet))

        if pool not in [&#39;avg&#39;, &#39;max&#39;]:
            raise ValueError(&#39;There is no &#34;{}&#34; pool. Availables: {}&#39;.format(pool, [&#39;avg&#39;, &#39;max&#39;]))

        self.dim = dim
        self.pool = pool
        self.kernels = kernels if kernels is not None else [2, 4, 8]

        if pool == &#39;avg&#39;:
            self.pools = [torch.nn.AvgPool2d(k) for k in kernels]
        if pool == &#39;max&#39;:
            self.pools = [torch.nn.MaxPool2d(k) for k in kernels]

    def forward(self, images):
        &#34;&#34;&#34;Get the embeddings and bounding boxes foor the given images.

        Arguments:
            images (torch.Tensor): with the batch of images. Shape `(batch size, 3, height, width)`.

        Returns:
            torch.Tensor: The embeddings generated for the images.
                Shape `(batch size, num of embeddings, dim)`.
            torch.Tensor: The bounding boxes for each one of the embeddings.
                Shape `(batch size, num of embeddings, 4)`
        &#34;&#34;&#34;
        batch_size, _, height, width = images.shape

        if height % 32 != 0:
            raise ValueError(&#39;This model only works for images with height multiple of 32.&#39;)
        if width % 32 != 0:
            raise ValueError(&#39;This model only works for images with width multiple of 32.&#39;)

        # Get the height and width of the feature map
        height, width = height / 32, width / 32

        # Generate feature map using the resnet
        features = self.resnet(images)[0]  # (b, f, h, w)

        # Reduce the length of the features by pooling them
        if self.dim != features.shape[1]:
            features = features.view(batch_size, self.dim, -1, height, width)  # (b, d, f-d, h, w)
            if self.pool == &#39;avg&#39;:
                features = features.mean(dim=2)  # (b, d, h, w)
            else:
                features = features.max(dim=2)  # (b, d, h, w)

        # Apply the pooling with kernels and get embeddings
        pooled = []
        for i, pool in enumerate(self.pools):
            kernel = self.kernels[i]
            embeddings = pool(features)  # (b, d, h/k, w/k)
            boxes = self.get_boxes(embeddings, stride=32*kernel, batch_size=batch_size)  # (b, h/k*w/k, 4)
            embeddings = embeddings.view(batch_size, self.dim, -1)  # (b, d, *)
            pooled.append([embeddings, boxes])

        # Transform the feature map to embeddings with shape (batch size, dim, *)
        boxes = self.get_boxes(features, stride=32, batch_size=batch_size)
        embeddings = features.view(batch_size, self.dim, -1)  # (b, d, *)

        # Concatenate all the embeddings
        embeddings = torch.cat([embeddings, *[p[0] for p in pooled]], dim=2)
        boxes = torch.cat([boxes, *[p[1] for p in pooled]], dim=1)

        # Transpose the dimensions to get the embeddings with shape (batch size, num of embeddings, dim)
        embeddings = embeddings.permute(0, 2, 1)

        return embeddings, boxes

    def get_boxes(self, feature_map, stride, batch_size):
        &#34;&#34;&#34;Get boxes for the given feature map that was got from applying the given stride to the image.

        Arguments:
            feature_map (torch.Tensor): with shape `(batch size, features, height, width)`.
            stride (int): the stride applied to the image to get this feature map.

        Returns:
            torch.Tensor: with the boxes as x1, y1, x2, y2 for top-left corner and bottom-right corner.
                Shape: `(batch size, h * w, 4)` where `h` and `w` are the height and width of the feature map.
        &#34;&#34;&#34;
        height, width = feature_map.shape[2:]
        boxes = feature_map.new_zeros(height, width, 4)

        for i in range(int(height)):
            for j in range(int(width)):
                boxes[i, j, 0] = stride * i
                boxes[i, j, 1] = stride * j
                boxes[i, j, 2] = stride * (i+1)
                boxes[i, j, 3] = stride * (j+1)

        return boxes.view(-1, 4).unsqueeze(dim=0).repeat((batch_size, 1, 1))</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="torchsight.models.resnet_detector.ResnetDetector"><code class="flex name class">
<span>class <span class="ident">ResnetDetector</span></span>
<span>(</span><span><small>ancestors:</small> torch.nn.modules.module.Module)</span>
</code></dt>
<dd>
<section class="desc"><p>A dummy detector based on the features extracted from a pretrained ResNet.</p>
<p>The model generates a feature map based on an image.
Let's call 'embedding' all the features for a location in the feature map.
Using a pooling strategy we can reduce the embedding size and get an embedding
over a kernel size in the feature map.</p>
<p>So, for example, if we have an image of 512x512, the ResNet has an stride of 32,
we get a feature map of 16x16 locations. Using a ResNet50, there are 2048 features
generated per location. We can reduce them to 256 by pooling them.
Now, we can apply kernels of size 2x2, 4x4 and 8x8 to get other embeddings for
"bigger" objects.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class ResnetDetector(torch.nn.Module):
    &#34;&#34;&#34;A dummy detector based on the features extracted from a pretrained ResNet.

    The model generates a feature map based on an image.
    Let&#39;s call &#39;embedding&#39; all the features for a location in the feature map.
    Using a pooling strategy we can reduce the embedding size and get an embedding
    over a kernel size in the feature map.

    So, for example, if we have an image of 512x512, the ResNet has an stride of 32,
    we get a feature map of 16x16 locations. Using a ResNet50, there are 2048 features
    generated per location. We can reduce them to 256 by pooling them.
    Now, we can apply kernels of size 2x2, 4x4 and 8x8 to get other embeddings for
    &#34;bigger&#34; objects.
    &#34;&#34;&#34;

    def __init__(self, resnet=18, dim=256, pool=&#39;avg&#39;, kernels=None):
        &#34;&#34;&#34;Initialize the model.

        Arguments:
            resnet (int, optional): The ResNet to use as feature extractor.
            dim (int, optional): The dimension of the embeddings to generate.
            pool (str, optional): The pool strategy to use. Options: &#39;avg&#39; or &#39;max&#39;.
            kernels (list of int, optional): The size of the kernels to use.
        &#34;&#34;&#34;
        super().__init__()

        if resnet == 18:
            self.resnet = resnet18(pretrained=True)
        elif resnet == 34:
            self.resnet = resnet34(pretrained=True)
        elif resnet == 50:
            self.resnet = resnet50(pretrained=True)
        elif resnet == 101:
            self.resnet = resnet101(pretrained=True)
        elif resnet == 152:
            self.resnet = resnet152(pretrained=True)
        else:
            raise ValueError(&#39;There is no resnet &#34;{}&#34;&#39;.format(resnet))

        if pool not in [&#39;avg&#39;, &#39;max&#39;]:
            raise ValueError(&#39;There is no &#34;{}&#34; pool. Availables: {}&#39;.format(pool, [&#39;avg&#39;, &#39;max&#39;]))

        self.dim = dim
        self.pool = pool
        self.kernels = kernels if kernels is not None else [2, 4, 8]

        if pool == &#39;avg&#39;:
            self.pools = [torch.nn.AvgPool2d(k) for k in kernels]
        if pool == &#39;max&#39;:
            self.pools = [torch.nn.MaxPool2d(k) for k in kernels]

    def forward(self, images):
        &#34;&#34;&#34;Get the embeddings and bounding boxes foor the given images.

        Arguments:
            images (torch.Tensor): with the batch of images. Shape `(batch size, 3, height, width)`.

        Returns:
            torch.Tensor: The embeddings generated for the images.
                Shape `(batch size, num of embeddings, dim)`.
            torch.Tensor: The bounding boxes for each one of the embeddings.
                Shape `(batch size, num of embeddings, 4)`
        &#34;&#34;&#34;
        batch_size, _, height, width = images.shape

        if height % 32 != 0:
            raise ValueError(&#39;This model only works for images with height multiple of 32.&#39;)
        if width % 32 != 0:
            raise ValueError(&#39;This model only works for images with width multiple of 32.&#39;)

        # Get the height and width of the feature map
        height, width = height / 32, width / 32

        # Generate feature map using the resnet
        features = self.resnet(images)[0]  # (b, f, h, w)

        # Reduce the length of the features by pooling them
        if self.dim != features.shape[1]:
            features = features.view(batch_size, self.dim, -1, height, width)  # (b, d, f-d, h, w)
            if self.pool == &#39;avg&#39;:
                features = features.mean(dim=2)  # (b, d, h, w)
            else:
                features = features.max(dim=2)  # (b, d, h, w)

        # Apply the pooling with kernels and get embeddings
        pooled = []
        for i, pool in enumerate(self.pools):
            kernel = self.kernels[i]
            embeddings = pool(features)  # (b, d, h/k, w/k)
            boxes = self.get_boxes(embeddings, stride=32*kernel, batch_size=batch_size)  # (b, h/k*w/k, 4)
            embeddings = embeddings.view(batch_size, self.dim, -1)  # (b, d, *)
            pooled.append([embeddings, boxes])

        # Transform the feature map to embeddings with shape (batch size, dim, *)
        boxes = self.get_boxes(features, stride=32, batch_size=batch_size)
        embeddings = features.view(batch_size, self.dim, -1)  # (b, d, *)

        # Concatenate all the embeddings
        embeddings = torch.cat([embeddings, *[p[0] for p in pooled]], dim=2)
        boxes = torch.cat([boxes, *[p[1] for p in pooled]], dim=1)

        # Transpose the dimensions to get the embeddings with shape (batch size, num of embeddings, dim)
        embeddings = embeddings.permute(0, 2, 1)

        return embeddings, boxes

    def get_boxes(self, feature_map, stride, batch_size):
        &#34;&#34;&#34;Get boxes for the given feature map that was got from applying the given stride to the image.

        Arguments:
            feature_map (torch.Tensor): with shape `(batch size, features, height, width)`.
            stride (int): the stride applied to the image to get this feature map.

        Returns:
            torch.Tensor: with the boxes as x1, y1, x2, y2 for top-left corner and bottom-right corner.
                Shape: `(batch size, h * w, 4)` where `h` and `w` are the height and width of the feature map.
        &#34;&#34;&#34;
        height, width = feature_map.shape[2:]
        boxes = feature_map.new_zeros(height, width, 4)

        for i in range(int(height)):
            for j in range(int(width)):
                boxes[i, j, 0] = stride * i
                boxes[i, j, 1] = stride * j
                boxes[i, j, 2] = stride * (i+1)
                boxes[i, j, 3] = stride * (j+1)

        return boxes.view(-1, 4).unsqueeze(dim=0).repeat((batch_size, 1, 1))</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="torchsight.models.resnet_detector.ResnetDetector.__init__"><code class="name flex">
<span>def <span class="ident">__init__</span></span>(<span>self, resnet=18, dim=256, pool=&#39;avg&#39;, kernels=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Initialize the model.</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>resnet</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The ResNet to use as feature extractor.</dd>
<dt><strong><code>dim</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The dimension of the embeddings to generate.</dd>
<dt><strong><code>pool</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The pool strategy to use. Options: 'avg' or 'max'.</dd>
<dt><strong><code>kernels</code></strong> :&ensp;<code>list</code> of <code>int</code>, optional</dt>
<dd>The size of the kernels to use.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def __init__(self, resnet=18, dim=256, pool=&#39;avg&#39;, kernels=None):
    &#34;&#34;&#34;Initialize the model.

    Arguments:
        resnet (int, optional): The ResNet to use as feature extractor.
        dim (int, optional): The dimension of the embeddings to generate.
        pool (str, optional): The pool strategy to use. Options: &#39;avg&#39; or &#39;max&#39;.
        kernels (list of int, optional): The size of the kernels to use.
    &#34;&#34;&#34;
    super().__init__()

    if resnet == 18:
        self.resnet = resnet18(pretrained=True)
    elif resnet == 34:
        self.resnet = resnet34(pretrained=True)
    elif resnet == 50:
        self.resnet = resnet50(pretrained=True)
    elif resnet == 101:
        self.resnet = resnet101(pretrained=True)
    elif resnet == 152:
        self.resnet = resnet152(pretrained=True)
    else:
        raise ValueError(&#39;There is no resnet &#34;{}&#34;&#39;.format(resnet))

    if pool not in [&#39;avg&#39;, &#39;max&#39;]:
        raise ValueError(&#39;There is no &#34;{}&#34; pool. Availables: {}&#39;.format(pool, [&#39;avg&#39;, &#39;max&#39;]))

    self.dim = dim
    self.pool = pool
    self.kernels = kernels if kernels is not None else [2, 4, 8]

    if pool == &#39;avg&#39;:
        self.pools = [torch.nn.AvgPool2d(k) for k in kernels]
    if pool == &#39;max&#39;:
        self.pools = [torch.nn.MaxPool2d(k) for k in kernels]</code></pre>
</details>
</dd>
<dt id="torchsight.models.resnet_detector.ResnetDetector.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, images)</span>
</code></dt>
<dd>
<section class="desc"><p>Get the embeddings and bounding boxes foor the given images.</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>images</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>with the batch of images. Shape <code>(batch size, 3, height, width)</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>torch.Tensor: The embeddings generated for the images.
Shape <code>(batch size, num of embeddings, dim)</code>.
torch.Tensor: The bounding boxes for each one of the embeddings.
Shape <code>(batch size, num of embeddings, 4)</code></p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def forward(self, images):
    &#34;&#34;&#34;Get the embeddings and bounding boxes foor the given images.

    Arguments:
        images (torch.Tensor): with the batch of images. Shape `(batch size, 3, height, width)`.

    Returns:
        torch.Tensor: The embeddings generated for the images.
            Shape `(batch size, num of embeddings, dim)`.
        torch.Tensor: The bounding boxes for each one of the embeddings.
            Shape `(batch size, num of embeddings, 4)`
    &#34;&#34;&#34;
    batch_size, _, height, width = images.shape

    if height % 32 != 0:
        raise ValueError(&#39;This model only works for images with height multiple of 32.&#39;)
    if width % 32 != 0:
        raise ValueError(&#39;This model only works for images with width multiple of 32.&#39;)

    # Get the height and width of the feature map
    height, width = height / 32, width / 32

    # Generate feature map using the resnet
    features = self.resnet(images)[0]  # (b, f, h, w)

    # Reduce the length of the features by pooling them
    if self.dim != features.shape[1]:
        features = features.view(batch_size, self.dim, -1, height, width)  # (b, d, f-d, h, w)
        if self.pool == &#39;avg&#39;:
            features = features.mean(dim=2)  # (b, d, h, w)
        else:
            features = features.max(dim=2)  # (b, d, h, w)

    # Apply the pooling with kernels and get embeddings
    pooled = []
    for i, pool in enumerate(self.pools):
        kernel = self.kernels[i]
        embeddings = pool(features)  # (b, d, h/k, w/k)
        boxes = self.get_boxes(embeddings, stride=32*kernel, batch_size=batch_size)  # (b, h/k*w/k, 4)
        embeddings = embeddings.view(batch_size, self.dim, -1)  # (b, d, *)
        pooled.append([embeddings, boxes])

    # Transform the feature map to embeddings with shape (batch size, dim, *)
    boxes = self.get_boxes(features, stride=32, batch_size=batch_size)
    embeddings = features.view(batch_size, self.dim, -1)  # (b, d, *)

    # Concatenate all the embeddings
    embeddings = torch.cat([embeddings, *[p[0] for p in pooled]], dim=2)
    boxes = torch.cat([boxes, *[p[1] for p in pooled]], dim=1)

    # Transpose the dimensions to get the embeddings with shape (batch size, num of embeddings, dim)
    embeddings = embeddings.permute(0, 2, 1)

    return embeddings, boxes</code></pre>
</details>
</dd>
<dt id="torchsight.models.resnet_detector.ResnetDetector.get_boxes"><code class="name flex">
<span>def <span class="ident">get_boxes</span></span>(<span>self, feature_map, stride, batch_size)</span>
</code></dt>
<dd>
<section class="desc"><p>Get boxes for the given feature map that was got from applying the given stride to the image.</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>feature_map</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>with shape <code>(batch size, features, height, width)</code>.</dd>
<dt><strong><code>stride</code></strong> :&ensp;<code>int</code></dt>
<dd>the stride applied to the image to get this feature map.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>torch.Tensor: with the boxes as x1, y1, x2, y2 for top-left corner and bottom-right corner.
Shape: <code>(batch size, h * w, 4)</code> where <code>h</code> and <code>w</code> are the height and width of the feature map.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_boxes(self, feature_map, stride, batch_size):
    &#34;&#34;&#34;Get boxes for the given feature map that was got from applying the given stride to the image.

    Arguments:
        feature_map (torch.Tensor): with shape `(batch size, features, height, width)`.
        stride (int): the stride applied to the image to get this feature map.

    Returns:
        torch.Tensor: with the boxes as x1, y1, x2, y2 for top-left corner and bottom-right corner.
            Shape: `(batch size, h * w, 4)` where `h` and `w` are the height and width of the feature map.
    &#34;&#34;&#34;
    height, width = feature_map.shape[2:]
    boxes = feature_map.new_zeros(height, width, 4)

    for i in range(int(height)):
        for j in range(int(width)):
            boxes[i, j, 0] = stride * i
            boxes[i, j, 1] = stride * j
            boxes[i, j, 2] = stride * (i+1)
            boxes[i, j, 3] = stride * (j+1)

    return boxes.view(-1, 4).unsqueeze(dim=0).repeat((batch_size, 1, 1))</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="torchsight.models" href="index.html">torchsight.models</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="torchsight.models.resnet_detector.ResnetDetector" href="#torchsight.models.resnet_detector.ResnetDetector">ResnetDetector</a></code></h4>
<ul class="">
<li><code><a title="torchsight.models.resnet_detector.ResnetDetector.__init__" href="#torchsight.models.resnet_detector.ResnetDetector.__init__">__init__</a></code></li>
<li><code><a title="torchsight.models.resnet_detector.ResnetDetector.forward" href="#torchsight.models.resnet_detector.ResnetDetector.forward">forward</a></code></li>
<li><code><a title="torchsight.models.resnet_detector.ResnetDetector.get_boxes" href="#torchsight.models.resnet_detector.ResnetDetector.get_boxes">get_boxes</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.5.4</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>