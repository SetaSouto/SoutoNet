<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.5.4" />
<title>torchsight.datasets.coco API documentation</title>
<meta name="description" content="Coco dataset" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.name small{font-weight:normal}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase;cursor:pointer}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title"><code>torchsight.datasets.coco</code> module</h1>
</header>
<section id="section-intro">
<p>Coco dataset</p>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">&#34;&#34;&#34;Coco dataset&#34;&#34;&#34;
import os
import time

import matplotlib
import numpy as np
import skimage
import skimage.io
import torch
from pycocotools.coco import COCO
from torch.utils.data import Dataset


class CocoDataset(Dataset):
    &#34;&#34;&#34;Coco dataset class.

    Heavily inspired by the code at:
    https://github.com/SetaSouto/pytorch-retinanet/blob/master/dataloader.py
    &#34;&#34;&#34;

    def __init__(self, root, dataset, classes_names=(), transform=None, stats=True):
        &#34;&#34;&#34;Initialize the dataset.

        Initialize the coco api under the &#39;coco&#39; attribute.

        Also, loads the classes and labels.

        It sets the &#39;classes&#39; attribute of the class that contains a dict with four dicts:
        - &#39;ids&#39;: Keep track of the classes&#39; ids given its label (label: int -&gt; id: int).
        - &#39;labels&#39;: Keep track of the label given the id of the class (id: int -&gt; label: int).
        - &#39;names&#39;: Keep track of the name of the class given its label (label: int -&gt; name: string).
        - &#39;length&#39;: Keep track of how many images are for the given label (label: int -&gt; length: int).

        Why labels and ids? Because ids are given by the coco api, are static, but if we filter the classes
        we want to reorder the labels from 0 to N if we load N classes.
        For example, the 2017 dataset contains classes with ids from 1 to 90, but we like to keep labels
        starting from 0 (zero indexed). If we want only to load the classes &#39;person&#39; (id: 1), &#39;bus&#39; (id: 6)
        and &#39;stop sign&#39; (id: 13) we don&#39;t want to work with labels 1, 6 and 13, we want to work with labels
        0, 1, 2, because we loaded only 3 classes.

        You can filter the images and classes to be loaded by using classes_names argument.

        Why we load the bounding boxes already? Because the COCO api already does that, so instead of keeping
        the coco api instance we format the bounding boxes in the initialization and keep track of the path
        of each image. The images are loaded in running time.
        This way we use the api in the initialization only and free some memory.

        Args:
            root (str): COCO root directory.
                Inside this directory we must find the &#39;annotations&#39; and &#39;images&#39; folder for example.
            dataset (str): The name of the set to be loaded. Is the name of the directory that contains
                the images and is in the name of the file that contains the annotations.
                Example: &#39;train2017&#39; will trigger the loading of the images at coco/images/train2017
                and the annotations from the file &#39;instances_train2017.json&#39;.
            classes_names (tuple): Tuple of strings with the name of the classes to load. Only load images with those
                classes&#39; names.
            transform (torchvision.transforms.Compose, optional): A list with transforms to apply to each image.
            stats (Boolean): Print the stats of the classes. Indicates how many images are per category.
                Keep in mind that the sum of all the images per category may not be the same to the total number
                of images, this is because some images could contain more than one object type (very common).
        &#34;&#34;&#34;
        self.transform = transform

        # Initialize the COCO api
        print(&#39;--- COCO API ---&#39;)
        coco = COCO(os.path.join(root, &#39;annotations&#39;, &#39;instances_{}.json&#39;.format(dataset)))
        print(&#39;----------------&#39;)

        # Load classes and set classes dict
        print(&#39;Loading classes and setting labels, names and lengths ...&#39;)
        self.classes = {&#39;ids&#39;: {}, &#39;names&#39;: {}, &#39;labels&#39;: {}, &#39;length&#39;: {}}
        for label, category in enumerate(coco.loadCats(coco.getCatIds(catNms=classes_names))):
            self.classes[&#39;ids&#39;][label] = category[&#39;id&#39;]
            self.classes[&#39;labels&#39;][category[&#39;id&#39;]] = label
            self.classes[&#39;names&#39;][label] = category[&#39;name&#39;]

        # Get filtered images ids
        print(&#39;Loading images info ...&#39;)
        images_ids = set()
        for category_id in self.classes[&#39;ids&#39;].values():
            category_images = set(coco.catToImgs[category_id])
            category_label = self.classes[&#39;labels&#39;][category_id]
            self.classes[&#39;length&#39;][category_label] = len(category_images)
            images_ids |= category_images

        # Init images array that contains tuples with the path to the images and the annotations
        print(&#39;Setting bounding boxes ...&#39;)
        self.images = []
        for image_info in coco.loadImgs(images_ids):
            bounding_boxes = np.zeros((0, 5))

            for annotation in coco.imgToAnns[image_info[&#39;id&#39;]]:
                try:
                    label = self.classes[&#39;labels&#39;][annotation[&#39;category_id&#39;]]
                    bounding_boxes = np.append(bounding_boxes, np.array([[*annotation[&#39;bbox&#39;], label]]), axis=0)
                except KeyError:
                    # The image has a bounding box from a class that does not exists in classes_names sequence
                    continue

            self.images.append((
                os.path.join(root, &#39;images&#39;, dataset, image_info[&#39;file_name&#39;]),  # Image&#39;s path
                bounding_boxes  # Bounding boxes with shape (N, 5)
            ))

        # Print stats
        if stats:
            print(&#39;Images per class:&#39;)
            stats = [(label,
                      self.classes[&#39;names&#39;][label],
                      self.classes[&#39;length&#39;][label])
                     for label in self.classes[&#39;labels&#39;].values()]
            stats = sorted(stats, key=lambda x: x[2], reverse=True)
            for label, name, length in stats:
                print(&#39;{}: {}{}&#39;.format(str(label).ljust(2), name.ljust(15), length))

    def __len__(self):
        &#34;&#34;&#34;Get the length of the dataset.

        Returns:
            int: The length of the dataset.
        &#34;&#34;&#34;
        return len(self.images)

    def __getitem__(self, index):
        &#34;&#34;&#34;Get an item from the dataset given its index.

        Load the image and bounding boxes for that image and return a tuple with
        image, bounding boxes. Both are ndarrays.

        The bounding boxes is a numpy array with shape (N, 5) where N is the number
        of bounding boxes in the image. Why 5? Because they are x1, y1 for the top
        left corner of the bounding box, x2, y2 for the bottom right corner of the
        bounding box and the last one is the label of the class.

        Args:
            index (int): Index of the image in the dataset.

        Returns:
            ndarray: The transformed image if there is any transformation or the original image.
            ndarray: The bounding boxes of the image.
        &#34;&#34;&#34;
        path, bounding_boxes = self.images[index]

        image = skimage.io.imread(path)
        if len(image.shape) == 2:
            image = skimage.color.gray2rgb(image)
        image = image.astype(np.float32) / 255.0

        # Transform from [x,y,w,h] to [x1,y1,x2,y2]
        bounding_boxes[:, 2] = bounding_boxes[:, 0] + bounding_boxes[:, 2]
        bounding_boxes[:, 3] = bounding_boxes[:, 1] + bounding_boxes[:, 3]

        if self.transform:
            image, bounding_boxes = self.transform((image, bounding_boxes))

        return image, bounding_boxes

    def visualize(self, image, boxes=None, initial_time=None, n_colors=20):
        &#34;&#34;&#34;Visualize an image and its bounding boxes.

        Arguments:
            image (torch.Tensor or ndarray): The image to visualize.
            boxes (torch.Tensor or ndarray): The bounding boxes to visualize in the image.
                It must contains the x1, y1, x2, y2 points and the label in the 4th index.
                You could provide the probability of the label too optionally in the 5th index.
                Shape:
                    (number of annotations, 5 or 6)
        &#34;&#34;&#34;
        initial_time = initial_time if initial_time is not None else time.time()

        if torch.is_tensor(image):
            image = image.numpy().transpose(1, 2, 0)

        # Matplotlib colormaps, for more information please visit:
        # https://matplotlib.org/examples/color/colormaps_reference.html
        # Is a continuous map of colors, you can get a color by calling it on a number between
        # 0 and 1
        colormap = matplotlib.pyplot.get_cmap(&#39;tab20&#39;)
        # Select n_colors colors from 0 to 1
        colors = [colormap(i) for i in np.linspace(0, 1, n_colors)]

        # Generate figure and axes
        _, axes = matplotlib.pyplot.subplots(1)

        # Generate rectangles
        if boxes is not None:
            for i in range(boxes.shape[0]):
                # We need the top left corner of the rectangle and its width and height
                if boxes[i].shape[0] == 6:
                    x, y, x2, y2, label, prob = boxes[i]
                    prob = &#39; {:.2f}&#39;.format(prob)
                else:
                    x, y, x2, y2, label = boxes[i]
                    prob = &#39;&#39;
                w, h = x2 - x, y2 - y
                color = colors[int(label) % n_colors]
                # Generate and add rectangle to plot
                axes.add_patch(matplotlib.patches.Rectangle((x, y), w, h, linewidth=2,
                                                            edgecolor=color, facecolor=&#39;none&#39;))
                # Generate text if there are any classes
                tag = &#39;{}{}&#39;.format(self.classes[&#39;names&#39;][int(label)], prob)
                matplotlib.pyplot.text(x, y, s=tag, color=&#39;white&#39;,
                                       verticalalignment=&#39;top&#39;, bbox={&#39;color&#39;: color, &#39;pad&#39;: 0})
        # Print stats
        print(&#39;-----\nProcessing time: {}\nBounding boxes:\n{}&#39;.format(time.time() - initial_time, boxes))
        # Show image and plot
        axes.imshow(image)
        matplotlib.pyplot.show()

    def visualize_annotations(self, index, *args, **kwargs):
        &#34;&#34;&#34;Visualize an image with its bounding boxes.

        Args:
            index (int): The index of the image in the dataset to be loaded.
            n_colors (int, optional): The number of colors to use. Optional. Already in the max value.
        &#34;&#34;&#34;
        image, boxes = self.__getitem__(index)
        self.visualize(image, boxes, *args, **kwargs)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="torchsight.datasets.coco.CocoDataset"><code class="flex name class">
<span>class <span class="ident">CocoDataset</span></span>
<span>(</span><span><small>ancestors:</small> torch.utils.data.dataset.Dataset)</span>
</code></dt>
<dd>
<section class="desc"><p>Coco dataset class.</p>
<p>Heavily inspired by the code at:
<a href="https://github.com/SetaSouto/pytorch-retinanet/blob/master/dataloader.py">https://github.com/SetaSouto/pytorch-retinanet/blob/master/dataloader.py</a></p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class CocoDataset(Dataset):
    &#34;&#34;&#34;Coco dataset class.

    Heavily inspired by the code at:
    https://github.com/SetaSouto/pytorch-retinanet/blob/master/dataloader.py
    &#34;&#34;&#34;

    def __init__(self, root, dataset, classes_names=(), transform=None, stats=True):
        &#34;&#34;&#34;Initialize the dataset.

        Initialize the coco api under the &#39;coco&#39; attribute.

        Also, loads the classes and labels.

        It sets the &#39;classes&#39; attribute of the class that contains a dict with four dicts:
        - &#39;ids&#39;: Keep track of the classes&#39; ids given its label (label: int -&gt; id: int).
        - &#39;labels&#39;: Keep track of the label given the id of the class (id: int -&gt; label: int).
        - &#39;names&#39;: Keep track of the name of the class given its label (label: int -&gt; name: string).
        - &#39;length&#39;: Keep track of how many images are for the given label (label: int -&gt; length: int).

        Why labels and ids? Because ids are given by the coco api, are static, but if we filter the classes
        we want to reorder the labels from 0 to N if we load N classes.
        For example, the 2017 dataset contains classes with ids from 1 to 90, but we like to keep labels
        starting from 0 (zero indexed). If we want only to load the classes &#39;person&#39; (id: 1), &#39;bus&#39; (id: 6)
        and &#39;stop sign&#39; (id: 13) we don&#39;t want to work with labels 1, 6 and 13, we want to work with labels
        0, 1, 2, because we loaded only 3 classes.

        You can filter the images and classes to be loaded by using classes_names argument.

        Why we load the bounding boxes already? Because the COCO api already does that, so instead of keeping
        the coco api instance we format the bounding boxes in the initialization and keep track of the path
        of each image. The images are loaded in running time.
        This way we use the api in the initialization only and free some memory.

        Args:
            root (str): COCO root directory.
                Inside this directory we must find the &#39;annotations&#39; and &#39;images&#39; folder for example.
            dataset (str): The name of the set to be loaded. Is the name of the directory that contains
                the images and is in the name of the file that contains the annotations.
                Example: &#39;train2017&#39; will trigger the loading of the images at coco/images/train2017
                and the annotations from the file &#39;instances_train2017.json&#39;.
            classes_names (tuple): Tuple of strings with the name of the classes to load. Only load images with those
                classes&#39; names.
            transform (torchvision.transforms.Compose, optional): A list with transforms to apply to each image.
            stats (Boolean): Print the stats of the classes. Indicates how many images are per category.
                Keep in mind that the sum of all the images per category may not be the same to the total number
                of images, this is because some images could contain more than one object type (very common).
        &#34;&#34;&#34;
        self.transform = transform

        # Initialize the COCO api
        print(&#39;--- COCO API ---&#39;)
        coco = COCO(os.path.join(root, &#39;annotations&#39;, &#39;instances_{}.json&#39;.format(dataset)))
        print(&#39;----------------&#39;)

        # Load classes and set classes dict
        print(&#39;Loading classes and setting labels, names and lengths ...&#39;)
        self.classes = {&#39;ids&#39;: {}, &#39;names&#39;: {}, &#39;labels&#39;: {}, &#39;length&#39;: {}}
        for label, category in enumerate(coco.loadCats(coco.getCatIds(catNms=classes_names))):
            self.classes[&#39;ids&#39;][label] = category[&#39;id&#39;]
            self.classes[&#39;labels&#39;][category[&#39;id&#39;]] = label
            self.classes[&#39;names&#39;][label] = category[&#39;name&#39;]

        # Get filtered images ids
        print(&#39;Loading images info ...&#39;)
        images_ids = set()
        for category_id in self.classes[&#39;ids&#39;].values():
            category_images = set(coco.catToImgs[category_id])
            category_label = self.classes[&#39;labels&#39;][category_id]
            self.classes[&#39;length&#39;][category_label] = len(category_images)
            images_ids |= category_images

        # Init images array that contains tuples with the path to the images and the annotations
        print(&#39;Setting bounding boxes ...&#39;)
        self.images = []
        for image_info in coco.loadImgs(images_ids):
            bounding_boxes = np.zeros((0, 5))

            for annotation in coco.imgToAnns[image_info[&#39;id&#39;]]:
                try:
                    label = self.classes[&#39;labels&#39;][annotation[&#39;category_id&#39;]]
                    bounding_boxes = np.append(bounding_boxes, np.array([[*annotation[&#39;bbox&#39;], label]]), axis=0)
                except KeyError:
                    # The image has a bounding box from a class that does not exists in classes_names sequence
                    continue

            self.images.append((
                os.path.join(root, &#39;images&#39;, dataset, image_info[&#39;file_name&#39;]),  # Image&#39;s path
                bounding_boxes  # Bounding boxes with shape (N, 5)
            ))

        # Print stats
        if stats:
            print(&#39;Images per class:&#39;)
            stats = [(label,
                      self.classes[&#39;names&#39;][label],
                      self.classes[&#39;length&#39;][label])
                     for label in self.classes[&#39;labels&#39;].values()]
            stats = sorted(stats, key=lambda x: x[2], reverse=True)
            for label, name, length in stats:
                print(&#39;{}: {}{}&#39;.format(str(label).ljust(2), name.ljust(15), length))

    def __len__(self):
        &#34;&#34;&#34;Get the length of the dataset.

        Returns:
            int: The length of the dataset.
        &#34;&#34;&#34;
        return len(self.images)

    def __getitem__(self, index):
        &#34;&#34;&#34;Get an item from the dataset given its index.

        Load the image and bounding boxes for that image and return a tuple with
        image, bounding boxes. Both are ndarrays.

        The bounding boxes is a numpy array with shape (N, 5) where N is the number
        of bounding boxes in the image. Why 5? Because they are x1, y1 for the top
        left corner of the bounding box, x2, y2 for the bottom right corner of the
        bounding box and the last one is the label of the class.

        Args:
            index (int): Index of the image in the dataset.

        Returns:
            ndarray: The transformed image if there is any transformation or the original image.
            ndarray: The bounding boxes of the image.
        &#34;&#34;&#34;
        path, bounding_boxes = self.images[index]

        image = skimage.io.imread(path)
        if len(image.shape) == 2:
            image = skimage.color.gray2rgb(image)
        image = image.astype(np.float32) / 255.0

        # Transform from [x,y,w,h] to [x1,y1,x2,y2]
        bounding_boxes[:, 2] = bounding_boxes[:, 0] + bounding_boxes[:, 2]
        bounding_boxes[:, 3] = bounding_boxes[:, 1] + bounding_boxes[:, 3]

        if self.transform:
            image, bounding_boxes = self.transform((image, bounding_boxes))

        return image, bounding_boxes

    def visualize(self, image, boxes=None, initial_time=None, n_colors=20):
        &#34;&#34;&#34;Visualize an image and its bounding boxes.

        Arguments:
            image (torch.Tensor or ndarray): The image to visualize.
            boxes (torch.Tensor or ndarray): The bounding boxes to visualize in the image.
                It must contains the x1, y1, x2, y2 points and the label in the 4th index.
                You could provide the probability of the label too optionally in the 5th index.
                Shape:
                    (number of annotations, 5 or 6)
        &#34;&#34;&#34;
        initial_time = initial_time if initial_time is not None else time.time()

        if torch.is_tensor(image):
            image = image.numpy().transpose(1, 2, 0)

        # Matplotlib colormaps, for more information please visit:
        # https://matplotlib.org/examples/color/colormaps_reference.html
        # Is a continuous map of colors, you can get a color by calling it on a number between
        # 0 and 1
        colormap = matplotlib.pyplot.get_cmap(&#39;tab20&#39;)
        # Select n_colors colors from 0 to 1
        colors = [colormap(i) for i in np.linspace(0, 1, n_colors)]

        # Generate figure and axes
        _, axes = matplotlib.pyplot.subplots(1)

        # Generate rectangles
        if boxes is not None:
            for i in range(boxes.shape[0]):
                # We need the top left corner of the rectangle and its width and height
                if boxes[i].shape[0] == 6:
                    x, y, x2, y2, label, prob = boxes[i]
                    prob = &#39; {:.2f}&#39;.format(prob)
                else:
                    x, y, x2, y2, label = boxes[i]
                    prob = &#39;&#39;
                w, h = x2 - x, y2 - y
                color = colors[int(label) % n_colors]
                # Generate and add rectangle to plot
                axes.add_patch(matplotlib.patches.Rectangle((x, y), w, h, linewidth=2,
                                                            edgecolor=color, facecolor=&#39;none&#39;))
                # Generate text if there are any classes
                tag = &#39;{}{}&#39;.format(self.classes[&#39;names&#39;][int(label)], prob)
                matplotlib.pyplot.text(x, y, s=tag, color=&#39;white&#39;,
                                       verticalalignment=&#39;top&#39;, bbox={&#39;color&#39;: color, &#39;pad&#39;: 0})
        # Print stats
        print(&#39;-----\nProcessing time: {}\nBounding boxes:\n{}&#39;.format(time.time() - initial_time, boxes))
        # Show image and plot
        axes.imshow(image)
        matplotlib.pyplot.show()

    def visualize_annotations(self, index, *args, **kwargs):
        &#34;&#34;&#34;Visualize an image with its bounding boxes.

        Args:
            index (int): The index of the image in the dataset to be loaded.
            n_colors (int, optional): The number of colors to use. Optional. Already in the max value.
        &#34;&#34;&#34;
        image, boxes = self.__getitem__(index)
        self.visualize(image, boxes, *args, **kwargs)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="torchsight.datasets.coco.CocoDataset.__init__"><code class="name flex">
<span>def <span class="ident">__init__</span></span>(<span>self, root, dataset, classes_names=(), transform=None, stats=True)</span>
</code></dt>
<dd>
<section class="desc"><p>Initialize the dataset.</p>
<p>Initialize the coco api under the 'coco' attribute.</p>
<p>Also, loads the classes and labels.</p>
<p>It sets the 'classes' attribute of the class that contains a dict with four dicts:
- 'ids': Keep track of the classes' ids given its label (label: int -&gt; id: int).
- 'labels': Keep track of the label given the id of the class (id: int -&gt; label: int).
- 'names': Keep track of the name of the class given its label (label: int -&gt; name: string).
- 'length': Keep track of how many images are for the given label (label: int -&gt; length: int).</p>
<p>Why labels and ids? Because ids are given by the coco api, are static, but if we filter the classes
we want to reorder the labels from 0 to N if we load N classes.
For example, the 2017 dataset contains classes with ids from 1 to 90, but we like to keep labels
starting from 0 (zero indexed). If we want only to load the classes 'person' (id: 1), 'bus' (id: 6)
and 'stop sign' (id: 13) we don't want to work with labels 1, 6 and 13, we want to work with labels
0, 1, 2, because we loaded only 3 classes.</p>
<p>You can filter the images and classes to be loaded by using classes_names argument.</p>
<p>Why we load the bounding boxes already? Because the COCO api already does that, so instead of keeping
the coco api instance we format the bounding boxes in the initialization and keep track of the path
of each image. The images are loaded in running time.
This way we use the api in the initialization only and free some memory.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>root</code></strong> :&ensp;<code>str</code></dt>
<dd>COCO root directory.
Inside this directory we must find the 'annotations' and 'images' folder for example.</dd>
<dt><strong><code>dataset</code></strong> :&ensp;<code>str</code></dt>
<dd>The name of the set to be loaded. Is the name of the directory that contains
the images and is in the name of the file that contains the annotations.
Example: 'train2017' will trigger the loading of the images at coco/images/train2017
and the annotations from the file 'instances_train2017.json'.</dd>
<dt><strong><code>classes_names</code></strong> :&ensp;<code>tuple</code></dt>
<dd>Tuple of strings with the name of the classes to load. Only load images with those
classes' names.</dd>
<dt><strong><code>transform</code></strong> :&ensp;<code>torchvision.transforms.Compose</code>, optional</dt>
<dd>A list with transforms to apply to each image.</dd>
<dt><strong><code>stats</code></strong> :&ensp;<code>Boolean</code></dt>
<dd>Print the stats of the classes. Indicates how many images are per category.
Keep in mind that the sum of all the images per category may not be the same to the total number
of images, this is because some images could contain more than one object type (very common).</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def __init__(self, root, dataset, classes_names=(), transform=None, stats=True):
    &#34;&#34;&#34;Initialize the dataset.

    Initialize the coco api under the &#39;coco&#39; attribute.

    Also, loads the classes and labels.

    It sets the &#39;classes&#39; attribute of the class that contains a dict with four dicts:
    - &#39;ids&#39;: Keep track of the classes&#39; ids given its label (label: int -&gt; id: int).
    - &#39;labels&#39;: Keep track of the label given the id of the class (id: int -&gt; label: int).
    - &#39;names&#39;: Keep track of the name of the class given its label (label: int -&gt; name: string).
    - &#39;length&#39;: Keep track of how many images are for the given label (label: int -&gt; length: int).

    Why labels and ids? Because ids are given by the coco api, are static, but if we filter the classes
    we want to reorder the labels from 0 to N if we load N classes.
    For example, the 2017 dataset contains classes with ids from 1 to 90, but we like to keep labels
    starting from 0 (zero indexed). If we want only to load the classes &#39;person&#39; (id: 1), &#39;bus&#39; (id: 6)
    and &#39;stop sign&#39; (id: 13) we don&#39;t want to work with labels 1, 6 and 13, we want to work with labels
    0, 1, 2, because we loaded only 3 classes.

    You can filter the images and classes to be loaded by using classes_names argument.

    Why we load the bounding boxes already? Because the COCO api already does that, so instead of keeping
    the coco api instance we format the bounding boxes in the initialization and keep track of the path
    of each image. The images are loaded in running time.
    This way we use the api in the initialization only and free some memory.

    Args:
        root (str): COCO root directory.
            Inside this directory we must find the &#39;annotations&#39; and &#39;images&#39; folder for example.
        dataset (str): The name of the set to be loaded. Is the name of the directory that contains
            the images and is in the name of the file that contains the annotations.
            Example: &#39;train2017&#39; will trigger the loading of the images at coco/images/train2017
            and the annotations from the file &#39;instances_train2017.json&#39;.
        classes_names (tuple): Tuple of strings with the name of the classes to load. Only load images with those
            classes&#39; names.
        transform (torchvision.transforms.Compose, optional): A list with transforms to apply to each image.
        stats (Boolean): Print the stats of the classes. Indicates how many images are per category.
            Keep in mind that the sum of all the images per category may not be the same to the total number
            of images, this is because some images could contain more than one object type (very common).
    &#34;&#34;&#34;
    self.transform = transform

    # Initialize the COCO api
    print(&#39;--- COCO API ---&#39;)
    coco = COCO(os.path.join(root, &#39;annotations&#39;, &#39;instances_{}.json&#39;.format(dataset)))
    print(&#39;----------------&#39;)

    # Load classes and set classes dict
    print(&#39;Loading classes and setting labels, names and lengths ...&#39;)
    self.classes = {&#39;ids&#39;: {}, &#39;names&#39;: {}, &#39;labels&#39;: {}, &#39;length&#39;: {}}
    for label, category in enumerate(coco.loadCats(coco.getCatIds(catNms=classes_names))):
        self.classes[&#39;ids&#39;][label] = category[&#39;id&#39;]
        self.classes[&#39;labels&#39;][category[&#39;id&#39;]] = label
        self.classes[&#39;names&#39;][label] = category[&#39;name&#39;]

    # Get filtered images ids
    print(&#39;Loading images info ...&#39;)
    images_ids = set()
    for category_id in self.classes[&#39;ids&#39;].values():
        category_images = set(coco.catToImgs[category_id])
        category_label = self.classes[&#39;labels&#39;][category_id]
        self.classes[&#39;length&#39;][category_label] = len(category_images)
        images_ids |= category_images

    # Init images array that contains tuples with the path to the images and the annotations
    print(&#39;Setting bounding boxes ...&#39;)
    self.images = []
    for image_info in coco.loadImgs(images_ids):
        bounding_boxes = np.zeros((0, 5))

        for annotation in coco.imgToAnns[image_info[&#39;id&#39;]]:
            try:
                label = self.classes[&#39;labels&#39;][annotation[&#39;category_id&#39;]]
                bounding_boxes = np.append(bounding_boxes, np.array([[*annotation[&#39;bbox&#39;], label]]), axis=0)
            except KeyError:
                # The image has a bounding box from a class that does not exists in classes_names sequence
                continue

        self.images.append((
            os.path.join(root, &#39;images&#39;, dataset, image_info[&#39;file_name&#39;]),  # Image&#39;s path
            bounding_boxes  # Bounding boxes with shape (N, 5)
        ))

    # Print stats
    if stats:
        print(&#39;Images per class:&#39;)
        stats = [(label,
                  self.classes[&#39;names&#39;][label],
                  self.classes[&#39;length&#39;][label])
                 for label in self.classes[&#39;labels&#39;].values()]
        stats = sorted(stats, key=lambda x: x[2], reverse=True)
        for label, name, length in stats:
            print(&#39;{}: {}{}&#39;.format(str(label).ljust(2), name.ljust(15), length))</code></pre>
</details>
</dd>
<dt id="torchsight.datasets.coco.CocoDataset.visualize"><code class="name flex">
<span>def <span class="ident">visualize</span></span>(<span>self, image, boxes=None, initial_time=None, n_colors=20)</span>
</code></dt>
<dd>
<section class="desc"><p>Visualize an image and its bounding boxes.</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>image</code></strong> :&ensp;<code>torch.Tensor</code> or <code>ndarray</code></dt>
<dd>The image to visualize.</dd>
<dt><strong><code>boxes</code></strong> :&ensp;<code>torch.Tensor</code> or <code>ndarray</code></dt>
<dd>The bounding boxes to visualize in the image.
It must contains the x1, y1, x2, y2 points and the label in the 4th index.
You could provide the probability of the label too optionally in the 5th index.
Shape:
(number of annotations, 5 or 6)</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def visualize(self, image, boxes=None, initial_time=None, n_colors=20):
    &#34;&#34;&#34;Visualize an image and its bounding boxes.

    Arguments:
        image (torch.Tensor or ndarray): The image to visualize.
        boxes (torch.Tensor or ndarray): The bounding boxes to visualize in the image.
            It must contains the x1, y1, x2, y2 points and the label in the 4th index.
            You could provide the probability of the label too optionally in the 5th index.
            Shape:
                (number of annotations, 5 or 6)
    &#34;&#34;&#34;
    initial_time = initial_time if initial_time is not None else time.time()

    if torch.is_tensor(image):
        image = image.numpy().transpose(1, 2, 0)

    # Matplotlib colormaps, for more information please visit:
    # https://matplotlib.org/examples/color/colormaps_reference.html
    # Is a continuous map of colors, you can get a color by calling it on a number between
    # 0 and 1
    colormap = matplotlib.pyplot.get_cmap(&#39;tab20&#39;)
    # Select n_colors colors from 0 to 1
    colors = [colormap(i) for i in np.linspace(0, 1, n_colors)]

    # Generate figure and axes
    _, axes = matplotlib.pyplot.subplots(1)

    # Generate rectangles
    if boxes is not None:
        for i in range(boxes.shape[0]):
            # We need the top left corner of the rectangle and its width and height
            if boxes[i].shape[0] == 6:
                x, y, x2, y2, label, prob = boxes[i]
                prob = &#39; {:.2f}&#39;.format(prob)
            else:
                x, y, x2, y2, label = boxes[i]
                prob = &#39;&#39;
            w, h = x2 - x, y2 - y
            color = colors[int(label) % n_colors]
            # Generate and add rectangle to plot
            axes.add_patch(matplotlib.patches.Rectangle((x, y), w, h, linewidth=2,
                                                        edgecolor=color, facecolor=&#39;none&#39;))
            # Generate text if there are any classes
            tag = &#39;{}{}&#39;.format(self.classes[&#39;names&#39;][int(label)], prob)
            matplotlib.pyplot.text(x, y, s=tag, color=&#39;white&#39;,
                                   verticalalignment=&#39;top&#39;, bbox={&#39;color&#39;: color, &#39;pad&#39;: 0})
    # Print stats
    print(&#39;-----\nProcessing time: {}\nBounding boxes:\n{}&#39;.format(time.time() - initial_time, boxes))
    # Show image and plot
    axes.imshow(image)
    matplotlib.pyplot.show()</code></pre>
</details>
</dd>
<dt id="torchsight.datasets.coco.CocoDataset.visualize_annotations"><code class="name flex">
<span>def <span class="ident">visualize_annotations</span></span>(<span>self, index, *args, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Visualize an image with its bounding boxes.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>index</code></strong> :&ensp;<code>int</code></dt>
<dd>The index of the image in the dataset to be loaded.</dd>
<dt><strong><code>n_colors</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The number of colors to use. Optional. Already in the max value.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def visualize_annotations(self, index, *args, **kwargs):
    &#34;&#34;&#34;Visualize an image with its bounding boxes.

    Args:
        index (int): The index of the image in the dataset to be loaded.
        n_colors (int, optional): The number of colors to use. Optional. Already in the max value.
    &#34;&#34;&#34;
    image, boxes = self.__getitem__(index)
    self.visualize(image, boxes, *args, **kwargs)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="torchsight.datasets" href="index.html">torchsight.datasets</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="torchsight.datasets.coco.CocoDataset" href="#torchsight.datasets.coco.CocoDataset">CocoDataset</a></code></h4>
<ul class="">
<li><code><a title="torchsight.datasets.coco.CocoDataset.__init__" href="#torchsight.datasets.coco.CocoDataset.__init__">__init__</a></code></li>
<li><code><a title="torchsight.datasets.coco.CocoDataset.visualize" href="#torchsight.datasets.coco.CocoDataset.visualize">visualize</a></code></li>
<li><code><a title="torchsight.datasets.coco.CocoDataset.visualize_annotations" href="#torchsight.datasets.coco.CocoDataset.visualize_annotations">visualize_annotations</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.5.4</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>