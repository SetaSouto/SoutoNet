<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.5.4" />
<title>torchsight.losses.dlde API documentation</title>
<meta name="description" content="The criterion for the weighted DLDENet." />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.name small{font-weight:normal}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase;cursor:pointer}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title"><code>torchsight.losses.dlde</code> module</h1>
</header>
<section id="section-intro">
<p>The criterion for the weighted DLDENet.</p>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">&#34;&#34;&#34;The criterion for the weighted DLDENet.&#34;&#34;&#34;
import torch
from torch import nn

from .ccs import CCSLoss
from .focal import FocalLoss


class DLDENetLoss(nn.Module):
    &#34;&#34;&#34;Join the CCS and the Focal losses in one single module.&#34;&#34;&#34;

    def __init__(self, alpha=0.25, gamma=2.0, sigma=3.0, iou_thresholds=None, device=None):
        &#34;&#34;&#34;Initialize the losses.

        See their corresponding docs for more information.

        Arguments:
            alpha (float): Alpha parameter for the focal loss.
            gamma (float): Gamma parameter for the focal loss.
            sigma (float): Point that defines the change from L1 loss to L2 loss (smooth L1).
            iou_thresholds (dict): Indicates the thresholds to assign an anchor as background or object.
            device (str, optional): Indicates the device where to run the loss.
        &#34;&#34;&#34;
        super().__init__()

        if iou_thresholds is None:
            iou_thresholds = {&#39;background&#39;: 0.4, &#39;object&#39;: 0.5}

        device = device if device is not None else &#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39;

        self.focal = FocalLoss(alpha, gamma, sigma, iou_thresholds, device)
        self.ccs = CCSLoss(iou_thresholds, device)

    def forward(self, anchors, regressions, classifications, annotations, model):
        &#34;&#34;&#34;Compute the different losses for the batch.

        Arguments:
            anchors (torch.Tensor): The base anchors (without the transformation to adjust the
                bounding boxes).
                Shape:
                    `(batch size, total boxes, 4)`
            regressions (torch.Tensor): The regression values to adjust the anchors to the predicted
                bounding boxes.
                Shape:
                    `(batch size, total boxes, 4)`
            classifications (torch.Tensor): The probabilities for each class at each bounding box.
                Shape:
                    `(batch size, total boxes, number of classes)`
            annotations (torch.Tensor): Ground truth. Tensor with the bounding boxes and the label for
                the object. The values must be x1, y1 (top left corner), x2, y2 (bottom right corner)
                and the last value is the label.
                Shape:
                    `(batch size, maximum objects in any image, 5)`
            model (torch.nn.Module): The DLDENet model with its embeddings and weights.

                Why `maximum objects in any image`? Because if we have more than one image, each image
                could have different amounts of objects inside and have different dimensions in the
                ground truth (dim 1 of the batch). So we could have the maximum amount of objects
                inside any image and then the rest of the images ground truths could be populated
                with -1.0. So if this loss finds a ground truth box populated with -1.0 it understands
                that it was to match the dimensions and have only one tensor and it won&#39;t use that
                annotation.

        Returns:
            tuple: A tuple with the tensors with the classification, regression and cosine similarity losses.
        &#34;&#34;&#34;
        classification, regression = self.focal(anchors, regressions, classifications, annotations)
        similarity = self.ccs(anchors, model.classification.embeddings, model.classification.weights, annotations)

        return classification, regression, similarity</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="torchsight.losses.dlde.DLDENetLoss"><code class="flex name class">
<span>class <span class="ident">DLDENetLoss</span></span>
<span>(</span><span><small>ancestors:</small> torch.nn.modules.module.Module)</span>
</code></dt>
<dd>
<section class="desc"><p>Join the CCS and the Focal losses in one single module.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class DLDENetLoss(nn.Module):
    &#34;&#34;&#34;Join the CCS and the Focal losses in one single module.&#34;&#34;&#34;

    def __init__(self, alpha=0.25, gamma=2.0, sigma=3.0, iou_thresholds=None, device=None):
        &#34;&#34;&#34;Initialize the losses.

        See their corresponding docs for more information.

        Arguments:
            alpha (float): Alpha parameter for the focal loss.
            gamma (float): Gamma parameter for the focal loss.
            sigma (float): Point that defines the change from L1 loss to L2 loss (smooth L1).
            iou_thresholds (dict): Indicates the thresholds to assign an anchor as background or object.
            device (str, optional): Indicates the device where to run the loss.
        &#34;&#34;&#34;
        super().__init__()

        if iou_thresholds is None:
            iou_thresholds = {&#39;background&#39;: 0.4, &#39;object&#39;: 0.5}

        device = device if device is not None else &#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39;

        self.focal = FocalLoss(alpha, gamma, sigma, iou_thresholds, device)
        self.ccs = CCSLoss(iou_thresholds, device)

    def forward(self, anchors, regressions, classifications, annotations, model):
        &#34;&#34;&#34;Compute the different losses for the batch.

        Arguments:
            anchors (torch.Tensor): The base anchors (without the transformation to adjust the
                bounding boxes).
                Shape:
                    `(batch size, total boxes, 4)`
            regressions (torch.Tensor): The regression values to adjust the anchors to the predicted
                bounding boxes.
                Shape:
                    `(batch size, total boxes, 4)`
            classifications (torch.Tensor): The probabilities for each class at each bounding box.
                Shape:
                    `(batch size, total boxes, number of classes)`
            annotations (torch.Tensor): Ground truth. Tensor with the bounding boxes and the label for
                the object. The values must be x1, y1 (top left corner), x2, y2 (bottom right corner)
                and the last value is the label.
                Shape:
                    `(batch size, maximum objects in any image, 5)`
            model (torch.nn.Module): The DLDENet model with its embeddings and weights.

                Why `maximum objects in any image`? Because if we have more than one image, each image
                could have different amounts of objects inside and have different dimensions in the
                ground truth (dim 1 of the batch). So we could have the maximum amount of objects
                inside any image and then the rest of the images ground truths could be populated
                with -1.0. So if this loss finds a ground truth box populated with -1.0 it understands
                that it was to match the dimensions and have only one tensor and it won&#39;t use that
                annotation.

        Returns:
            tuple: A tuple with the tensors with the classification, regression and cosine similarity losses.
        &#34;&#34;&#34;
        classification, regression = self.focal(anchors, regressions, classifications, annotations)
        similarity = self.ccs(anchors, model.classification.embeddings, model.classification.weights, annotations)

        return classification, regression, similarity</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="torchsight.losses.dlde.DLDENetLoss.__init__"><code class="name flex">
<span>def <span class="ident">__init__</span></span>(<span>self, alpha=0.25, gamma=2.0, sigma=3.0, iou_thresholds=None, device=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Initialize the losses.</p>
<p>See their corresponding docs for more information.</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float</code></dt>
<dd>Alpha parameter for the focal loss.</dd>
<dt><strong><code>gamma</code></strong> :&ensp;<code>float</code></dt>
<dd>Gamma parameter for the focal loss.</dd>
<dt><strong><code>sigma</code></strong> :&ensp;<code>float</code></dt>
<dd>Point that defines the change from L1 loss to L2 loss (smooth L1).</dd>
<dt><strong><code>iou_thresholds</code></strong> :&ensp;<code>dict</code></dt>
<dd>Indicates the thresholds to assign an anchor as background or object.</dd>
<dt><strong><code>device</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Indicates the device where to run the loss.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def __init__(self, alpha=0.25, gamma=2.0, sigma=3.0, iou_thresholds=None, device=None):
    &#34;&#34;&#34;Initialize the losses.

    See their corresponding docs for more information.

    Arguments:
        alpha (float): Alpha parameter for the focal loss.
        gamma (float): Gamma parameter for the focal loss.
        sigma (float): Point that defines the change from L1 loss to L2 loss (smooth L1).
        iou_thresholds (dict): Indicates the thresholds to assign an anchor as background or object.
        device (str, optional): Indicates the device where to run the loss.
    &#34;&#34;&#34;
    super().__init__()

    if iou_thresholds is None:
        iou_thresholds = {&#39;background&#39;: 0.4, &#39;object&#39;: 0.5}

    device = device if device is not None else &#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39;

    self.focal = FocalLoss(alpha, gamma, sigma, iou_thresholds, device)
    self.ccs = CCSLoss(iou_thresholds, device)</code></pre>
</details>
</dd>
<dt id="torchsight.losses.dlde.DLDENetLoss.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, anchors, regressions, classifications, annotations, model)</span>
</code></dt>
<dd>
<section class="desc"><p>Compute the different losses for the batch.</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>anchors</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The base anchors (without the transformation to adjust the
bounding boxes).
Shape:
<code>(batch size, total boxes, 4)</code></dd>
<dt><strong><code>regressions</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The regression values to adjust the anchors to the predicted
bounding boxes.
Shape:
<code>(batch size, total boxes, 4)</code></dd>
<dt><strong><code>classifications</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The probabilities for each class at each bounding box.
Shape:
<code>(batch size, total boxes, number of classes)</code></dd>
<dt><strong><code>annotations</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>Ground truth. Tensor with the bounding boxes and the label for
the object. The values must be x1, y1 (top left corner), x2, y2 (bottom right corner)
and the last value is the label.
Shape:
<code>(batch size, maximum objects in any image, 5)</code></dd>
<dt><strong><code>model</code></strong> :&ensp;<code>torch.nn.Module</code></dt>
<dd>
<p>The DLDENet model with its embeddings and weights.</p>
<p>Why <code>maximum objects in any image</code>? Because if we have more than one image, each image
could have different amounts of objects inside and have different dimensions in the
ground truth (dim 1 of the batch). So we could have the maximum amount of objects
inside any image and then the rest of the images ground truths could be populated
with -1.0. So if this loss finds a ground truth box populated with -1.0 it understands
that it was to match the dimensions and have only one tensor and it won't use that
annotation.</p>
</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>tuple</code></strong></dt>
<dd>A tuple with the tensors with the classification, regression and cosine similarity losses.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def forward(self, anchors, regressions, classifications, annotations, model):
    &#34;&#34;&#34;Compute the different losses for the batch.

    Arguments:
        anchors (torch.Tensor): The base anchors (without the transformation to adjust the
            bounding boxes).
            Shape:
                `(batch size, total boxes, 4)`
        regressions (torch.Tensor): The regression values to adjust the anchors to the predicted
            bounding boxes.
            Shape:
                `(batch size, total boxes, 4)`
        classifications (torch.Tensor): The probabilities for each class at each bounding box.
            Shape:
                `(batch size, total boxes, number of classes)`
        annotations (torch.Tensor): Ground truth. Tensor with the bounding boxes and the label for
            the object. The values must be x1, y1 (top left corner), x2, y2 (bottom right corner)
            and the last value is the label.
            Shape:
                `(batch size, maximum objects in any image, 5)`
        model (torch.nn.Module): The DLDENet model with its embeddings and weights.

            Why `maximum objects in any image`? Because if we have more than one image, each image
            could have different amounts of objects inside and have different dimensions in the
            ground truth (dim 1 of the batch). So we could have the maximum amount of objects
            inside any image and then the rest of the images ground truths could be populated
            with -1.0. So if this loss finds a ground truth box populated with -1.0 it understands
            that it was to match the dimensions and have only one tensor and it won&#39;t use that
            annotation.

    Returns:
        tuple: A tuple with the tensors with the classification, regression and cosine similarity losses.
    &#34;&#34;&#34;
    classification, regression = self.focal(anchors, regressions, classifications, annotations)
    similarity = self.ccs(anchors, model.classification.embeddings, model.classification.weights, annotations)

    return classification, regression, similarity</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="torchsight.losses" href="index.html">torchsight.losses</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="torchsight.losses.dlde.DLDENetLoss" href="#torchsight.losses.dlde.DLDENetLoss">DLDENetLoss</a></code></h4>
<ul class="">
<li><code><a title="torchsight.losses.dlde.DLDENetLoss.__init__" href="#torchsight.losses.dlde.DLDENetLoss.__init__">__init__</a></code></li>
<li><code><a title="torchsight.losses.dlde.DLDENetLoss.forward" href="#torchsight.losses.dlde.DLDENetLoss.forward">forward</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.5.4</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>