<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.5.4" />
<title>torchsight.losses.ccs API documentation</title>
<meta name="description" content="Implementation of the Classification vector-centered Cosine Similarity from the paper
[One-shot Face Recognition by Promoting Underrepresented â€¦" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.name small{font-weight:normal}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase;cursor:pointer}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title"><code>torchsight.losses.ccs</code> module</h1>
</header>
<section id="section-intro">
<p>Implementation of the Classification vector-centered Cosine Similarity from the paper
<a href="https://arxiv.org/pdf/1707.05574.pdf">One-shot Face Recognition by Promoting Underrepresented Classes</a>.</p>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">&#34;&#34;&#34;Implementation of the Classification vector-centered Cosine Similarity from the paper
[One-shot Face Recognition by Promoting Underrepresented Classes](https://arxiv.org/pdf/1707.05574.pdf).
&#34;&#34;&#34;
import torch
from torch import nn

from ..models import Anchors


class CCSLoss(nn.Module):
    &#34;&#34;&#34;Classification vector-centered Cosine Similarity Loss.

    As indicated in the equation 5 of the paper, this loss tries to minimize the angular distance
    between the embeddings (or features before the classification) and the weighted vector that does
    the classification.

    This is done by simply doing a dot product between the embedding and the classification vector
    and normalizing by their norms.

    It will apply this loss term only to those embeddings that are assigned to an object.
    &#34;&#34;&#34;

    def __init__(self, iou_thresholds=None, device=None):
        &#34;&#34;&#34;Initialize the loss.

        Arguments:
            iou_thresholds (dict, optional): Indicates the thresholds to assign an anchor as background or object.
            device (str, optional): Indicates the device where to run the loss.
        &#34;&#34;&#34;
        super().__init__()

        if iou_thresholds is None:
            iou_thresholds = {&#39;background&#39;: 0.4, &#39;object&#39;: 0.5}
        self.iou_thresholds = iou_thresholds
        self.device = device if device is not None else &#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39;

    def forward(self, anchors, embeddings, weights, annotations):
        &#34;&#34;&#34;Get the mean CCS loss.

        Arguments:
            anchors (torch.Tensor): The base anchors (without the transformation to adjust the
                bounding boxes).
                Shape:
                    (batch size, total boxes, 4)
            embeddings (torch.Tensor): The embeddings generated for each anchor.
                Shape:
                    (batch size, number of anchors, embedding size)
            annotations (torch.Tensor): Ground truth. Tensor with the bounding boxes and the label for
                the object. The values must be x1, y1 (top left corner), x2, y2 (bottom right corner)
                and the last value is the label.
                Shape:
                    (batch size, maximum objects in any image, 5).

                Why maximum objects in any image? Because if we have more than one image, each image
                could have different amounts of objects inside and have different dimensions in the
                ground truth (dim 1 of the batch). So we could have the maximum amount of objects
                inside any image and then the rest of the images ground truths could be populated
                with -1.0. So if this loss finds a ground truth box populated with -1.0 it understands
                that it was to match the dimensions and have only one tensor.

        Returns:
            torch.Tensor: The mean CSS loss.
        &#34;&#34;&#34;
        # We want to use the weights but not backprop over they, we want to backprop over the embeddings
        original_weights = weights.detach()

        batch_anchors = anchors
        batch_embeddings = embeddings
        batch_annotations = annotations

        losses = []

        for i, anchors in enumerate(batch_anchors):
            embeddings = batch_embeddings[i]
            annotations = batch_annotations[i]
            weights = original_weights.clone()

            # Keep only the real labels
            annotations = annotations[annotations[:, -1] != -1]

            # Zero loss for this image if it does not have any annotation
            if annotations.shape[0] == 0:
                losses.append(torch.zeros(1).mean().to(self.device))
                continue

            # Get assignations of the annotations to the anchors
            # Get the assigned annotations (the i-th assigned annotation is the annotation assigned to the i-th
            # anchor)
            # Get the masks to select the anchors assigned to an object (IoU bigger than iou_object threshold)
            assignations = Anchors.assign(anchors, annotations, thresholds=self.iou_thresholds)
            assigned_annotations, selected_anchors_objects, *_ = assignations

            # Continue with the next image if there are no selected objects
            if selected_anchors_objects.sum() == 0:
                losses.append(torch.zeros(1).mean().to(self.device))
                continue

            # We must compute the cosine similarity between each embedding and its corresponding weight vector of its
            # assigned annotation. So we can do this by a single matrix multiplication between all the selected anchors
            # as objects embeddings and their corresponding vectors.
            # Shape (selected embeddings, embedding size)
            embeddings = embeddings[selected_anchors_objects]
            # Shape (embedding size, number of selected embeddings)
            weights = weights[:, assigned_annotations[selected_anchors_objects, -1].long()]

            # We need to do a batch matrix multiplication with shape:
            # (number of selected anchors, 1, embedding size) * (number of selected anchors, embedding size, 1)

            # Reshape the embeddings to have shape (number of selected embeddings, 1, embedding size)
            embeddings = embeddings.unsqueeze(dim=1)
            # Reshape the weights to have shape (number of selected embeddings, embedding size, 1)
            weights = weights.t().unsqueeze(dim=2)

            # Compute the loss
            loss = -1 * torch.matmul(embeddings, weights).view(-1)  # Shape (selected embeddings,)
            loss /= embeddings.squeeze(dim=1).norm(dim=1)  # Normalize by the embeddings&#39; norms
            loss /= weights.squeeze(dim=2).norm(dim=1)  # Normalize by the weights&#39; norms
            # Add one to have a minimum loss of zero (because cosine similarity ranges from -1 to 1) and normalize
            # the value between 0 and 1 to have a more meaningfull loss
            loss = (loss + 1) / 2
            losses.append(loss.mean())

        return torch.stack(losses).mean()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="torchsight.losses.ccs.CCSLoss"><code class="flex name class">
<span>class <span class="ident">CCSLoss</span></span>
<span>(</span><span><small>ancestors:</small> torch.nn.modules.module.Module)</span>
</code></dt>
<dd>
<section class="desc"><p>Classification vector-centered Cosine Similarity Loss.</p>
<p>As indicated in the equation 5 of the paper, this loss tries to minimize the angular distance
between the embeddings (or features before the classification) and the weighted vector that does
the classification.</p>
<p>This is done by simply doing a dot product between the embedding and the classification vector
and normalizing by their norms.</p>
<p>It will apply this loss term only to those embeddings that are assigned to an object.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class CCSLoss(nn.Module):
    &#34;&#34;&#34;Classification vector-centered Cosine Similarity Loss.

    As indicated in the equation 5 of the paper, this loss tries to minimize the angular distance
    between the embeddings (or features before the classification) and the weighted vector that does
    the classification.

    This is done by simply doing a dot product between the embedding and the classification vector
    and normalizing by their norms.

    It will apply this loss term only to those embeddings that are assigned to an object.
    &#34;&#34;&#34;

    def __init__(self, iou_thresholds=None, device=None):
        &#34;&#34;&#34;Initialize the loss.

        Arguments:
            iou_thresholds (dict, optional): Indicates the thresholds to assign an anchor as background or object.
            device (str, optional): Indicates the device where to run the loss.
        &#34;&#34;&#34;
        super().__init__()

        if iou_thresholds is None:
            iou_thresholds = {&#39;background&#39;: 0.4, &#39;object&#39;: 0.5}
        self.iou_thresholds = iou_thresholds
        self.device = device if device is not None else &#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39;

    def forward(self, anchors, embeddings, weights, annotations):
        &#34;&#34;&#34;Get the mean CCS loss.

        Arguments:
            anchors (torch.Tensor): The base anchors (without the transformation to adjust the
                bounding boxes).
                Shape:
                    (batch size, total boxes, 4)
            embeddings (torch.Tensor): The embeddings generated for each anchor.
                Shape:
                    (batch size, number of anchors, embedding size)
            annotations (torch.Tensor): Ground truth. Tensor with the bounding boxes and the label for
                the object. The values must be x1, y1 (top left corner), x2, y2 (bottom right corner)
                and the last value is the label.
                Shape:
                    (batch size, maximum objects in any image, 5).

                Why maximum objects in any image? Because if we have more than one image, each image
                could have different amounts of objects inside and have different dimensions in the
                ground truth (dim 1 of the batch). So we could have the maximum amount of objects
                inside any image and then the rest of the images ground truths could be populated
                with -1.0. So if this loss finds a ground truth box populated with -1.0 it understands
                that it was to match the dimensions and have only one tensor.

        Returns:
            torch.Tensor: The mean CSS loss.
        &#34;&#34;&#34;
        # We want to use the weights but not backprop over they, we want to backprop over the embeddings
        original_weights = weights.detach()

        batch_anchors = anchors
        batch_embeddings = embeddings
        batch_annotations = annotations

        losses = []

        for i, anchors in enumerate(batch_anchors):
            embeddings = batch_embeddings[i]
            annotations = batch_annotations[i]
            weights = original_weights.clone()

            # Keep only the real labels
            annotations = annotations[annotations[:, -1] != -1]

            # Zero loss for this image if it does not have any annotation
            if annotations.shape[0] == 0:
                losses.append(torch.zeros(1).mean().to(self.device))
                continue

            # Get assignations of the annotations to the anchors
            # Get the assigned annotations (the i-th assigned annotation is the annotation assigned to the i-th
            # anchor)
            # Get the masks to select the anchors assigned to an object (IoU bigger than iou_object threshold)
            assignations = Anchors.assign(anchors, annotations, thresholds=self.iou_thresholds)
            assigned_annotations, selected_anchors_objects, *_ = assignations

            # Continue with the next image if there are no selected objects
            if selected_anchors_objects.sum() == 0:
                losses.append(torch.zeros(1).mean().to(self.device))
                continue

            # We must compute the cosine similarity between each embedding and its corresponding weight vector of its
            # assigned annotation. So we can do this by a single matrix multiplication between all the selected anchors
            # as objects embeddings and their corresponding vectors.
            # Shape (selected embeddings, embedding size)
            embeddings = embeddings[selected_anchors_objects]
            # Shape (embedding size, number of selected embeddings)
            weights = weights[:, assigned_annotations[selected_anchors_objects, -1].long()]

            # We need to do a batch matrix multiplication with shape:
            # (number of selected anchors, 1, embedding size) * (number of selected anchors, embedding size, 1)

            # Reshape the embeddings to have shape (number of selected embeddings, 1, embedding size)
            embeddings = embeddings.unsqueeze(dim=1)
            # Reshape the weights to have shape (number of selected embeddings, embedding size, 1)
            weights = weights.t().unsqueeze(dim=2)

            # Compute the loss
            loss = -1 * torch.matmul(embeddings, weights).view(-1)  # Shape (selected embeddings,)
            loss /= embeddings.squeeze(dim=1).norm(dim=1)  # Normalize by the embeddings&#39; norms
            loss /= weights.squeeze(dim=2).norm(dim=1)  # Normalize by the weights&#39; norms
            # Add one to have a minimum loss of zero (because cosine similarity ranges from -1 to 1) and normalize
            # the value between 0 and 1 to have a more meaningfull loss
            loss = (loss + 1) / 2
            losses.append(loss.mean())

        return torch.stack(losses).mean()</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="torchsight.losses.ccs.CCSLoss.__init__"><code class="name flex">
<span>def <span class="ident">__init__</span></span>(<span>self, iou_thresholds=None, device=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Initialize the loss.</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>iou_thresholds</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>Indicates the thresholds to assign an anchor as background or object.</dd>
<dt><strong><code>device</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Indicates the device where to run the loss.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def __init__(self, iou_thresholds=None, device=None):
    &#34;&#34;&#34;Initialize the loss.

    Arguments:
        iou_thresholds (dict, optional): Indicates the thresholds to assign an anchor as background or object.
        device (str, optional): Indicates the device where to run the loss.
    &#34;&#34;&#34;
    super().__init__()

    if iou_thresholds is None:
        iou_thresholds = {&#39;background&#39;: 0.4, &#39;object&#39;: 0.5}
    self.iou_thresholds = iou_thresholds
    self.device = device if device is not None else &#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39;</code></pre>
</details>
</dd>
<dt id="torchsight.losses.ccs.CCSLoss.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, anchors, embeddings, weights, annotations)</span>
</code></dt>
<dd>
<section class="desc"><p>Get the mean CCS loss.</p>
<h2 id="arguments">Arguments</h2>
<dl>
<dt><strong><code>anchors</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The base anchors (without the transformation to adjust the
bounding boxes).
Shape:
(batch size, total boxes, 4)</dd>
<dt><strong><code>embeddings</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The embeddings generated for each anchor.
Shape:
(batch size, number of anchors, embedding size)</dd>
<dt><strong><code>annotations</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>
<p>Ground truth. Tensor with the bounding boxes and the label for
the object. The values must be x1, y1 (top left corner), x2, y2 (bottom right corner)
and the last value is the label.
Shape:
(batch size, maximum objects in any image, 5).</p>
<p>Why maximum objects in any image? Because if we have more than one image, each image
could have different amounts of objects inside and have different dimensions in the
ground truth (dim 1 of the batch). So we could have the maximum amount of objects
inside any image and then the rest of the images ground truths could be populated
with -1.0. So if this loss finds a ground truth box populated with -1.0 it understands
that it was to match the dimensions and have only one tensor.</p>
</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>torch.Tensor: The mean CSS loss.</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def forward(self, anchors, embeddings, weights, annotations):
    &#34;&#34;&#34;Get the mean CCS loss.

    Arguments:
        anchors (torch.Tensor): The base anchors (without the transformation to adjust the
            bounding boxes).
            Shape:
                (batch size, total boxes, 4)
        embeddings (torch.Tensor): The embeddings generated for each anchor.
            Shape:
                (batch size, number of anchors, embedding size)
        annotations (torch.Tensor): Ground truth. Tensor with the bounding boxes and the label for
            the object. The values must be x1, y1 (top left corner), x2, y2 (bottom right corner)
            and the last value is the label.
            Shape:
                (batch size, maximum objects in any image, 5).

            Why maximum objects in any image? Because if we have more than one image, each image
            could have different amounts of objects inside and have different dimensions in the
            ground truth (dim 1 of the batch). So we could have the maximum amount of objects
            inside any image and then the rest of the images ground truths could be populated
            with -1.0. So if this loss finds a ground truth box populated with -1.0 it understands
            that it was to match the dimensions and have only one tensor.

    Returns:
        torch.Tensor: The mean CSS loss.
    &#34;&#34;&#34;
    # We want to use the weights but not backprop over they, we want to backprop over the embeddings
    original_weights = weights.detach()

    batch_anchors = anchors
    batch_embeddings = embeddings
    batch_annotations = annotations

    losses = []

    for i, anchors in enumerate(batch_anchors):
        embeddings = batch_embeddings[i]
        annotations = batch_annotations[i]
        weights = original_weights.clone()

        # Keep only the real labels
        annotations = annotations[annotations[:, -1] != -1]

        # Zero loss for this image if it does not have any annotation
        if annotations.shape[0] == 0:
            losses.append(torch.zeros(1).mean().to(self.device))
            continue

        # Get assignations of the annotations to the anchors
        # Get the assigned annotations (the i-th assigned annotation is the annotation assigned to the i-th
        # anchor)
        # Get the masks to select the anchors assigned to an object (IoU bigger than iou_object threshold)
        assignations = Anchors.assign(anchors, annotations, thresholds=self.iou_thresholds)
        assigned_annotations, selected_anchors_objects, *_ = assignations

        # Continue with the next image if there are no selected objects
        if selected_anchors_objects.sum() == 0:
            losses.append(torch.zeros(1).mean().to(self.device))
            continue

        # We must compute the cosine similarity between each embedding and its corresponding weight vector of its
        # assigned annotation. So we can do this by a single matrix multiplication between all the selected anchors
        # as objects embeddings and their corresponding vectors.
        # Shape (selected embeddings, embedding size)
        embeddings = embeddings[selected_anchors_objects]
        # Shape (embedding size, number of selected embeddings)
        weights = weights[:, assigned_annotations[selected_anchors_objects, -1].long()]

        # We need to do a batch matrix multiplication with shape:
        # (number of selected anchors, 1, embedding size) * (number of selected anchors, embedding size, 1)

        # Reshape the embeddings to have shape (number of selected embeddings, 1, embedding size)
        embeddings = embeddings.unsqueeze(dim=1)
        # Reshape the weights to have shape (number of selected embeddings, embedding size, 1)
        weights = weights.t().unsqueeze(dim=2)

        # Compute the loss
        loss = -1 * torch.matmul(embeddings, weights).view(-1)  # Shape (selected embeddings,)
        loss /= embeddings.squeeze(dim=1).norm(dim=1)  # Normalize by the embeddings&#39; norms
        loss /= weights.squeeze(dim=2).norm(dim=1)  # Normalize by the weights&#39; norms
        # Add one to have a minimum loss of zero (because cosine similarity ranges from -1 to 1) and normalize
        # the value between 0 and 1 to have a more meaningfull loss
        loss = (loss + 1) / 2
        losses.append(loss.mean())

    return torch.stack(losses).mean()</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="torchsight.losses" href="index.html">torchsight.losses</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="torchsight.losses.ccs.CCSLoss" href="#torchsight.losses.ccs.CCSLoss">CCSLoss</a></code></h4>
<ul class="">
<li><code><a title="torchsight.losses.ccs.CCSLoss.__init__" href="#torchsight.losses.ccs.CCSLoss.__init__">__init__</a></code></li>
<li><code><a title="torchsight.losses.ccs.CCSLoss.forward" href="#torchsight.losses.ccs.CCSLoss.forward">forward</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.5.4</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>