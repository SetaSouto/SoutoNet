"""Deep Local Directional Embedding (DLDE) module.

This net is based on the RetinaNet architecture but provides a different submodule
for classification and additional methods.
"""
import torch
from torch import nn

from .retinanet import RetinaNet, SubModule
from .anchors import Anchors


class DirectionalClassification(nn.Module):
    """Directional classification module.

    This module takes the features generated by the Feature Pyramid Network and generates
    "embeddings" (normalized vectors for each base anchor) that must point in the same direction
    that its correct label and so it can calculate the probability of being for a given class.

    So, in simple words, we take a picture and project each section of the image to a sphere with
    unit radius. Each section has an embedding, a vector that points in some direction that it is
    in this sphere.

    What we are going to do? We are going to try that embeddings of similar objects (i.e. same class)
    point to the same direction. It would be an error that the embeddings point to the exact same
    direction, we must have a threshold, so we can model this with a Von Mises-Fisher distribution.

    At this point we need a picture, check this and create your mental image:
    https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTDBP4M7VABT1wGuXccdg707MzyQPTpb5O6D3TUCZFapDBG_jiX

    So, we want that semantically similar objects points to similar directions, so the direction of the
    embedding contains the semantic of the object without losing much visual detail.

    I was inspired by the following paper:

    Directional Statistics-based Deep Metric Learning for Image Classification and Retrieval.
    Arxiv: https://arxiv.org/abs/1802.09662

    ----- How does it work -----

    Basically, it assumes that we have a CNN that generates unit embeddings (L2 normalized)
    with size d for images of C classes. Assuming that y is the correct label for the image x,
    the loss tries to maximize the probability P(y | x, theta, means, k) where theta are the
    parameters of the CNN, means are the mean vectors for each class and k is the concentration
    of the distribution. The distribution is like a gaussian but projected to an hypersphere.
    See equation 10 in the paper.

    It updates the mean vector of each class based on the embeddings that pass through the network.

    How can it update the means?
    Because every time that the embeddings pass through the network we must provide the real annotations
    to update the classes' means with those embeddings. Not a fully update of course, we sum this embeddings
    to the previously seen embeddings and then you can call model.update_means() to normalize those sums
    and set them as the new means for each class.

    Obviously, a correct way to set a more precise mean is to call update_means() after each epoch,
    because it will compute the mean with the average of the class' embeddings.
    """

    def __init__(self, in_channels, embedding_size, anchors, features, classes, concentration, assignation_thresholds,
                 device=None):
        """Initialize the module.

        Arguments:
            in_channels (int): The number of channels of the feature map.
            embedding_size (int): Length of the embedding vector to generate.
            anchors (int): Number of anchors per location in the feature map.
            features (int): Number of features in the conv layers that generates the embedding.
            classes (int): The number of classes to detect.
            concentration (int): The concentration parameter for the distribution.
            assignation_thresholds (dict): A dict with the thresholds to assign an anchor to an object
                or to background. It must have the keys 'object' and 'background' with float values.
            device (str, optional): The device where the module will run.
        """
        super(DirectionalClassification, self).__init__()

        if 'object' not in assignation_thresholds:
            raise ValueError('There is no "object" threshold in the assignation threshold dict')
        if 'background' not in assignation_thresholds:
            raise ValueError('There is no "background" threshold in the assignation threshold dict')

        self.assignation_thresholds = assignation_thresholds
        self.classes = classes
        self.concentration = concentration
        self.device = device if device is not None else 'cuda:0' if torch.cuda.is_available() else 'cpu'
        self.embedding_size = embedding_size

        # Start the means for the distributions as zero vectors
        # We can get the mean for the i-th class with self.means[i]
        self.means = torch.zeros(classes, embedding_size).type(torch.float).to(self.device)

        # We need to keep track of embeddings for each class to update the means. How? The mean could be
        # calculated by the average of the embeddings of the same class normalized. So it's the sum of
        # embeddings that passes through the network and that result normalized to have unit norm.
        self.embeddings_sums = torch.zeros_like(self.means)

        # Create the encoder
        self.encoder = SubModule(in_channels=in_channels, outputs=embedding_size,
                                 anchors=anchors, features=features).to(self.device)

    def encode(self, feature_map):
        """Generate the embeddings for the given feature map.

        Arguments:
            feature_map (torch.Tensor): The feature map to use to generate the embeddings.
                Shape:
                    (batch size, number of features, feature map's height, width)

        Returns:
            torch.Tensor: The embedding for each anchor for each location in the feature map.
                Shape:
                    (batch size, number of total anchors, embedding size)
        """
        batch_size = feature_map.shape[0]
        # Shape (batch size, number of anchors per location * embedding size, height, width)
        embeddings = self.encoder(feature_map)
        # Move the embeddings to the last dimension
        embeddings = embeddings.permute(0, 2, 3, 1).contiguous()
        # Shape (batch size, number of total anchors, embedding size)
        return embeddings.view(batch_size, -1, self.embedding_size)

    def track(self, embeddings, anchors, annotations):
        """Take the embeddings, assign the annotations to each anchor get the assigned anchors
        to objects and with those embeddings sum to the means_sum.

        Why? Because this way we can track all the embeddings per class and then call update_means()
        to set the new means of the model.

        Arguments:
            embeddings (torch.Tensor): All the embeddings generated for each image in the batch.
                Shape:
                    (batch size, number of total anchors, embedding size)
            anchors (torch.Tensor): The base anchors for each location in the feature map.
                Shape:
                    (batch size, number of total anchors, 4)
            annotations (torch.Tensor): The ground truth annotations for the images.
                It assumes that each annotation contains the label in the last value.
                Shape:
                    (batch size, number of annotations, 5)
        """
        # First get the assigned annotation for each anchor and which anchors are assigned as objects
        assignations = Anchors.assign(anchors, annotations, thresholds=self.assignation_thresholds)
        assigned_annotations, anchors_objects_mask, _ = assignations
        # We cannot sum more than one embedding to a given class sum in parallel, we must iterate.
        # Why? What happens if we have 5 embeddings assigned to the class 0? How we can sum all of those
        # embeddings in one instruction?
        with torch.no_grad():
            # Keep only the assigned to objects embeddings
            assigned_annotations = assigned_annotations[anchors_objects_mask]
            embeddings = embeddings.clone().detach()[anchors_objects_mask]
            for index, embedding in enumerate(embeddings):
                assigned_label = assigned_annotations[index, -1].type(torch.long)
                self.embeddings_sums[assigned_label] += embedding

    def update_means(self):
        """Normalize the embeddings_sums and set them as the new means for the module."""
        with torch.no_grad():
            self.means = self.embeddings_sums / self.embeddings_sums.norm(dim=1, keepdim=True)

    def classify(self, embeddings):
        """Get the probability for each embedding to below to each class.

        Arguments:
            embeddings (torch.Tensor): All the embeddings generated.
                Shape:
                    (total embeddings, embedding size)

        Returns:
            torch.Tensor: The probabilities for each embedding.
                Shape:
                    (total embeddings, number of classes)
        """
        logits = torch.exp(self.concentration * torch.matmul(embeddings, self.means.permute(1, 0)))
        return logits / logits.sum(dim=1, keepdim=True)

    def forward(self, embeddings, anchors=None, annotations=None):
        """Update means and get the probabilities for each embedding to belong to each class.

        It needs the annotations and the anchors to keep track of the mean for each class.
        If you only want to get the probabilities for each class it does not need the annotations nor anchors.

        Arguments:
            embeddings (torch.Tensor): All the embeddings generated for each image in the batch.
                Shape:
                    (batch size, number of total anchors, embedding size)
            anchors (torch.Tensor): The base anchors for each location in the feature map.
                Shape:
                    (batch size, number of total anchors, 4)
            annotations (torch.Tensor, optional): The annotations of the image. Useful to keep track
                of the mean for each class. It assumes that each annotation contains the label in the
                last value.
                Shape:
                    (batch size, number of annotations, 5)

        Returns:
            torch.Tensor: Tensor with the probability for each anchor to belong to each class.
                Shape:
                    (batch size, feature map's height * width * number of anchors, classes)
        """
        # We cannot train without the annotations
        if self.training:
            if anchors is None:
                raise ValueError('Directional classification cannot train without the base anchors')
            if annotations is None:
                raise ValueError('Directional classification cannot train without the annotations')

            self.track(embeddings, anchors, annotations)

        # Compute the probabilities
        return self.classify(embeddings)

    def state_dict(self):
        """Get the state dict of the model.

        Why to override this method? Because we must also save the means.

        Returns:
            dict: The dict with the whole state of the model.
        """
        return {
            'means': self.means.state_dict(),
            'parameters': super(DirectionalClassification, self).state_dict()
        }

    def load_state_dict(self, state_dict):
        """Load a given state dict of this model.

        Arguments:
            state_dict (dict): A state dict generated by this model.
        """
        if 'means' not in state_dict:
            raise ValueError('The given state dict does not contains "means" key.')
        if 'parameters' not in state_dict:
            raise ValueError('The given state dict does not contains "parameters" key.')

        self.means.load_state_dict(state_dict['means'])
        super(DirectionalClassification, self).load_state_dict(state_dict['parameters'])
