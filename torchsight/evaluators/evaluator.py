"""Base Evaluator class."""
import time

import torch
from tqdm import tqdm

from torchsight.utils import JsonObject
from torchsight.utils.print import PrintMixin


class Evaluator(PrintMixin):
    """An evaluator base class.

    The evaluator is an interface to evaluate different models. You can override the methods to get the dataset,
    the dataloader and the model, and override the method to compute the forward pass where you can put any
    metric that you want to track.

    This class is intended to avoid boilerplate code and only focus on compute the metric or evaluation that you
    want to have.
    """

    def __init__(self, checkpoint, params=None, device=None):
        """Initialize the evaluator, get the dataset and the model to evaluate.

        Arguments:
            checkpoint (str): The checkpoint generated by the trainer where we can find the model state.
            params (dict): A dict to replace the base params of the evaluator.
            device (str, optional): The device to use for the evaluation.
        """
        self.params = self.get_params().merge(params, verbose=True)
        self.device = device if device is not None else 'cuda:0' if torch.cuda.is_available() else 'cpu'
        self.print('Using device "{}"'.format(self.device))

        self.checkpoint_path = checkpoint
        self.print('Loading checkpoint ...')
        self.checkpoint = torch.load(checkpoint, map_location=self.device)

        self.print('Loading dataset ...')
        self.dataset = self.get_dataset()
        self.print('Loading dataloader ...')
        self.dataloader = self.get_dataloader()
        self.print('Loading model ...')
        self.model = self.get_model()

        # Set a progress bar using the tqdm package
        self.progress_bar = tqdm(self.dataloader)
        # Keep a dict with the key-value pairs to log after each batch
        self.current_log = {}

        # Avoid memory leaks by removing the loaded checkpoint but keep the path to it
        self.checkpoint = None

    @staticmethod
    def get_params():
        """Get the base parameters of the evaluator."""
        return JsonObject({})

    ###############################
    ###         GETTERS         ###
    ###############################

    def get_dataset(self):
        """Get the dataset for the evaluation.

        Returns:
            torch.utils.data.Dataset: The dataset to use for the evaluation.
        """
        raise NotImplementedError('You must provide your own dataset')

    def get_dataloader(self):
        """Get the dataloader to use for the evaluation.

        Returns:
            torch.utils.data.Dataloader: The dataloader to use for the evaluation.
        """
        raise NotImplementedError('You must provide your own dataloader')

    def get_model(self):
        """Get the model to use to make the predictions.

        Returns:
            torch.nn.Module: The model to use to make the predictions over the data.
        """
        raise NotImplementedError('You must provide your own model')

    ###############################
    ###         METHODS         ###
    ###############################

    def eval_mode(self):
        """Put the model in evaluation mode.

        You can override this method to pass arguments to the eval() method of the model
        or do anything what you want.
        """
        self.model.eval()

    def evaluate(self):
        """Run the model over the entire dataset and compute the evaluation metric."""
        tqdm.write('Starting evaluation ...')
        self.model.to(self.device)
        self.eval_mode()

        # The start time of the evaluation and the last batch's end time
        start_time = time.time()
        last_endtime = start_time

        with torch.no_grad():
            for batch, data in enumerate(self.progress_bar):
                self.forward(*data)

                # Update the description of the progress bar with the items of the current log
                batch_time = time.time() - last_endtime
                last_endtime = time.time()
                self.current_log['Time'] = f'{batch_time:.3f}'
                description = ' '.join(f'[{key} {value}]' for key, value in self.current_log.items())
                self.progress_bar.set_description(description)
                self.current_log = {}

                # A callback after each batch
                self.batch_callback(batch)

        # A final callback
        self.evaluate_callback()

    def forward(self, *args):
        """Forward pass through the model, make the predictions and store them.

        Inside this method you should store the predictions made by the model, perform
        some metrics and do the evaluation.
        """
        raise NotImplementedError('Yous should implement your own forward to make the predictions')

    def batch_callback(self, batch):
        """A callback that is called after is batch.

        Arguments:
            batch (int): The number of the batch that was ran before this callback.
        """

    def evaluate_callback(self):
        """A callback that is called at the end of the evaluation."""
